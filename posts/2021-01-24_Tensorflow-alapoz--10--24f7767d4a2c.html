<!DOCTYPE html><html><head><meta http-equiv="Content-Type" content="text/html; charset=utf-8"><title>Tensorflow alapozó 10.</title><style>
      * {
        font-family: Georgia, Cambria, "Times New Roman", Times, serif;
      }
      html, body {
        margin: 0;
        padding: 0;
      }
      h1 {
        font-size: 50px;
        margin-bottom: 17px;
        color: #333;
      }
      h2 {
        font-size: 24px;
        line-height: 1.6;
        margin: 30px 0 0 0;
        margin-bottom: 18px;
        margin-top: 33px;
        color: #333;
      }
      h3 {
        font-size: 30px;
        margin: 10px 0 20px 0;
        color: #333;
      }
      header {
        width: 640px;
        margin: auto;
      }
      section {
        width: 640px;
        margin: auto;
      }
      section p {
        margin-bottom: 27px;
        font-size: 20px;
        line-height: 1.6;
        color: #333;
      }
      section img {
        max-width: 640px;
      }
      footer {
        padding: 0 20px;
        margin: 50px 0;
        text-align: center;
        font-size: 12px;
      }
      .aspectRatioPlaceholder {
        max-width: auto !important;
        max-height: auto !important;
      }
      .aspectRatioPlaceholder-fill {
        padding-bottom: 0 !important;
      }
      header,
      section[data-field=subtitle],
      section[data-field=description] {
        display: none;
      }
      </style></head><body><article class="h-entry">
<header>
<h1 class="p-name">Tensorflow alapozó 10.</h1>
</header>
<section data-field="subtitle" class="p-summary">
Neurális hálózatok tenyésztése genetikus algoritmussal PyGAD és OpenAI Gym használatával
</section>
<section data-field="body" class="e-content">
<section name="b0f4" class="section section--body section--first section--last"><div class="section-divider"><hr class="section-divider"></div><div class="section-content"><div class="section-inner sectionLayout--insetColumn"><h3 name="3e88" id="3e88" class="graf graf--h3 graf--leading graf--title">Tensorflow alapozó 10.</h3><h4 name="ef73" id="ef73" class="graf graf--h4 graf-after--h3 graf--subtitle">Neurális hálózatok tenyésztése genetikus algoritmussal PyGAD és OpenAI Gym használatával</h4><figure name="d797" id="d797" class="graf graf--figure graf-after--h4"><img class="graf-image" data-image-id="0*-1K57shFWoMtbehs.png" data-width="925" data-height="782" data-is-featured="true" src="https://cdn-images-1.medium.com/max/800/0*-1K57shFWoMtbehs.png"><figcaption class="imageCaption">Source: <a href="https://pygad.readthedocs.io/en/latest/" data-href="https://pygad.readthedocs.io/en/latest/" class="markup--anchor markup--figure-anchor" rel="nofollow noopener" target="_blank">https://pygad.readthedocs.io/en/latest/</a></figcaption></figure><p name="33c7" id="33c7" class="graf graf--p graf-after--figure">Hogy kontextusba helyezzem a genetikus algoritmusokat, ismételjük kicsit át, hogy hogyan működik a gradient descent és a backpropagation, ami a neurális hálók tanításának általános módszere. Az erről írt cikkemet itt tudjátok elolvasni:</p><div name="ca4f" id="ca4f" class="graf graf--mixtapeEmbed graf-after--p"><a href="https://thebojda.medium.com/tensorflow-alapoz%C3%B3-2-14720a33aca" data-href="https://thebojda.medium.com/tensorflow-alapoz%C3%B3-2-14720a33aca" class="markup--anchor markup--mixtapeEmbed-anchor" title="https://thebojda.medium.com/tensorflow-alapoz%C3%B3-2-14720a33aca"><strong class="markup--strong markup--mixtapeEmbed-strong">Tensorflow alapozó 2.</strong><br><em class="markup--em markup--mixtapeEmbed-em">(backpropagation, avagy hogyan működik a varázslat)</em>thebojda.medium.com</a><a href="https://thebojda.medium.com/tensorflow-alapoz%C3%B3-2-14720a33aca" class="js-mixtapeImage mixtapeImage u-ignoreBlock" data-media-id="dbaa50b5ec69daef0e595d136920f801" data-thumbnail-img-id="1*YTDwPXrnfbPndXxNw77O-w.png" style="background-image: url(https://cdn-images-1.medium.com/fit/c/160/160/1*YTDwPXrnfbPndXxNw77O-w.png);"></a></div><p name="3f0c" id="3f0c" class="graf graf--p graf-after--mixtapeEmbed">Dióhéjban annyi a lényeg, hogy a gradient descent (gradiens mászás?) segítségével egy függvény minimumát próbáljuk megtalálni. Egy Andrej Karpathytól vett hasonlattal élve olyan ez mintha bekötött szemmel le akarnánk jönni egy hegy véletlenszerűen választott pontjáról. Ilyen esetben a legcélravezetőbb megoldás ha körbetapogatjuk a környezetünket és mindig arra megyünk amerre legjobban lejt a hegy, így jó eséllyel lejuthatunk róla. Azt, hogy egy függvény mennyire “lejt” egy adott pontban, a parciális derivált adja meg. Nincs más dolgunk tehát, mint kiszámolni az adott pontban vett parciális deriváltat, majd lépni egyet az adott irányba mindaddig amíg el nem érjük a minimumot. Ez persze mind szép és jó, de mi köze van ennek a gépi tanuláshoz?</p><p name="da87" id="da87" class="graf graf--p graf-after--p">Vegyünk egy Y=f(X,W) függvényt, ahol X a függvény bemeneteit jelöli, W az állítható paramétereket, Y pedig a kimenetet. A gépi tanulás lényege, hogy megtaláljuk azokat a W paramétereket, amelyek esetén az X bemenetekre adott Y kimenet a legközelebb van az Ye elvárt kimenetekhez. Ehhez definiálunk egy L(Y,Ye) hibafüggvényt, ami megadja, hogy az f függvény által adott Y kimenet mennyire van távol az Ye elvárt kimenettől. A cél, hogy a teljes hibát, vagyis a sum(L(f(Xi,W),Yei)) függvény minimumát megkeressük, ahol Xi és Yei az adott minta be és kimenetei (pl. a bemenet egy kép pixeleinek RGB komponensei, a kimenet pedig az, hogy a kép cica vagy nem cica). Így már értelmet nyer a fenti metódus, hiszen a “hegy” minden pontjához a W paraméterek adott konfigurációja tartozik, az adott pont magassága pedig a hibától függ. Azt a W konfigurációt keressük ahol a magasság minimális, azaz le akarunk jönni a hegyről. A bevezetőben említett backpropagation (hiba visszaterjesztés?) pedig egy a deriválás láncszabályára épülő metódus a parciális deriváltak számításához.</p><p name="e0af" id="e0af" class="graf graf--p graf-after--p">Aki figyelmesen olvasta a fentieket, az észrevehette, hogy egyszer sem említettem a neurális hálózatokat. Végig csak deriválható függvényekről beszéltem. Fontos kiemelni, hogy a gradient descent egy általános módszer, ami minden olyan probléma esetén alkalmazható, ahol a probléma megoldására fel tudunk írni parciálisan deriválható függvényt. Valójában a gépi tanulás programkönyvtárak (amilyen a Tensorflow és a PyTorch) elsődleges képessége nem a neurális hálózatok futtatása, hanem tenzorműveletek hatékony végrehajtása (pl. GPU segítségével) valamint a deriváltak számítása (Tensorflow esetén a <a href="https://thebojda.medium.com/tensorflow-alapoz%C3%B3-2-14720a33aca" data-href="https://thebojda.medium.com/tensorflow-alapoz%C3%B3-2-14720a33aca" class="markup--anchor markup--p-anchor" rel="noopener" target="_blank">GradientTape-el</a>, PyTorch esetén pedig az <a href="https://thebojda.medium.com/tensorflow-alapoz%C3%B3-9-pytorch-alapoz%C3%B3-ee4e9d98b46b" data-href="https://thebojda.medium.com/tensorflow-alapoz%C3%B3-9-pytorch-alapoz%C3%B3-ee4e9d98b46b" class="markup--anchor markup--p-anchor" rel="noopener" target="_blank">autograd rendszernek köszönhetően</a>). A neurális hálók futtatása már egy következő réteg a tenzoros alap felett. Minél mélyebben beleássa magát az ember a neurális hálók témakörébe, annál gyakrabban találkozik olyan megoldásokkal, ahol igazából már nem csak neuronok hálózatáról beszélünk. Az attention mechanizmusok például külön mátrix szorzások segítségével valósulnak meg, vagy a mostanában divatos kapszula hálózatok esetén ugyan vannak konvolúciós rétegek, de ezen kívűl mátrix szorzások és egy egész speciális tanulási módszernek köszönhetően áll össze az eredmény. Ezek az új megoldások tehát már sokkal többek mint neuronok előre csatolt hálózata, sokkal jobb inkább általánosan, deriválható tenzorfügvényekként felfognunk őket.</p><p name="90b5" id="90b5" class="graf graf--p graf-after--p">Ahogyan a fentiekből látszik, a gradient descent (és az ezt megvalósító backpropagation) alapvető fontosságú a gépi tanulás szempontjából. Enélkül a módszer nélkül aligha tartana ott a gépi tanulás ahol most tart. Van azonban a gradient descentnek pár alapvető korlátja, amiért érdemes lehet más módszereket is megvizsgálnunk. Az egyik legalapvetőbb korlát, hogy amennyiben a problémára nem tudunk parciálisan deriválható függvényt felírni, úgy nem alkalmazható rá a gradient descent. A másik probléma, hogy ha a hibafüggvény rendelkezik lokális minimumokkal, ugy ezekbe beleragadhat az algoritmus. Olyan ez mintha hegyről lemászás közben valahol középen egy gödörbe jutnánk. Itt akármerre is tapogatózunk, minden irányban csak felfelé haladhatunk, így azt gondolhatjuk, hogy lejutottunk a hegyről. Ilyen problémák kiküszöbölésére jöhetnek jól az evolúciós algoritmusok amik kevésbé hatékonyak ugyan, de cserébe sokkal általánosabbak, használhatóak nem deriválható függvények esetén is és immunisak a lokális minimumokra.</p><p name="f3b6" id="f3b6" class="graf graf--p graf-after--p">Amikor evolúciós algoritmusok segítségével keressük a megoldást, a “manónk” nem csak hogy teljesen vak, de még tapogatózni sem tud (nem ismerjük a deriváltakat), cserébe viszont egyetlen “manó” helyett egy egész populációt használunk, akiket a hibafüggvény véletlenszerű helyeire “dobálunk le”. Minden manóról csak annyit tudunk, hogy mennyire magasan van. Minél alacsonyabban van egy manó, annál életképesebb. Ezt követően megöljük a manók bizonyos százalékát (szelekció). Minél életképesebb egy manó, annál kisebb eséllyel fog meghalni, így annál nagyobb eséllyel kerül át a következő populációba. Ezt követően a megmaradt populációt használva újra feldúsítjuk a manók számát. A genetikus algoritmusok esetén ehhez genetikus operátorokat használunk. Ezek jellemzően a mutáció, amikor egy vagy több ponton véletlenszerűen változtatunk valamit a géneken, vagy a keresztezés, amikor két vagy több szülő génjeit véletlenszerű ponton elvágva felcseréljük így kapva új utódokat. Jól láthatóan a genetikus algoritmusok használata a biológiából ellesett módszerek használatát jelenti. Az egyes populációk egyedei a folyamat során egyre inkább a minimum köré csoportosulnak, majd elég iteráció után meg is találják azt. Nézzük hogy néz ki ez a gyakorlatban…</p><p name="bfd6" id="bfd6" class="graf graf--p graf-after--p">A genetikus algoritmusok használata esetén mindig szükség van egy DNS-re ami jellemzően egy számlista (egy dimenziós tenzor), valamint egy rátermettségi (fitness) függvényre ami megmondja, hogy az adott DNS-el rendelkező egyed mennyire életképes. Ha ez a két dolog megvan, már indulhat is a tenyésztés. Ha neurális hálózatokat akarunk tenyészteni, akkor a DNS célszerűen a neurális háló súlyainak listája, a rátermettségi függvény pedig a hálózat teljes hibájának reciproka (minél kisebb a hiba, annál rátermettebb az egyed). A cél, hogy megtaláljuk a legrátermettebb egyedet, ami a legjobb DNS-el (legjobb súly kombinációval) rendelkezik.</p><p name="f29d" id="f29d" class="graf graf--p graf-after--p">Ennyi elmélet után lássuk a kódot. Olvastam pár tanulmányt arról, hogy a genetikus algoritmusok a backpropagation-höz mérhető sebességgel, vagy bizonyos esetekben annál gyorsabban (!) megtalálják a megoldást megerősítéses tanulás (reinforcement learning) feladatok esetén, így gondoltam előveszem a reinforcement learningről szóló 4. rész kódját és azt fogom átírni úgy, hogy genetikus algoritmust használjon. Aki esetleg nem olvasta a 4. részt, az itt megteheti:</p><div name="ce52" id="ce52" class="graf graf--mixtapeEmbed graf-after--p"><a href="https://thebojda.medium.com/tensorflow-alapoz%C3%B3-4-cfeee8b9e34c" data-href="https://thebojda.medium.com/tensorflow-alapoz%C3%B3-4-cfeee8b9e34c" class="markup--anchor markup--mixtapeEmbed-anchor" title="https://thebojda.medium.com/tensorflow-alapoz%C3%B3-4-cfeee8b9e34c"><strong class="markup--strong markup--mixtapeEmbed-strong">Tensorflow alapozó 4.</strong><br><em class="markup--em markup--mixtapeEmbed-em">(Reinforcement learning, Deep Q-learning és OpenAI Gym)</em>thebojda.medium.com</a><a href="https://thebojda.medium.com/tensorflow-alapoz%C3%B3-4-cfeee8b9e34c" class="js-mixtapeImage mixtapeImage u-ignoreBlock" data-media-id="80c9dc1c7bb73936100c68d3eb0f0fac" data-thumbnail-img-id="1*fBRNFgr42ZsRwFEke0zoUA.jpeg" style="background-image: url(https://cdn-images-1.medium.com/fit/c/160/160/1*fBRNFgr42ZsRwFEke0zoUA.jpeg);"></a></div><p name="767c" id="767c" class="graf graf--p graf-after--mixtapeEmbed">Szokásos módon a kód most is <a href="https://colab.research.google.com/gist/TheBojda/3188064d9596d2c3691eba0372b4e972/pygad_reinforcement.ipynb" data-href="https://colab.research.google.com/gist/TheBojda/3188064d9596d2c3691eba0372b4e972/pygad_reinforcement.ipynb" class="markup--anchor markup--p-anchor" rel="noopener" target="_blank">kipróbálható Google Colabban</a>, így akinek kedve van egy külön böngészőfülön megnyithatja a kódot és olvasás közben követheti annak futását.</p><p name="d7a0" id="d7a0" class="graf graf--p graf-after--p">A hálózatok tenyésztéséhez a <a href="https://pygad.readthedocs.io/en/latest/" data-href="https://pygad.readthedocs.io/en/latest/" class="markup--anchor markup--p-anchor" rel="noopener" target="_blank">PyGAD</a> nevű programkönyvtárat használjuk, így mindenek előtt ezt kell telepítenünk, valamint a Tensorflow-t és a Gym-et, amit Colabban már eleve telepítve kapunk.</p><figure name="864c" id="864c" class="graf graf--figure graf--iframe graf-after--p"><script src="https://gist.github.com/TheBojda/2b35fae2dea7d59f55e3d8f02b50224f.js"></script></figure><p name="abaa" id="abaa" class="graf graf--p graf-after--figure">Maga a kód láthatóan nem túl bonyolult. A 37. sortól inicializáljuk a Gym CartPole környezetét, a 41. sortól pedig definiáljuk ugyanazt az egyszerű neurális hálózatot, amit a 4. részben is használtunk. A 49. sorban felparaméterezzük a genetikus algoritmusunkat, amit a 60. sorban le is futtatunk. A paraméterek jelentése megtalálható a dokumentációban, így csak azokra térnék ki, amik a konkrét működés szempontjából fontosak.</p><p name="f15a" id="f15a" class="graf graf--p graf-after--p">Maga a PyGAD egy teljesen általános genetikus algoritmusok futtatására képes rendszer. Ennek a kiterjesztése a KerasGA, ami az általános motor Tensorflow (Keras) neurális hálókon történő futtatását segíti. A 47. sorban létrehozott KerasGA objektum ennek a kiterjesztésnek a része és arra szolgál, hogy a paraméterként átadott modellből a második paraméterben megadott számosságú populációt hozzon létre. Mivel a hálózatunk 386 állítható paraméterrel rendelkezik, ezért a DNS-ünk itt 386 elemből fog állni. A populáció mérete 10 egyed, így a kezdő populációnk egy 10x386 elemű mátrix lesz. Ezt adjuk át az 51. sorban az initial_population paraméterben.</p><p name="1341" id="1341" class="graf graf--p graf-after--p">Az egész algoritmus lelke az 52. sorban átadott fitness_func függvény, amit a 9. sortól definiálunk. Ez a függvény adja meg az adott DNS-hez (ez esetben az adott súly konfigurációhoz), hogy mennyire jó. A függvény a solution paraméterben kapja meg a DNS-t, amit a 12-es sorban egy KerasGA függvénnyel alakítunk vissza súly listává, amit a 13. sorban be is állítunk a modellnek. Mivel reinforcement learningről van szó, ezért a modellt úgy teszteljük, hogy lejátszunk vele egy teljes játékot, majd eredményként az összegyűjtött rewardot adjuk vissza. Minél több rewardot sikerült összegyűjteni a játék során, annál sikeresebb a hálózat, így az összes reward pont megfelel a rátermettségi függvény visszatérési értékének.</p><p name="14b3" id="14b3" class="graf graf--p graf-after--p">Dióhéjban ennyi az egész. A kód végén még rajzolunk egy grafikont az egészből, illetve elmentjük a súlyokat. A Colab jegyzetfüzet végén található még egy minimális kód, ami a mentett súlyok alapján le tud játszani egy játékot, így visszaellenőrizhetjük, hogy milyen jó lett a neurális hálónk.</p><p name="ba0b" id="ba0b" class="graf graf--p graf-after--p">Mivel az evolúciós algoritmusok esetén nagy szerep jut a véletlennek, ezért több futtatás esetén egészen eltérő eredményeket kaphatunk. Én azt tapasztaltam, hogy az esetek többségében sokkal hamarabb és sokkal jobb eredményeket sikerült kihozni, mint Q learninggel. Ebből persze hiba lenne messzemenő következtetéseket levonni, de annyi talán ebből is látszik, hogy a genetikus algoritmusoknak van létjogosultásága, sőt, a fentebb említettek miatt vannak esetek, ahol nincs is nagyon más választásunk (pl. mert nem írható a problémára deriválható függvény). Emellett a genetikus algoritmusok rendelkeznek pár előnyös tulajdonsággal amiért érdemes lehet rájuk odafigyalni. Az egyik ilyen tulajdonság, hogy a populáció méretét a végtelenségig növelehtjük mivel az egy populációban lévő egyedek rátermettségi vizsgálata párhuzamosan elvégezhető. A gradient descent nem ennyire jól párhuzamosítható hiszen az egyes lépések egymásra épülnek, így elképzelhetőnek tartom, hogy megfelelő párhuzamos számítási kapacitás mellett a genetikus algoritmusok képesek sebességben túlteljesíteni a gradient descent-et.</p><p name="bdce" id="bdce" class="graf graf--p graf-after--p">A másik dolog ami miatt nagyon izgalmasak a genetikus algoritmusok (és bármilyen gradiens mentes megoldás), hogy sokkal közelebb állhatnak ahhoz ahogyan a biológiai agy is működik (biologically plausible). El tudok például képzelni egy olyan modellt az emberi agy működésére ahol az agy fő szerkezetét a “makroevolúció” alakítja ki, de ahol ez az egyed szintjén nem áll meg és “mikroevolúció” zajlik az érzékelés és a gondolatok szintjén, így alakítva ki a tudatos gondolkodást. De ez persze csak az én saját bejáratú elméletem…</p><p name="56ef" id="56ef" class="graf graf--p graf-after--p">Bárhogy is legyen, a genetikus algoritmusok hasznos eszközök lehetnek a gépi tanulás eszköztárában, vagy akár önálló problémamegoldó eszközként. Azt sem tartom kizártnak, hogy a jövőben (biológiai számítógépek, stb.) meghatározó területévé válnak majd az informatikának. Minden esetre érdemes ismerni a területet és odafigyelni rá…</p><p name="09e4" id="09e4" class="graf graf--p graf-after--p">Ha tetszett az írás, olvasd el az előző részeket is:</p><div name="5153" id="5153" class="graf graf--mixtapeEmbed graf-after--p"><a href="https://thebojda.medium.com/tensorflow-alapoz%C3%B3-d2d1ee97c9db" data-href="https://thebojda.medium.com/tensorflow-alapoz%C3%B3-d2d1ee97c9db" class="markup--anchor markup--mixtapeEmbed-anchor" title="https://thebojda.medium.com/tensorflow-alapoz%C3%B3-d2d1ee97c9db"><strong class="markup--strong markup--mixtapeEmbed-strong">TensorFlow alapozó</strong><br><em class="markup--em markup--mixtapeEmbed-em">(neurális hálózatok, tenzorok és képfelismerés a gyakorlatban)</em>thebojda.medium.com</a><a href="https://thebojda.medium.com/tensorflow-alapoz%C3%B3-d2d1ee97c9db" class="js-mixtapeImage mixtapeImage u-ignoreBlock" data-media-id="fe513714c6817ef21c055e71d95adde1" data-thumbnail-img-id="1*T4ARzySpEQvEnr_9pc78pg.jpeg" style="background-image: url(https://cdn-images-1.medium.com/fit/c/160/160/1*T4ARzySpEQvEnr_9pc78pg.jpeg);"></a></div><div name="252e" id="252e" class="graf graf--mixtapeEmbed graf-after--mixtapeEmbed"><a href="https://thebojda.medium.com/tensorflow-alapoz%C3%B3-2-14720a33aca" data-href="https://thebojda.medium.com/tensorflow-alapoz%C3%B3-2-14720a33aca" class="markup--anchor markup--mixtapeEmbed-anchor" title="https://thebojda.medium.com/tensorflow-alapoz%C3%B3-2-14720a33aca"><strong class="markup--strong markup--mixtapeEmbed-strong">Tensorflow alapozó 2.</strong><br><em class="markup--em markup--mixtapeEmbed-em">(backpropagation, avagy hogyan működik a varázslat)</em>thebojda.medium.com</a><a href="https://thebojda.medium.com/tensorflow-alapoz%C3%B3-2-14720a33aca" class="js-mixtapeImage mixtapeImage u-ignoreBlock" data-media-id="dbaa50b5ec69daef0e595d136920f801" data-thumbnail-img-id="1*YTDwPXrnfbPndXxNw77O-w.png" style="background-image: url(https://cdn-images-1.medium.com/fit/c/160/160/1*YTDwPXrnfbPndXxNw77O-w.png);"></a></div><div name="e627" id="e627" class="graf graf--mixtapeEmbed graf-after--mixtapeEmbed"><a href="https://thebojda.medium.com/tensorflow-alapoz%C3%B3-3-ac3d26071b27" data-href="https://thebojda.medium.com/tensorflow-alapoz%C3%B3-3-ac3d26071b27" class="markup--anchor markup--mixtapeEmbed-anchor" title="https://thebojda.medium.com/tensorflow-alapoz%C3%B3-3-ac3d26071b27"><strong class="markup--strong markup--mixtapeEmbed-strong">Tensorflow alapozó 3.</strong><br><em class="markup--em markup--mixtapeEmbed-em">(autoencoderek, word2vec és embedding avagy dimenzió redukció neurális hálókkal)</em>thebojda.medium.com</a><a href="https://thebojda.medium.com/tensorflow-alapoz%C3%B3-3-ac3d26071b27" class="js-mixtapeImage mixtapeImage u-ignoreBlock" data-media-id="76b3cae0c589e0a3269a80b43ab599fc" data-thumbnail-img-id="1*JOkfva-B2_D3vJV3AYPOCQ.png" style="background-image: url(https://cdn-images-1.medium.com/fit/c/160/160/1*JOkfva-B2_D3vJV3AYPOCQ.png);"></a></div><div name="b48f" id="b48f" class="graf graf--mixtapeEmbed graf-after--mixtapeEmbed"><a href="https://thebojda.medium.com/tensorflow-alapoz%C3%B3-4-cfeee8b9e34c" data-href="https://thebojda.medium.com/tensorflow-alapoz%C3%B3-4-cfeee8b9e34c" class="markup--anchor markup--mixtapeEmbed-anchor" title="https://thebojda.medium.com/tensorflow-alapoz%C3%B3-4-cfeee8b9e34c"><strong class="markup--strong markup--mixtapeEmbed-strong">Tensorflow alapozó 4.</strong><br><em class="markup--em markup--mixtapeEmbed-em">(Reinforcement learning, Deep Q-learning és OpenAI Gym)</em>thebojda.medium.com</a><a href="https://thebojda.medium.com/tensorflow-alapoz%C3%B3-4-cfeee8b9e34c" class="js-mixtapeImage mixtapeImage u-ignoreBlock" data-media-id="80c9dc1c7bb73936100c68d3eb0f0fac" data-thumbnail-img-id="1*fBRNFgr42ZsRwFEke0zoUA.jpeg" style="background-image: url(https://cdn-images-1.medium.com/fit/c/160/160/1*fBRNFgr42ZsRwFEke0zoUA.jpeg);"></a></div><div name="fc72" id="fc72" class="graf graf--mixtapeEmbed graf-after--mixtapeEmbed"><a href="https://thebojda.medium.com/tensorflow-alapoz%C3%B3-5-df99bc48e306" data-href="https://thebojda.medium.com/tensorflow-alapoz%C3%B3-5-df99bc48e306" class="markup--anchor markup--mixtapeEmbed-anchor" title="https://thebojda.medium.com/tensorflow-alapoz%C3%B3-5-df99bc48e306"><strong class="markup--strong markup--mixtapeEmbed-strong">Tensorflow alapozó 5.</strong><br><em class="markup--em markup--mixtapeEmbed-em">Visszacsatolt hálózatok és LSTM</em>thebojda.medium.com</a><a href="https://thebojda.medium.com/tensorflow-alapoz%C3%B3-5-df99bc48e306" class="js-mixtapeImage mixtapeImage u-ignoreBlock" data-media-id="c8f5c61cbf715b9073dfc93d153e6e6a" data-thumbnail-img-id="1*IMalbwl6uj3nlqxixZYFvA.jpeg" style="background-image: url(https://cdn-images-1.medium.com/fit/c/160/160/1*IMalbwl6uj3nlqxixZYFvA.jpeg);"></a></div><div name="624a" id="624a" class="graf graf--mixtapeEmbed graf-after--mixtapeEmbed"><a href="https://thebojda.medium.com/tensorflow-alapoz%C3%B3-6-1ba1a32a79d5" data-href="https://thebojda.medium.com/tensorflow-alapoz%C3%B3-6-1ba1a32a79d5" class="markup--anchor markup--mixtapeEmbed-anchor" title="https://thebojda.medium.com/tensorflow-alapoz%C3%B3-6-1ba1a32a79d5"><strong class="markup--strong markup--mixtapeEmbed-strong">Tensorflow alapozó 6.</strong><br><em class="markup--em markup--mixtapeEmbed-em">GAN-ok, avagy hogyan generáljunk cicákat neurális hálóval</em>thebojda.medium.com</a><a href="https://thebojda.medium.com/tensorflow-alapoz%C3%B3-6-1ba1a32a79d5" class="js-mixtapeImage mixtapeImage u-ignoreBlock" data-media-id="19bce41806ff458545b5f14303550246" data-thumbnail-img-id="1*LqPaOCYKuzRjMis_sKWYwQ.png" style="background-image: url(https://cdn-images-1.medium.com/fit/c/160/160/1*LqPaOCYKuzRjMis_sKWYwQ.png);"></a></div><div name="51f7" id="51f7" class="graf graf--mixtapeEmbed graf-after--mixtapeEmbed"><a href="https://thebojda.medium.com/tensorflow-alapoz%C3%B3-7-57eb9a077adc" data-href="https://thebojda.medium.com/tensorflow-alapoz%C3%B3-7-57eb9a077adc" class="markup--anchor markup--mixtapeEmbed-anchor" title="https://thebojda.medium.com/tensorflow-alapoz%C3%B3-7-57eb9a077adc"><strong class="markup--strong markup--mixtapeEmbed-strong">TensorFlow alapozó 7.</strong><br><em class="markup--em markup--mixtapeEmbed-em">TensorFlow.js, avagy neurális hálók futtatása és tanítása böngészőben és Node.js-en</em>thebojda.medium.com</a><a href="https://thebojda.medium.com/tensorflow-alapoz%C3%B3-7-57eb9a077adc" class="js-mixtapeImage mixtapeImage u-ignoreBlock" data-media-id="43843f4a201196967180ca38c33fadd1" data-thumbnail-img-id="1*YvKKzsjspxIfzY4Xj2e9pQ.png" style="background-image: url(https://cdn-images-1.medium.com/fit/c/160/160/1*YvKKzsjspxIfzY4Xj2e9pQ.png);"></a></div><div name="8ae4" id="8ae4" class="graf graf--mixtapeEmbed graf-after--mixtapeEmbed"><a href="https://thebojda.medium.com/tensorflow-alapoz%C3%B3-8-909a84f7a961" data-href="https://thebojda.medium.com/tensorflow-alapoz%C3%B3-8-909a84f7a961" class="markup--anchor markup--mixtapeEmbed-anchor" title="https://thebojda.medium.com/tensorflow-alapoz%C3%B3-8-909a84f7a961"><strong class="markup--strong markup--mixtapeEmbed-strong">Tensorflow alapozó 8.</strong><br><em class="markup--em markup--mixtapeEmbed-em">avagy modellek futtatása mobilon, IoT eszközökön és akár mikrokontrollereken</em>thebojda.medium.com</a><a href="https://thebojda.medium.com/tensorflow-alapoz%C3%B3-8-909a84f7a961" class="js-mixtapeImage mixtapeImage u-ignoreBlock" data-media-id="4e11fd997a0a7d5666e86453f35de10c" data-thumbnail-img-id="1*Ry0sAkDB6QJEH3KS-KN5dQ.png" style="background-image: url(https://cdn-images-1.medium.com/fit/c/160/160/1*Ry0sAkDB6QJEH3KS-KN5dQ.png);"></a></div><div name="5813" id="5813" class="graf graf--mixtapeEmbed graf-after--mixtapeEmbed graf--trailing"><a href="https://thebojda.medium.com/tensorflow-alapoz%C3%B3-9-pytorch-alapoz%C3%B3-ee4e9d98b46b" data-href="https://thebojda.medium.com/tensorflow-alapoz%C3%B3-9-pytorch-alapoz%C3%B3-ee4e9d98b46b" class="markup--anchor markup--mixtapeEmbed-anchor" title="https://thebojda.medium.com/tensorflow-alapoz%C3%B3-9-pytorch-alapoz%C3%B3-ee4e9d98b46b"><strong class="markup--strong markup--mixtapeEmbed-strong">Tensorflow alapozó 9: PyTorch alapozó</strong><br><em class="markup--em markup--mixtapeEmbed-em">A Google által fejlesztett Tensorflow mellett a másik nagy, ugyancsak Python alapú gépi tanulás rendszer a Facebook…</em>thebojda.medium.com</a><a href="https://thebojda.medium.com/tensorflow-alapoz%C3%B3-9-pytorch-alapoz%C3%B3-ee4e9d98b46b" class="js-mixtapeImage mixtapeImage u-ignoreBlock" data-media-id="0e0ce43dae9b962c7a0f97ab8a091541" data-thumbnail-img-id="1*VSQ0XEywxSgZBwW05GsZtw.png" style="background-image: url(https://cdn-images-1.medium.com/fit/c/160/160/1*VSQ0XEywxSgZBwW05GsZtw.png);"></a></div></div></div></section>
</section>
<footer><p>By <a href="https://medium.com/@thebojda" class="p-author h-card">Laszlo Fazekas</a> on <a href="https://medium.com/p/24f7767d4a2c"><time class="dt-published" datetime="2021-01-24T10:42:41.869Z">January 24, 2021</time></a>.</p><p><a href="https://medium.com/@thebojda/tensorflow-alapoz%C3%B3-10-24f7767d4a2c" class="p-canonical">Canonical link</a></p><p>Exported from <a href="https://medium.com">Medium</a> on March 9, 2021.</p></footer></article></body></html>