<!DOCTYPE html><html><head><meta http-equiv="Content-Type" content="text/html; charset=utf-8"><title>How Machines Can Learn Using TensorFlow or PyTorch</title><style>
      * {
        font-family: Georgia, Cambria, "Times New Roman", Times, serif;
      }
      html, body {
        margin: 0;
        padding: 0;
      }
      h1 {
        font-size: 50px;
        margin-bottom: 17px;
        color: #333;
      }
      h2 {
        font-size: 24px;
        line-height: 1.6;
        margin: 30px 0 0 0;
        margin-bottom: 18px;
        margin-top: 33px;
        color: #333;
      }
      h3 {
        font-size: 30px;
        margin: 10px 0 20px 0;
        color: #333;
      }
      header {
        width: 640px;
        margin: auto;
      }
      section {
        width: 640px;
        margin: auto;
      }
      section p {
        margin-bottom: 27px;
        font-size: 20px;
        line-height: 1.6;
        color: #333;
      }
      section img {
        max-width: 640px;
      }
      footer {
        padding: 0 20px;
        margin: 50px 0;
        text-align: center;
        font-size: 12px;
      }
      .aspectRatioPlaceholder {
        max-width: auto !important;
        max-height: auto !important;
      }
      .aspectRatioPlaceholder-fill {
        padding-bottom: 0 !important;
      }
      header,
      section[data-field=subtitle],
      section[data-field=description] {
        display: none;
      }
      </style></head><body><article class="h-entry">
<header>
<h1 class="p-name">How Machines Can Learn Using TensorFlow or PyTorch</h1>
</header>
<section data-field="subtitle" class="p-summary">
A deep dive into their minds, their evolution
</section>
<section data-field="body" class="e-content">
<section name="461c" class="section section--body section--first section--last"><div class="section-divider"><hr class="section-divider"></div><div class="section-content"><div class="section-inner sectionLayout--insetColumn"><h3 name="b9c8" id="b9c8" class="graf graf--h3 graf--leading graf--title">How Machines Can Learn Using TensorFlow or PyTorch</h3><h4 name="a66a" id="a66a" class="graf graf--h4 graf-after--h3 graf--subtitle">A deep dive into their minds, their evolution</h4><figure name="fd58" id="fd58" class="graf graf--figure graf-after--h4"><img class="graf-image" data-image-id="1*v7v6s2Nni8KOjgzeK_gzsA.jpeg" data-width="1920" data-height="2400" data-is-featured="true" src="https://cdn-images-1.medium.com/max/800/1*v7v6s2Nni8KOjgzeK_gzsA.jpeg"><figcaption class="imageCaption">source: <a href="https://unsplash.com/" data-href="https://unsplash.com/" class="markup--anchor markup--figure-anchor" rel="nofollow noopener" target="_blank">https://unsplash.com/</a></figcaption></figure><p name="169a" id="169a" class="graf graf--p graf-after--figure">AI and machine learning are very hot topics these days. Self-driving cars, real-time automatic translation, voice recognition, etc. These are only some of the applications that cannot exist without machine learning. But how can machines learn? I will show you how the magic works in this article, but I won’t talk about neural networks! I will show you what is in the deepest deep of machine learning.</p><p name="ad84" id="ad84" class="graf graf--p graf-after--p">One of the best presentations about machine learning is Fei Fei Li’s TED talk.</p><figure name="2732" id="2732" class="graf graf--figure graf--iframe graf-after--p"><iframe src="https://embed.ted.com/talks/fei_fei_li_how_we_re_teaching_computers_to_understand_pictures" width="560" height="316" frameborder="0" scrolling="no"></iframe></figure><p name="23db" id="23db" class="graf graf--p graf-after--figure">As Li said in her talk, in the early days, programmers tried to solve computer vision tasks by algorithms. These algorithms were looking for shapes and tried to identify things by preprogrammed rules. But they could not solve it in this way. Computer vision is a very complex problem, and we cannot solve it algorithmically. So, here comes machine learning into the picture.</p><p name="b7f4" id="b7f4" class="graf graf--p graf-after--p"><em class="markup--em markup--p-em">If we cannot write the program, let’s write a program that writes it instead of us.</em></p><p name="4a9e" id="4a9e" class="graf graf--p graf-after--p">Machine learning algorithms do this. If we have enough data, they can write the algorithm that calculates the expected output from the inputs for us. In the case of image recognition, the input is the image, and the output is a label or a description of the content of the image.</p><p name="d024" id="d024" class="graf graf--p graf-after--p">This is why Li and her team worked really hard to build ImageNet, which was the biggest labeled image database in the world, with 15 million images and 22,000 categories.</p><p name="ed54" id="ed54" class="graf graf--p graf-after--p">Thanks to ImageNet, we had enough data, but how did programmers use it to solve the problem of image recognition? This is the point where I should talk about neural networks like Li does, but I won’t. Neural networks are indeed inspired by the biological brain, but today’s transformers are far from the biological model.</p><p name="06ee" id="06ee" class="graf graf--p graf-after--p">This is why I think the name ‘neural network’ is misleading. But what would be a better name?</p><p name="facb" id="facb" class="graf graf--p graf-after--p">My favorite is Karpathy’s (head of AI @ Tesla) <a href="https://karpathy.medium.com/software-2-0-a64152b37c35" data-href="https://karpathy.medium.com/software-2-0-a64152b37c35" class="markup--anchor markup--p-anchor" rel="noopener" target="_blank">Software 2.0</a>.</p><figure name="dccf" id="dccf" class="graf graf--figure graf--iframe graf-after--p"><iframe src="https://www.youtube.com/embed/y57wwucbXR8?feature=oembed" width="700" height="393" frameborder="0" scrolling="no"></iframe></figure><p name="f7f4" id="f7f4" class="graf graf--p graf-after--figure">As Karpathy said in this talk, Software 1.0 is the classical software, where the programmer writes the code, but in the case of Software 2.0, another software finds it based on the big data. But how can a software write another software? Let me quote Karpathy:</p><blockquote name="43f8" id="43f8" class="graf graf--pullquote graf--startsWithDoubleQuote graf-after--p">“Gradient descent can write code better than you. I’m sorry.”</blockquote><p name="7347" id="7347" class="graf graf--p graf-after--pullquote"><a href="https://en.wikipedia.org/wiki/Gradient_descent" data-href="https://en.wikipedia.org/wiki/Gradient_descent" class="markup--anchor markup--p-anchor" rel="noopener" target="_blank">Gradient descent</a> is the name of the magic that is used by most machine learning systems. To understand it, let’s imagine a machine learning system. It’s a black box with inputs and outputs. If the purpose of the system is image recognition, then the input is an array of pixels, and the output is a probability distribution vector.</p><p name="33d2" id="33d2" class="graf graf--p graf-after--p">If the system can identify cats and dogs, and the input is an image of a cat, the output will be like 10% dog and 90% cat. So, the inputs are numbers, the outputs are also numbers, and the content of the black box is a giant (really huge) mathematical expression. The black box calculates the output vector from the pixel data.</p><p name="0ff7" id="0ff7" class="graf graf--p graf-after--p">I promised to talk about programs writing programs, and now I’m talking about mathematical expression. But in fact, programs are mathematical expressions. CPUs are built from logic gates. They use the binary representation of numbers and logic operators.</p><p name="62b5" id="62b5" class="graf graf--p graf-after--p">Any existing algorithm can be implemented by these logical expressions, so a long logical expression can represent any program that can run on classical computers. As you see, classical programs are also not more than binary calculations from a binary input to a binary output.</p><p name="98df" id="98df" class="graf graf--p graf-after--p">Machine learning systems use real numbers and operators instead of binary numbers and operators, but basically, these mathematical expressions are also “programs.”</p><blockquote name="12f7" id="12f7" class="graf graf--blockquote graf-after--p">Alan Turing <a href="http://www.alanturing.net/turing_archive/pages/Reference%20Articles/connectionism/Turing%27s%20neural%20networks.html" data-href="http://www.alanturing.net/turing_archive/pages/Reference%20Articles/connectionism/Turing&#39;s%20neural%20networks.html" class="markup--anchor markup--blockquote-anchor" rel="noopener" target="_blank">wrote a paper in 1948</a> about “B-type unorganized machines.” These machines are built from interconnected logical NAND gates, and can be trained by enabling/disabling the wires between the nodes. In binary algebra, NAND is a universal operator, because every other operator can be expressed by it. These B-type unorganized machines are universal computers because every algorithm can be implemented on them.</blockquote><blockquote name="bf2a" id="bf2a" class="graf graf--blockquote graf-after--blockquote">These B-type machines are similar to nowadays neural networks, but they are implemented upon logic gates like today’s CPUs, so the algorithms that are implemented by these B-type machines would be more like our today’s algorithms.</blockquote><blockquote name="ee82" id="ee82" class="graf graf--blockquote graf-after--blockquote">Unfortunately, Turing never published the paper, this is why we do not know him as an inventor of early neural networks. The problem with these B-type networks is that they cannot be trained efficiently.</blockquote><p name="05ac" id="05ac" class="graf graf--p graf-after--blockquote">If we have a set of inputs and outputs and a parameterized expression, how can we find the correct parameters that calculate the outputs from the inputs with the fewest error? It is something like a black box that has many potentiometers. Every combination of the potentiometer positions is a program, and we are searching for the correct positions.</p><figure name="b5b8" id="b5b8" class="graf graf--figure graf-after--p"><img class="graf-image" data-image-id="1*gGWOooPm0v9ZT_Wr6u4Sqw.jpeg" data-width="1920" data-height="1280" src="https://cdn-images-1.medium.com/max/800/1*gGWOooPm0v9ZT_Wr6u4Sqw.jpeg"><figcaption class="imageCaption">source: <a href="https://unsplash.com/" data-href="https://unsplash.com/" class="markup--anchor markup--figure-anchor" rel="nofollow noopener" target="_blank">https://unsplash.com/</a></figcaption></figure><p name="9112" id="9112" class="graf graf--p graf-after--figure">To solve this problem, let’s imagine the error function. It looks like a landscape with hills and valleys. Every single parameter of the expression is a dimension of the landscape, and the height of the current point is the error with the given parameters.</p><figure name="60f3" id="60f3" class="graf graf--figure graf-after--p"><img class="graf-image" data-image-id="0*H2kAUwqp2VV_WsNl.png" data-width="760" data-height="624" src="https://cdn-images-1.medium.com/max/800/0*H2kAUwqp2VV_WsNl.png"><figcaption class="imageCaption">source: <a href="https://en.wikipedia.org/wiki/Gradient_descent" data-href="https://en.wikipedia.org/wiki/Gradient_descent" class="markup--anchor markup--figure-anchor" rel="noopener ugc nofollow noopener" target="_blank">https://en.wikipedia.org/wiki/Gradient_descent</a></figcaption></figure><p name="e41d" id="e41d" class="graf graf--p graf-after--figure">When we initialize the expression with random numbers, we are at a random location on the landscape. To minimize the errors, we have to come down to the lowest point (that represents the lowest error). The problem is that we are completely blind. How can we come down from the hill?</p><p name="78fe" id="78fe" class="graf graf--p graf-after--p">We can grope around to find the steepest slope and go there. But how can we determine the slope of a function? Here comes the gradient in the picture. Gradient shows how steep the slope on the function is at a given point. This is why we call this method “gradient descent.”</p><figure name="4e3b" id="4e3b" class="graf graf--figure graf-after--p"><img class="graf-image" data-image-id="0*1X2naEd5RM3kZj0I.png" data-width="600" data-height="375" src="https://cdn-images-1.medium.com/max/800/0*1X2naEd5RM3kZj0I.png"><figcaption class="imageCaption">source: <a href="https://www.youtube.com/watch?v=i1gGsE66b5s&amp;t=3138s" data-href="https://www.youtube.com/watch?v=i1gGsE66b5s&amp;t=3138s" class="markup--anchor markup--figure-anchor" rel="noopener" target="_blank"><em class="markup--em markup--figure-em">Andrej Karpathy’s Stanford Course Lecture 3</em></a></figcaption></figure><p name="dfed" id="dfed" class="graf graf--p graf-after--figure">The gradient at the given point can be calculated by partial derivation. A function has to satisfy some requirements to be derivable. If we want to use gradient descent for optimization, we have to use these types of functions. If the functions are derivable, then the chain of functions will also be derivable, and there is an algorithmic method to calculate the gradient.</p><p name="84ac" id="84ac" class="graf graf--p graf-after--p">Now we have all the knowledge to understand how <a href="https://www.tensorflow.org/" data-href="https://www.tensorflow.org/" class="markup--anchor markup--p-anchor" rel="noopener" target="_blank">TensorFlow</a> and <a href="https://pytorch.org/" data-href="https://pytorch.org/" class="markup--anchor markup--p-anchor" rel="noopener" target="_blank">PyTorch</a> (the two most popular machine learning frameworks) find the correct parameters for our expression.</p><p name="e431" id="e431" class="graf graf--p graf-after--p">First, let’s look at TensorFlow.</p><p name="4417" id="4417" class="graf graf--p graf-after--p">In Tensorflow, there is a gradient registry where gradient functions are registered for the operators by the <code class="markup--code markup--p-code"><a href="https://www.tensorflow.org/api_docs/python/tf/RegisterGradient" data-href="https://www.tensorflow.org/api_docs/python/tf/RegisterGradient" class="markup--anchor markup--p-anchor" rel="noopener" target="_blank">RegisterGradient</a></code> method. In the learning phase, in every step, a <code class="markup--code markup--p-code"><a href="https://www.tensorflow.org/api_docs/python/tf/GradientTape" data-href="https://www.tensorflow.org/api_docs/python/tf/GradientTape" class="markup--anchor markup--p-anchor" rel="noopener" target="_blank">GradientTape</a></code> has to be started. <code class="markup--code markup--p-code">GradientTape</code> is something like a video recorder. When TensorFlow does any operation, the gradient tape logs it.</p><p name="d452" id="d452" class="graf graf--p graf-after--p">After the end of the forward phase (when the output is generated from the input), we stop the gradient tape that goes backward on the log by using the error and calculating the gradients using the registered gradient functions. We can modify the parameters and repeat the process until we reach the minimum error by using the gradients.</p><p name="33e7" id="33e7" class="graf graf--p graf-after--p">Let’s look at the code in Python:</p><figure name="7851" id="7851" class="graf graf--figure graf--iframe graf-after--p"><script src="https://gist.github.com/TheBojda/6133e24a17da2c79117a632332953fcf.js"></script></figure><p name="e99c" id="e99c" class="graf graf--p graf-after--figure">This code shows how we can solve the simplest problem (linear regression) by using gradient descent in TensorFlow. The aim of linear regression is to find the parameters of a line that is closest to each point, where closest means that the sum of squares of the distances is minimal.</p><p name="691a" id="691a" class="graf graf--p graf-after--p">TensorFlow uses tensor operations for the calculations. Tensors are generalizations of matrices. From the programmer’s perspective, tensors are simple arrays. A zero-dimensional tensor is a scalar, a one-dimensional tensor is a vector, a two-dimensional tensor is a matrix, and from third dimension, tensors are simply tensors. Tensor operations can be done in parallel to run efficiently, especially on GPUs or TPUs.</p><p name="03d8" id="03d8" class="graf graf--p graf-after--p">At the beginning of the code, we define our model, which is a linear expression. It has two scalar parameters, <code class="markup--code markup--p-code">W</code> and <code class="markup--code markup--p-code">b</code>, and the expression is <code class="markup--code markup--p-code">y=W*x+b</code>. The default value of <code class="markup--code markup--p-code">W</code> is <code class="markup--code markup--p-code">16</code>, and<code class="markup--code markup--p-code"> b</code> is <code class="markup--code markup--p-code">10</code>. This is in our black box, our code will change the <code class="markup--code markup--p-code">W</code> and the <code class="markup--code markup--p-code">b</code> to minimize the error. Real-world models have millions or billions of parameters, but these two parameters are enough to understand the method.</p><p name="9461" id="9461" class="graf graf--p graf-after--p">From line 21 to line 23, we define the random point set. The <code class="markup--code markup--p-code">tf.random.normal</code> method generates a vector with 1,000 random numbers in a normal distribution, and we use that to generate the points near a line.</p><p name="aaf0" id="aaf0" class="graf graf--p graf-after--p">The loss function is defined in line 34. <code class="markup--code markup--p-code">y</code> and <code class="markup--code markup--p-code">y_pred</code> parameters are vectors. <code class="markup--code markup--p-code">y_pred</code> is the actual output of our model, and <code class="markup--code markup--p-code">y</code> is the expected output. The square function calculates the square of every vector element, and the output is also a vector with the squares. The <code class="markup--code markup--p-code">reduce_mean</code> function calculates the mean of elements, and its result is a scalar. This is the error itself that we want to minimize.</p><p name="4bed" id="4bed" class="graf graf--p graf-after--p">The gradient descent is from line 36 to line 42. This is the essence of the code where the learning happens. The <code class="markup--code markup--p-code">with</code> in line 37 is a Python expression. It calls the parameter object’s <code class="markup--code markup--p-code">__enter__</code> method at the beginning of the block and <code class="markup--code markup--p-code">__exit__</code> at the end.</p><p name="b5d5" id="b5d5" class="graf graf--p graf-after--p">In the case of <code class="markup--code markup--p-code">GradientTape</code>, the<code class="markup--code markup--p-code"> __enter__</code> method starts the recording, and <code class="markup--code markup--p-code">__exit__ </code>stops it. In the block (line 38), we calculate the model output by <code class="markup--code markup--p-code">model(X)</code> and the error. In line 40, the <code class="markup--code markup--p-code">GradientTape</code> calculates the gradients for the parameters (<code class="markup--code markup--p-code">dW</code> and <code class="markup--code markup--p-code">db</code>), and in lines 41 and 42, modify the parameters.</p><p name="e62b" id="e62b" class="graf graf--p graf-after--p">There are different optimization strategies. We are using the simplest, where the gradients are multiplied by a fixed learning rate (<code class="markup--code markup--p-code">lr</code>).</p><p name="cc30" id="cc30" class="graf graf--p graf-after--p">In a nutshell, this is how gradient descent and TensorFlow’s <code class="markup--code markup--p-code">GradientTape</code> work. You can find many <a href="https://www.tensorflow.org/tutorials" data-href="https://www.tensorflow.org/tutorials" class="markup--anchor markup--p-anchor" rel="noopener" target="_blank">tutorials on TensorFlow’s webpage</a>. Neural networks for image recognition, reinforcement learning, etc., but keep in your mind, there are always tensor operations and a GradientTape.</p><p name="e2c0" id="e2c0" class="graf graf--p graf-after--p">Now, let’s see how gradient descent works in the other big framework, PyTorch.</p><p name="c386" id="c386" class="graf graf--p graf-after--p">PyTorch uses the <a href="https://pytorch.org/tutorials/beginner/blitz/autograd_tutorial.html" data-href="https://pytorch.org/tutorials/beginner/blitz/autograd_tutorial.html" class="markup--anchor markup--p-anchor" rel="noopener" target="_blank">autograd</a> system for gradient calculation, which is embedded into the torch tensors. If a tensor is a result of an operator, it contains a back pointer to the operator and the source tensors. The source tensors also contain back pointers, etc., and the full operator chain is traceable.</p><p name="e50d" id="e50d" class="graf graf--p graf-after--p">Every operator can calculate its own gradient. When you call the backward method on the last tensor, it goes back on the chain and calculates the gradients to the tensors.</p><p name="6cb5" id="6cb5" class="graf graf--p graf-after--p">Let’s see the previous linear regression code in PyTorch:</p><figure name="e7ee" id="e7ee" class="graf graf--figure graf--iframe graf-after--p"><script src="https://gist.github.com/TheBojda/7b3e9dbbfc66afe9c489c72ba66450c5.js"></script></figure><p name="11a7" id="11a7" class="graf graf--p graf-after--figure">The model and the loss part are very similar to TensorFlow. You find the difference in the <code class="markup--code markup--p-code">train</code> method from line 37 to line 46. After the calculation of the <code class="markup--code markup--p-code">current_loss</code> tensor, we call the backward method on it. It recursively goes back on the chain and calculates the gradient for the <code class="markup--code markup--p-code">W</code> and <code class="markup--code markup--p-code">b</code> tensors.</p><p name="54c3" id="54c3" class="graf graf--p graf-after--p">From line 41 to line 43, we modify the <code class="markup--code markup--p-code">W</code> and <code class="markup--code markup--p-code">b</code> tensors. It’s important that this calculation is in a <code class="markup--code markup--p-code">torch.no_grad()</code> block. The <code class="markup--code markup--p-code">no_grad()</code> method temporarily disables the gradient calculation for the operators, which is not needed when we modify the parameters.</p><p name="9c97" id="9c97" class="graf graf--p graf-after--p">At the end of the <code class="markup--code markup--p-code">train</code> method calling the zero method clears the gradients. Without this, PyTorch will sum up the gradients, which results in strange behavior. The other parts of the code are very similar to TensorFlow. Like TensorFlow, PyTorch also has good tutorials, community, and documentation.</p><p name="a505" id="a505" class="graf graf--p graf-after--p">You will find everything to build any neural network, but the most important part is the autograd system, which is the base of the training.</p><p name="1096" id="1096" class="graf graf--p graf-after--p graf--trailing">Next time, when you see a picture that DALL-E generates, a car that drives itself, or simply wonder how Google Assistant understands what you say, you will know how the magic works, and how an algorithm (the gradient descent) wrote these cool algorithms for us.</p></div></div></section>
</section>
<footer><p>By <a href="https://medium.com/@thebojda" class="p-author h-card">Laszlo Fazekas</a> on <a href="https://medium.com/p/8f85cd04979d"><time class="dt-published" datetime="2022-05-25T02:21:25.417Z">May 25, 2022</time></a>.</p><p><a href="https://medium.com/@thebojda/how-machines-can-learn-using-tensorflow-or-pytorch-8f85cd04979d" class="p-canonical">Canonical link</a></p><p>Exported from <a href="https://medium.com">Medium</a> on September 27, 2022.</p></footer></article></body></html>