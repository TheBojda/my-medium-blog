<!DOCTYPE html><html><head><meta http-equiv="Content-Type" content="text/html; charset=utf-8"><title>Tensorflow alapozó 9: PyTorch alapozó</title><style>
      * {
        font-family: Georgia, Cambria, "Times New Roman", Times, serif;
      }
      html, body {
        margin: 0;
        padding: 0;
      }
      h1 {
        font-size: 50px;
        margin-bottom: 17px;
        color: #333;
      }
      h2 {
        font-size: 24px;
        line-height: 1.6;
        margin: 30px 0 0 0;
        margin-bottom: 18px;
        margin-top: 33px;
        color: #333;
      }
      h3 {
        font-size: 30px;
        margin: 10px 0 20px 0;
        color: #333;
      }
      header {
        width: 640px;
        margin: auto;
      }
      section {
        width: 640px;
        margin: auto;
      }
      section p {
        margin-bottom: 27px;
        font-size: 20px;
        line-height: 1.6;
        color: #333;
      }
      section img {
        max-width: 640px;
      }
      footer {
        padding: 0 20px;
        margin: 50px 0;
        text-align: center;
        font-size: 12px;
      }
      .aspectRatioPlaceholder {
        max-width: auto !important;
        max-height: auto !important;
      }
      .aspectRatioPlaceholder-fill {
        padding-bottom: 0 !important;
      }
      header,
      section[data-field=subtitle],
      section[data-field=description] {
        display: none;
      }
      </style></head><body><article class="h-entry">
<header>
<h1 class="p-name">Tensorflow alapozó 9: PyTorch alapozó</h1>
</header>
<section data-field="subtitle" class="p-summary">
A Google által fejlesztett Tensorflow mellett a másik nagy, ugyancsak Python alapú gépi tanulás rendszer a Facebook által fejlesztett…
</section>
<section data-field="body" class="e-content">
<section name="830f" class="section section--body section--first section--last"><div class="section-divider"><hr class="section-divider"></div><div class="section-content"><div class="section-inner sectionLayout--insetColumn"><h3 name="ecf1" id="ecf1" class="graf graf--h3 graf--leading graf--title">Tensorflow alapozó 9: PyTorch alapozó</h3><figure name="48c1" id="48c1" class="graf graf--figure graf-after--h3"><img class="graf-image" data-image-id="1*VSQ0XEywxSgZBwW05GsZtw.png" data-width="691" data-height="449" src="https://cdn-images-1.medium.com/max/800/1*VSQ0XEywxSgZBwW05GsZtw.png"><figcaption class="imageCaption">Forrás: <a href="https://www.educative.io/edpresso/what-is-pytorch" data-href="https://www.educative.io/edpresso/what-is-pytorch" class="markup--anchor markup--figure-anchor" rel="noopener" target="_blank">https://www.educative.io/edpresso/what-is-pytorch</a></figcaption></figure><p name="87d2" id="87d2" class="graf graf--p graf-after--figure">A Google által fejlesztett Tensorflow mellett a másik nagy, ugyancsak Python alapú gépi tanulás rendszer a Facebook által fejlesztett PyTorch. Ha csak a GitHub csillagok számát nézzük, a Tensorflow népszerűbbnek mondható, de PyTorch-ot használ például a Tesla vagy az OpenAI. Felmerülhet a kérdés, hogy mit keres a “konkurens” PyTorch ismertetője egy alapvetően Tensorflowról szóló cikksorozatban? Nos, a két rendszer nagyon hasonló, így akinek sikerült megérteni a Tensorflow logikáját, az nagyon könnyen beletanulhat a PyTorch használatába is. Ez pedig nagyon hasznos tudás, hiszen sokszor lehet szükség arra, hogy értelmezzünk egy PyTorch-al készült kódot, vagy akár mi fejlesszünk PyTorch-ban (pl.: hozzá akarunk járulni egy PyTorch-ban készült projekthez, vagy egyszerűen csak jobban megkedveljük ezt a rendszert, mint a Tensorflow-t). Ez a cikk akár tekinthető különálló PyTorch bevezetőnek is, de mivel a Tensorflow esetén már megtanult dolgokra fogok hivatkozni, ezért elengedhetetlen hozzá legalább az első 2 rész elolvasása:</p><div name="4fab" id="4fab" class="graf graf--mixtapeEmbed graf-after--p"><a href="https://medium.com/@thebojda/tensorflow-alapoz%C3%B3-d2d1ee97c9db" data-href="https://medium.com/@thebojda/tensorflow-alapoz%C3%B3-d2d1ee97c9db" class="markup--anchor markup--mixtapeEmbed-anchor" title="https://medium.com/@thebojda/tensorflow-alapoz%C3%B3-d2d1ee97c9db"><strong class="markup--strong markup--mixtapeEmbed-strong">TensorFlow alapozó</strong><br><em class="markup--em markup--mixtapeEmbed-em">(neurális hálózatok, tenzorok és képfelismerés a gyakorlatban)</em>medium.com</a><a href="https://medium.com/@thebojda/tensorflow-alapoz%C3%B3-d2d1ee97c9db" class="js-mixtapeImage mixtapeImage u-ignoreBlock" data-media-id="15660260b8dadb0ef12eafefd0be7499" data-thumbnail-img-id="1*T4ARzySpEQvEnr_9pc78pg.jpeg" style="background-image: url(https://cdn-images-1.medium.com/fit/c/160/160/1*T4ARzySpEQvEnr_9pc78pg.jpeg);"></a></div><div name="7ebf" id="7ebf" class="graf graf--mixtapeEmbed graf-after--mixtapeEmbed"><a href="https://medium.com/@thebojda/tensorflow-alapoz%C3%B3-2-14720a33aca" data-href="https://medium.com/@thebojda/tensorflow-alapoz%C3%B3-2-14720a33aca" class="markup--anchor markup--mixtapeEmbed-anchor" title="https://medium.com/@thebojda/tensorflow-alapoz%C3%B3-2-14720a33aca"><strong class="markup--strong markup--mixtapeEmbed-strong">Tensorflow alapozó 2.</strong><br><em class="markup--em markup--mixtapeEmbed-em">(backpropagation, avagy hogyan működik a varázslat)</em>medium.com</a><a href="https://medium.com/@thebojda/tensorflow-alapoz%C3%B3-2-14720a33aca" class="js-mixtapeImage mixtapeImage u-ignoreBlock" data-media-id="acf8cadc94685ee340590198b4aca40e" data-thumbnail-img-id="1*YTDwPXrnfbPndXxNw77O-w.png" style="background-image: url(https://cdn-images-1.medium.com/fit/c/160/160/1*YTDwPXrnfbPndXxNw77O-w.png);"></a></div><p name="4346" id="4346" class="graf graf--p graf-after--mixtapeEmbed">Ennyi bevezető után csapjunk is a lovak közé és lássuk milyen alapelemekből épül fel a PyTorch.</p><h4 name="3554" id="3554" class="graf graf--h4 graf-after--p">Tenzorok</h4><p name="c077" id="c077" class="graf graf--p graf-after--h4">A Tensorflowhoz hasonlóan a PyTorch is tenzor műveletek sorozatára képezi le a neurális hálózatokat, így ennek a rendszernek is a tenzorok képezik az alapját. A Tensorflowhoz hasonlóan létrehozhatunk új tenzorokat valamilyen kezdő értékkel inicializálva, de konvertálhatunk NumPy tömböket is oda-vissza tenzorokká. A PyTorchban is elérhető minden alap tenzor művelet akár operátor overloading-on keresztül (két tenzor közti szorzás tenzor szorzást jelent, stb.) akár direktben a függvényt hívva. A tenzor létrehozásakor, vagy később a <em class="markup--em markup--p-em">.to()</em> függvény hívásával megadhatjuk, hogy a tenzor a CPU-t vagy a GPU-t használja. Az utóbbi esetben minden tenzor művelet a GPU-n fog futni, ami mint tudjuk sokkal gyorsabb végrehajtást eredményez. Nagy vonalakban talán ennyi elég is a tenzorokról, mivel minden szempontból úgy működnek, mint a Tensorflow tenzorai. (Akit részletesebben érdekel a dolog, olvassa el a <a href="https://pytorch.org/tutorials/beginner/blitz/tensor_tutorial.html#sphx-glr-beginner-blitz-tensor-tutorial-py" data-href="https://pytorch.org/tutorials/beginner/blitz/tensor_tutorial.html#sphx-glr-beginner-blitz-tensor-tutorial-py" class="markup--anchor markup--p-anchor" rel="noopener" target="_blank">PyTorch tutorial tenzorokról szóló fejezetét</a>.)</p><h4 name="b466" id="b466" class="graf graf--h4 graf-after--p">Deriváltak számítása és backpropagation</h4><p name="6a27" id="6a27" class="graf graf--p graf-after--h4">Aki olvasta a <a href="https://medium.com/@thebojda/tensorflow-alapoz%C3%B3-2-14720a33aca" data-href="https://medium.com/@thebojda/tensorflow-alapoz%C3%B3-2-14720a33aca" class="markup--anchor markup--p-anchor" target="_blank">cikksorozat második részét</a>, az tudja, hogy a neurális hálók tantásának alapja a backpropagation (hiba visszaterjesztés), amikor a hálózat hibája alapján kiszámtjuk a deriváltakat, majd ezek alapján módosítjuk a hálózat súlyait. Tensorflow esetén a <em class="markup--em markup--p-em">GradientTape</em> osztály szolgált arra, hogy rögzítse a tenzor műveleteket, majd a hiba meghatározását követően visszagörgetve kiszámolja a deriváltakat. PyTorch esetén is létezik egy hasonló mechanizmus, aminek autograd a neve de a Tensorflowval ellentétben nem külön osztály valósítja meg, hanem a <em class="markup--em markup--p-em">Tensor</em> osztályba került beépítésre. A <em class="markup--em markup--p-em">Tensor</em> osztálynak van egy <em class="markup--em markup--p-em">requires_grad</em> tulajdonsága. Ha ez <em class="markup--em markup--p-em">True</em>-ra van álltva, a rendszer rögzíti a tenzoron elvégzett műveleteket. Ha egy ilyen követett tenzoron elvégzünk egy összeadást, akkor az eredmény tenzor <em class="markup--em markup--p-em">grad_fn</em> tulajdonságába bekerül egy hivatkozás a műveletet végző függvényre. Tehát minden tenzor ami valamilyen művelet eredményeként jött létre, tartalmazni fog egy hivatkozást az őt létrehozó függvényre, ami pedig a forrás tenzorokra tartalmaz hivatkozásokat. Nézzünk is gyorsan egy példát:</p><pre name="49c2" id="49c2" class="graf graf--pre graf-after--p">x = torch.ones(2, 2, requires_grad=True)<br>print(x)<br>y = x + 2<br>print(y)</pre><p name="0dd7" id="0dd7" class="graf graf--p graf-after--pre">A fenti Python kód létrehoz egy 2x2-es mátrixot (tenzort) aminek be van álltva a <em class="markup--em markup--p-em">requires_grad</em> tulajdonsága, majd ehhez hozzáad kettőt. Ha kiiratjuk a két tenzort, így fog kinézni az output:</p><pre name="be26" id="be26" class="graf graf--pre graf-after--p">tensor([[1., 1.], [1., 1.]], requires_grad=True) <br>tensor([[3., 3.], [3., 3.]], grad_fn=&lt;AddBackward0&gt;)</pre><p name="a468" id="a468" class="graf graf--p graf-after--pre">Látszik, hogy az y tenzor ami az összeadás eredményeként jött létre, tartalmaz egy visszahivatkozást az őt létrehozó műveletre (AddBackward0). Ezeken a visszahivatkozásokon keresztül visszakövethető a művelet folyam egészen a legelső tenzorig, így ki tudjuk számolni a deriváltakat. A deriváltak számításához az eredmény tenzoron meg kell hívnunk a <em class="markup--em markup--p-em">backward</em> függvényt, ami rekurzívan visszafut a láncon és minden tenzorhoz kiszámolja a deriváltakat, ami a tenzor <em class="markup--em markup--p-em">grad</em> mezőjébe kerül. Nézzünk erre egy példát! A cikksorozat második részében található lineáris modell tantást fogjuk portolni PyTorch-ra:</p><figure name="bcee" id="bcee" class="graf graf--figure graf--iframe graf-after--p"><script src="https://gist.github.com/TheBojda/57bdc551ac62f774dce09c0dc2a48ea9.js"></script><figcaption class="imageCaption"><a href="https://gist.github.com/TheBojda/57bdc551ac62f774dce09c0dc2a48ea9" data-href="https://gist.github.com/TheBojda/57bdc551ac62f774dce09c0dc2a48ea9" class="markup--anchor markup--figure-anchor" rel="noopener" target="_blank">https://gist.github.com/TheBojda/57bdc551ac62f774dce09c0dc2a48ea9</a></figcaption></figure><p name="269b" id="269b" class="graf graf--p graf-after--figure">A kód elején kigeneráljuk az 1000 elemű ponthalmazt, amire a <em class="markup--em markup--p-em">W</em> és <em class="markup--em markup--p-em">b</em> változók által defeiniált egyenest fogjuk illeszteni. A modell változók ebben az esetben 1 elemű tenzorok. Mindkét tenzor esetén a <em class="markup--em markup--p-em">requires_grad</em> tulajdonság értéke <em class="markup--em markup--p-em">True</em>, mivel szeretnénk, ha a rendszer ezekre a tenzorokra kiszámolná a deriváltakat. Az egyszerűség kedvéért definiálunk egy <em class="markup--em markup--p-em">model</em> függvényt, hogy ne kelljen minden esetben kiírni a képletet, majd pyplot segítségével megjelenítjük a ponthalmazt és az egyenest. A lényeg ezután következik a for ciklusban. A model futtatásának eredménye az <em class="markup--em markup--p-em">y_pred</em> tenzorba kerül, majd kiszámoljuk a négyzetes hibát, amit a <em class="markup--em markup--p-em">loss</em> tenzorban tárolunk. A <em class="markup--em markup--p-em">loss</em> tenzor tehát tartalmazni fogja a teljes műveletlistát a <em class="markup--em markup--p-em">W</em> és <em class="markup--em markup--p-em">b</em> tenzorokig visszamenően. A <em class="markup--em markup--p-em">loss.backward()</em> hívással kiszámoltatjuk a deriváltakat, amik a <em class="markup--em markup--p-em">W.grad</em> és <em class="markup--em markup--p-em">b.grad</em> tenzorokba kerülnek. A súlyok módostása a <em class="markup--em markup--p-em">with torch.no_grad()</em> blokkon belül történik. A <em class="markup--em markup--p-em">no_grad()</em> függvény segítségével közölhetjük a rendszerrel, hogy a blokkon belül nincs szükség a tenzor műveletek követésére. Olyan mint ha lokálisan kikapcsolnánk a tenzorok <em class="markup--em markup--p-em">requires_grad</em> tulajdonságát. A következő két sor módosítja a <em class="markup--em markup--p-em">W</em> és <em class="markup--em markup--p-em">b</em> változók értékét a deriváltaknak megfelelően, majd az ezt követő 2 sor nullázza a deriváltakat. Ez utóbbira mindig érdemes odafigyelni. Gyakori hiba, hogy valaki elfelejti nullázni a deriváltakat, így azok összegződnek, így tanítás értelemszerűen nem vezet eredményre. A fenti példából jól látható, hogy hogyan működik a PyTorch “tenzorokba éptett” autograd rendszere. (Akit részletesebben érdekel a dolog, olvassa el a <a href="https://pytorch.org/tutorials/beginner/blitz/autograd_tutorial.html" data-href="https://pytorch.org/tutorials/beginner/blitz/autograd_tutorial.html" class="markup--anchor markup--p-anchor" rel="noopener" target="_blank">PyTorch tutorial autograd fejezetét</a>.)</p><h4 name="67b7" id="67b7" class="graf graf--h4 graf-after--p">Neurális hálózatok</h4><p name="e93f" id="e93f" class="graf graf--p graf-after--h4">PyTorchban úgy hozhatunk létre neurális hálót, hogy az <em class="markup--em markup--p-em">nn</em><strong class="markup--strong markup--p-strong"><em class="markup--em markup--p-em">.</em></strong><em class="markup--em markup--p-em">Module</em>-ból származtatunk egy saját osztályt, ahol a konstruktorban definiáljuk a paraméterezhető rétegeket, a forward metódusban pedig a teljes hálózatot.</p><pre name="28eb" id="28eb" class="graf graf--pre graf-after--p">class Net(nn.Module):<br>  <br>  def __init__(self):<br>    super(Net, self).__init__()<br>    self.conv1 = nn.Conv2d(3, 64, 3)<br>    self.conv2 = nn.Conv2d(64, 64, 3)<br>    self.conv3 = nn.Conv2d(64, 64, 3)<br>    self.fc1 = nn.Linear(1024, 64)<br>    self.fc2 = nn.Linear(64, 10)</pre><pre name="6114" id="6114" class="graf graf--pre graf-after--pre">def forward(self, x):<br>    x = F.max_pool2d(F.relu(self.conv1(x)), (2, 2))<br>    x = F.max_pool2d(F.relu(self.conv2(x)), 2)<br>    x = F.relu(self.conv3(x))<br>    x = x.view(-1, 1024)<br>    x = F.relu(self.fc1(x))<br>    x = F.softmax(self.fc2(x))<br>    return x</pre><pre name="b755" id="b755" class="graf graf--pre graf-after--pre">net = Net()<br>print(net)</pre><pre name="94fb" id="94fb" class="graf graf--pre graf-after--pre">params = list(net.parameters())<br>print(len(params))</pre><p name="2ce0" id="2ce0" class="graf graf--p graf-after--pre">A fenti kód egy egyszerű 3 konvolúciós és 2 teljesen kapcsolt rétegből álló hálózat. A paraméterezhető rétegeket a konstruktorban definiáljuk, ahol minden egyes réteg az objektum egy belső változója (ennek később jelentősége lesz). A <em class="markup--em markup--p-em">forward</em> metódusban definiáljuk a teljes hálózatot függvény hívások sorozataként. Az utolsó pár sorban létrehozunk egy példányt a neurális hálózatunkból és lekérdezzük a paraméterek számát. A neurális háló állítható paramétereit a <em class="markup--em markup--p-em">parameters()</em> függvény adja vissza egy tenzor lista formájában. A függvény úgy működik, hogy végigszalad az objektum belső változóin és innen gyűjti össze a paraméter tenzor hivatkozásokat. Ezért fontos, hogy minden egyes paraméterezhető réteget belső változóként hozzunk létre a konstruktorban. Erről meg is győződhetünk. Ha végigkommenteljük a konstruktorban a belső változó definíciókat, a <em class="markup--em markup--p-em">parameters()</em> visszatérési értéke üres lista lesz. A fenti hálózat esetén egyébként a <em class="markup--em markup--p-em">parameters()</em> 10-et ad vissza, mivel mind az 5 réteghez tartozik egy súly tenzor és külön egy tenzor, ami a bias-t tartalmazza. Ha a rétegeket <em class="markup--em markup--p-em">bias=False</em> módosítóval hoztuk volna létre, úgy a <em class="markup--em markup--p-em">parameters()</em> függvény egy 5 elemű listát adna vissza.</p><p name="e4fc" id="e4fc" class="graf graf--p graf-after--p">A Tensorflowhoz hasonlóan PyTorch esetén is létezik egy Sequential objektum, amivel egyszerűen definiálhatunk szekvenciális hálózatokat. A fenti mintahálózat például így néz ki a Sequential használatával:</p><pre name="2f13" id="2f13" class="graf graf--pre graf-after--p">net = nn.Sequential(<br>    nn.Conv2d(3, 6, 5),<br>    nn.MaxPool2d(2, 2),<br>    nn.Conv2d(6, 16, 5),<br>    nn.MaxPool2d(2, 2),<br>    nn.Flatten(),<br>    nn.Linear(16 * 5 * 5, 120),<br>    nn.ReLU(),<br>    nn.Linear(120, 84),<br>    nn.ReLU(),<br>    nn.Linear(84, 10)<br>)</pre><p name="19a5" id="19a5" class="graf graf--p graf-after--pre">Ennyi alapozás után lássuk a teljes kódot, ami a szokásos CIFAR10-es mintahalmazon tanul meg képeket felismerni.</p><figure name="d182" id="d182" class="graf graf--figure graf--iframe graf-after--p"><script src="https://gist.github.com/TheBojda/29324b9e69638024d9a6513b38f442a6.js"></script></figure><p name="6263" id="6263" class="graf graf--p graf-after--figure">A kód első pár sora a torchvision könyvtár segítségével betölti a CIFAR10-es adathalmazt, valamint tenzor formára hozza azt. Mivel 4 elemű batcheket definiáltunk, ezért [4, 3, 32, 32] formájú tenzorok lesznek a hálózat bemenete. A következő pár sor pyplot segítségével kirajzolja az első batch-t (első 4 képet), valamint kiírja a képekhez tartozó címkéket. Ezt követi a hálózat definíciója, amit a fenti minta alapján már könnyen értelmezhetünk. A 68. sorban ellenőrizzük, hogy a futtató hardveren van-e cuda támogatás. Ha van, akkor GPU-n fogjuk futtatni a tanítást, ha nincs, akkor CPU-n. Ahogyan az egyes tenzorokat, úgy a teljes neurális hálót is a <em class="markup--em markup--p-em">.to()</em> metódus hívásával tudjuk GPU-hoz rendelni. Ezt követi a hibafüggvény és a tantó algoritmus definíciója. Hibafüggvénynek az osztályozásnál megszokott <em class="markup--em markup--p-em">CrossEntropyLoss</em>-t használjuk, tanító algoritmusnak pedig a már ismerős <em class="markup--em markup--p-em">Adam</em>-et. A tanító algoritmus paraméterként kapja meg a hálózat paraméter tenzor listáját. Erre azért van szükség, mivel a deriválás után innen lehet majd kiolvasni a tenzorokhoz tartozó deriváltakat, valamint ezen keresztül módosíthatóak maguk a tenzorok. A tantás az egymásba ágyazott for ciklusokban történik. A teljes mintahalmazt kétszer futtatjuk át a hálózaton (külső ciklus). A belső ciklusban végigmegyünk a batch-eken. Minden batch 4 képet és ehhez tartozó 4 cimkét tartalmaz tenzor formában. Mivel a futtatás opcionálisan GPU-n történik, ezért fontos, hogy a bemeneti tenzorok is ugyanahhoz a device-hoz legyenek rendelve, amihez a teljes hálózat. Erre szolgál a 79.-es sorban a <em class="markup--em markup--p-em">.to()</em> hívás a <em class="markup--em markup--p-em">data</em> tömb elemein. Az <em class="markup--em markup--p-em">optimizer.zero_grad()</em> nullázza a deriváltakat, majd lefuttatjuk a hálózatot és kiszámoljuk a hibát. A hiba tenzoron meghívjuk a <em class="markup--em markup--p-em">backward</em> függvényt, ami kiszámolja a deriváltakat, végül az <em class="markup--em markup--p-em">optimizer.step()</em> a tenzoroknál rögzített deriváltak alapján módosítja a tenzorokat. Lényegében ennyi a tanítás. A kód végén még van egy <em class="markup--em markup--p-em">torch.save()</em> hívás, ami menti a hálózat súlyait. (A teljes leírás itt megtalálható: <a href="https://pytorch.org/tutorials/beginner/blitz/cifar10_tutorial.html" data-href="https://pytorch.org/tutorials/beginner/blitz/cifar10_tutorial.html" class="markup--anchor markup--p-anchor" rel="noopener" target="_blank">https://pytorch.org/tutorials/beginner/blitz/cifar10_tutorial.html</a>)</p><p name="658c" id="658c" class="graf graf--p graf-after--p graf--trailing">Körülbelül ennyit szerettem volna írni a PyTorch-ról a Tensorflow fényében. Látható, hogy a két rendszer lényegében ugyanazt tudja. Mindket rendszer tenzor műveletekre épül és hasonló logika mentén működik. Különbség igazából a deriváltak kezelésében és a neurális hálózatok definiciójában van, de aki érti az egyik rendszer működését, az könnyen bele tud tanulni a másik rendszer használatába is.</p></div></div></section>
</section>
<footer><p>By <a href="https://medium.com/@thebojda" class="p-author h-card">Laszlo Fazekas</a> on <a href="https://medium.com/p/ee4e9d98b46b"><time class="dt-published" datetime="2020-08-24T09:05:38.937Z">August 24, 2020</time></a>.</p><p><a href="https://medium.com/@thebojda/tensorflow-alapoz%C3%B3-9-pytorch-alapoz%C3%B3-ee4e9d98b46b" class="p-canonical">Canonical link</a></p><p>Exported from <a href="https://medium.com">Medium</a> on March 9, 2021.</p></footer></article></body></html>