<!DOCTYPE html><html><head><meta http-equiv="Content-Type" content="text/html; charset=utf-8"><title>How LLMs like ChatGPT can use plugins and tools</title><style>
      * {
        font-family: Georgia, Cambria, "Times New Roman", Times, serif;
      }
      html, body {
        margin: 0;
        padding: 0;
      }
      h1 {
        font-size: 50px;
        margin-bottom: 17px;
        color: #333;
      }
      h2 {
        font-size: 24px;
        line-height: 1.6;
        margin: 30px 0 0 0;
        margin-bottom: 18px;
        margin-top: 33px;
        color: #333;
      }
      h3 {
        font-size: 30px;
        margin: 10px 0 20px 0;
        color: #333;
      }
      header {
        width: 640px;
        margin: auto;
      }
      section {
        width: 640px;
        margin: auto;
      }
      section p {
        margin-bottom: 27px;
        font-size: 20px;
        line-height: 1.6;
        color: #333;
      }
      section img {
        max-width: 640px;
      }
      footer {
        padding: 0 20px;
        margin: 50px 0;
        text-align: center;
        font-size: 12px;
      }
      .aspectRatioPlaceholder {
        max-width: auto !important;
        max-height: auto !important;
      }
      .aspectRatioPlaceholder-fill {
        padding-bottom: 0 !important;
      }
      header,
      section[data-field=subtitle],
      section[data-field=description] {
        display: none;
      }
      </style></head><body><article class="h-entry">
<header>
<h1 class="p-name">How LLMs like ChatGPT can use plugins and tools</h1>
</header>
<section data-field="subtitle" class="p-summary">
Spoken language is only the first step because anything can be tokenized
</section>
<section data-field="body" class="e-content">
<section name="5de3" class="section section--body section--first section--last"><div class="section-divider"><hr class="section-divider"></div><div class="section-content"><div class="section-inner sectionLayout--insetColumn"><h3 name="e6d2" id="e6d2" class="graf graf--h3 graf--leading graf--title">How LLMs Like ChatGPT Can Use Plugins andÂ Tools</h3><h4 name="0420" id="0420" class="graf graf--h4 graf-after--h3 graf--subtitle">Spoken language is only the first step because anything can be tokenized</h4><figure name="dd2f" id="dd2f" class="graf graf--figure graf-after--h4"><img class="graf-image" data-image-id="1*da_MXY2mFyGU9I1-AAhOUQ.png" data-width="1024" data-height="1024" data-is-featured="true" src="https://cdn-images-1.medium.com/max/800/1*da_MXY2mFyGU9I1-AAhOUQ.png"><figcaption class="imageCaption">Generated by Midjourney</figcaption></figure><p name="16e0" id="16e0" class="graf graf--p graf-after--figure">Large Language Models (LLMs), such as ChatGPT, have made a significant impact, although they have some very strong limitations.</p><p name="7552" id="7552" class="graf graf--p graf-after--p">One of these limitations is that these models are prewired. This means they are trained on a big set of documents, so they have a very big knowledge, but they cannot learn new things (I have a <a href="https://medium.com/better-programming/how-machines-can-learn-using-tensorflow-or-pytorch-8f85cd04979d" data-href="https://medium.com/better-programming/how-machines-can-learn-using-tensorflow-or-pytorch-8f85cd04979d" class="markup--anchor markup--p-anchor" target="_blank">full article</a> about how neural networks are trained). From this perspective, the training of neural networks is much more similar to instincts evolved by evolution than when we learn new things in school. But if they cannot learn new things, how can they use tools like calculators or do a Google search?</p><p name="2a67" id="2a67" class="graf graf--p graf-after--p">To investigate how this works, I made a small Vue.js app by using <a href="https://js.langchain.com/docs/" data-href="https://js.langchain.com/docs/" class="markup--anchor markup--p-anchor" rel="noopener" target="_blank">LangChain.js</a>. LangChain.js is a wrapper library for LLMs and LLM-related things like prompts, agents, vector databases, etc. With LangChain, you can use tools with OpenAI models, which is a very similar concept to plugins in ChatGPT. Fortunately, LangChain is working fine in browsers, so if you have a browser app, you can simply check the API calls in the network tab.</p><p name="8624" id="8624" class="graf graf--p graf-after--p">My code is available <a href="https://github.com/TheBojda/langchain-tools-example" data-href="https://github.com/TheBojda/langchain-tools-example" class="markup--anchor markup--p-anchor" rel="noopener" target="_blank">here</a>. It is simple, but the code is less relevant in this case. In <a href="https://github.com/TheBojda/langchain-tools-example/blob/main/src/components/App.vue" data-href="https://github.com/TheBojda/langchain-tools-example/blob/main/src/components/App.vue" class="markup--anchor markup--p-anchor" rel="noopener" target="_blank">the code</a>, I created a LangChain agent executor that can use the calculator tool:</p><pre data-code-block-mode="2" spellcheck="false" data-code-block-lang="typescript" name="5596" id="5596" class="graf graf--pre graf-after--p graf--preV2"><span class="pre--content"><span class="hljs-keyword">const</span> model = <span class="hljs-keyword">new</span> <span class="hljs-title class_">OpenAI</span>({<br />  <span class="hljs-attr">temperature</span>: <span class="hljs-number">0</span>,<br />  <span class="hljs-attr">openAIApiKey</span>: process.<span class="hljs-property">env</span>.<span class="hljs-property">OPENAI_API_KEY</span>,<br />});<br />    <br /><span class="hljs-keyword">const</span> tools = [<span class="hljs-keyword">new</span> <span class="hljs-title class_">Calculator</span>()];<br /><span class="hljs-variable language_">this</span>.<span class="hljs-property">executor</span> = <span class="hljs-keyword">await</span> <span class="hljs-title function_">initializeAgentExecutor</span>(<br />  tools,<br />  model,<br />  <span class="hljs-string">&quot;zero-shot-react-description&quot;</span><br />);</span></pre><p name="03d5" id="03d5" class="graf graf--p graf-after--pre">I sent the following prompt to see how the tool is used:</p><pre data-code-block-mode="0" spellcheck="false" name="34c4" id="34c4" class="graf graf--pre graf-after--p graf--preV2"><span class="pre--content">How much is 10+12+33+(5*8)?</span></pre><p name="3cde" id="3cde" class="graf graf--p graf-after--pre">Langchain sent the following prompt to OpenAI text-davinci-003:</p><pre data-code-block-mode="0" spellcheck="false" name="52ff" id="52ff" class="graf graf--pre graf-after--p graf--preV2"><span class="pre--content">Answer the following questions as best you can. You have access to the <br>following tools:<br><br>calculator: Useful for getting the result of a math expression. <br>The input to this tool should be a valid mathematical expression <br>that could be executed by a simple calculator.<br><br>Use the following format:<br><br>Question: the input question you must answer<br>Thought: you should always think about what to do<br>Action: the action to take, should be one of [calculator]<br>Action Input: the input to the action<br>Observation: the result of the action<br>... (this Thought/Action/Action Input/Observation can repeat N times)<br>Thought: I now know the final answer<br>Final Answer: the final answer to the original input question<br><br>Begin!<br><br>Question: How much is 10+12+33+(5*8)?<br>Thought:</span></pre><p name="71de" id="71de" class="graf graf--p graf-after--pre">The response was the following:</p><pre data-code-block-mode="0" spellcheck="false" name="5d38" id="5d38" class="graf graf--pre graf-after--p graf--preV2"><span class="pre--content">I need to calculate the expression<br>Action: calculator<br>Action Input: 10+12+33+(5*8)</span></pre><p name="2c9b" id="2c9b" class="graf graf--p graf-after--pre">Then LangChain sent a new prompt to OpenAI with the merged content where the result was calculated by the tool:</p><pre data-code-block-mode="0" spellcheck="false" name="522c" id="522c" class="graf graf--pre graf-after--p graf--preV2"><span class="pre--content">Answer the following questions as best you can. You have access to the <br>following tools:<br><br>calculator: Useful for getting the result of a math expression. <br>The input to this tool should be a valid mathematical expression <br>that could be executed by a simple calculator.<br><br>Use the following format:<br><br>Question: the input question you must answer<br>Thought: you should always think about what to do<br>Action: the action to take, should be one of [calculator]<br>Action Input: the input to the action<br>Observation: the result of the action<br>... (this Thought/Action/Action Input/Observation can repeat N times)<br>Thought: I now know the final answer<br>Final Answer: the final answer to the original input question<br><br>Begin!<br><br>Question: How much is 10+12+33+(5*8)?<br>Thought: I need to calculate the expression<br>Action: calculator<br>Action Input: 10+12+33+(5*8)<br>Observation: 95<br>Thought:</span></pre><p name="e3c4" id="e3c4" class="graf graf--p graf-after--pre">And the final response was:</p><pre data-code-block-mode="0" spellcheck="false" name="8efc" id="8efc" class="graf graf--pre graf-after--p graf--preV2"><span class="pre--content">I now know the final answer<br>Final Answer: 95</span></pre><p name="55f9" id="55f9" class="graf graf--p graf-after--pre">LLMs have no memory and cannot learn new things. The only thing they see is the input sequence, a list of tokens. In the case of GPT-3.5, the maximum number of input tokens is 4096, and 8192 in the case of GPT-4. This number is limited, but not so little. If you want to use tools, you must write the instructions into the prompt.</p><p name="3222" id="3222" class="graf graf--p graf-after--p">When I first saw the ChatGPT plugin API, it was weird that the interface was defined with natural language. Now the reason is absolutely clear. The interface is directly given to the model to use it. It understands the description because it understands the language and generates the required output processed by the framework that calls the chosen tool.</p><p name="1168" id="1168" class="graf graf--p graf-after--p">When the tool returns the result, the framework writes it back to the prompt and sends it again to the LLM, which gives back the final answer. Thatâs all. A simple but genius solution to expand the boundaries of LLMs.</p><p name="b29c" id="b29c" class="graf graf--p graf-after--p">Todayâs models contain a lot of unnecessary lexical knowledge that is not really needed. This knowledge is unnecessary because the model can search it from the web. If you drop this unnecessary knowledge, the model&#39;s size can be radically reduced. These smaller models (like <a href="https://github.com/antimatter15/alpaca.cpp" data-href="https://github.com/antimatter15/alpaca.cpp" class="markup--anchor markup--p-anchor" rel="noopener" target="_blank">Alpaca</a>) can efficiently run on your phone or your <a href="https://github.com/tloen/alpaca-lora" data-href="https://github.com/tloen/alpaca-lora" class="markup--anchor markup--p-anchor" rel="noopener" target="_blank">Raspberry Pi</a> with relatively the same performance.</p><p name="93de" id="93de" class="graf graf--p graf-after--p">Another advantage of shrinking the model is that you can make it wider. If the number of input tokens is more, then the model can use more tools, understand the wider context, and give more accurate results.</p><p name="2002" id="2002" class="graf graf--p graf-after--p">I think the future of LLMs is bright, and using tools is a massive leap toward AGI. These models will be with us on our phones. We can talk with them through our Bluetooth headsets or <a href="https://tech.facebook.com/reality-labs/2021/7/bci-milestone-new-research-from-ucsf-with-support-from-facebook-shows-the-potential-of-brain-computer-interfaces-for-restoring-speech-communication/" data-href="https://tech.facebook.com/reality-labs/2021/7/bci-milestone-new-research-from-ucsf-with-support-from-facebook-shows-the-potential-of-brain-computer-interfaces-for-restoring-speech-communication/" class="markup--anchor markup--p-anchor" rel="noopener" target="_blank">BCI devices</a>. Spoken language is only the first step because anything can be tokenized.</p><p name="90d1" id="90d1" class="graf graf--p graf-after--p graf--trailing"><a href="https://paperswithcode.com/method/vision-transformer" data-href="https://paperswithcode.com/method/vision-transformer" class="markup--anchor markup--p-anchor" rel="noopener" target="_blank">Vision transformers</a> are using the same architecture (transformers) that is used by LLMs, and these models can be mixed as <a href="https://www.deepmind.com/publications/a-generalist-agent" data-href="https://www.deepmind.com/publications/a-generalist-agent" class="markup--anchor markup--p-anchor" rel="noopener" target="_blank">DeepMind Gato</a> shows. In the future, these models can see through our smartglasses or hear what we hear and help us as an integrated part of ourselves, and connect us to the internet through tools as a new layer of our brain. The exocortexâ¦</p></div></div></section>
</section>
<footer><p>By <a href="https://medium.com/@thebojda" class="p-author h-card">Laszlo Fazekas</a> on <a href="https://medium.com/p/2d0571869e01"><time class="dt-published" datetime="2023-05-03T22:14:56.809Z">May 3, 2023</time></a>.</p><p><a href="https://medium.com/@thebojda/how-llms-like-chatgpt-can-use-plugins-and-tools-2d0571869e01" class="p-canonical">Canonical link</a></p><p>Exported from <a href="https://medium.com">Medium</a> on May 25, 2023.</p></footer></article></body></html>