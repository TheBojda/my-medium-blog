<!DOCTYPE html><html><head><meta http-equiv="Content-Type" content="text/html; charset=utf-8"><title>Tensorflow alapozó 4.</title><style>
      * {
        font-family: Georgia, Cambria, "Times New Roman", Times, serif;
      }
      html, body {
        margin: 0;
        padding: 0;
      }
      h1 {
        font-size: 50px;
        margin-bottom: 17px;
        color: #333;
      }
      h2 {
        font-size: 24px;
        line-height: 1.6;
        margin: 30px 0 0 0;
        margin-bottom: 18px;
        margin-top: 33px;
        color: #333;
      }
      h3 {
        font-size: 30px;
        margin: 10px 0 20px 0;
        color: #333;
      }
      header {
        width: 640px;
        margin: auto;
      }
      section {
        width: 640px;
        margin: auto;
      }
      section p {
        margin-bottom: 27px;
        font-size: 20px;
        line-height: 1.6;
        color: #333;
      }
      section img {
        max-width: 640px;
      }
      footer {
        padding: 0 20px;
        margin: 50px 0;
        text-align: center;
        font-size: 12px;
      }
      .aspectRatioPlaceholder {
        max-width: auto !important;
        max-height: auto !important;
      }
      .aspectRatioPlaceholder-fill {
        padding-bottom: 0 !important;
      }
      header,
      section[data-field=subtitle],
      section[data-field=description] {
        display: none;
      }
      </style></head><body><article class="h-entry">
<header>
<h1 class="p-name">Tensorflow alapozó 4.</h1>
</header>
<section data-field="subtitle" class="p-summary">
(Reinforcement learning, Deep Q-learning és OpenAI Gym)
</section>
<section data-field="body" class="e-content">
<section name="3187" class="section section--body section--first section--last"><div class="section-divider"><hr class="section-divider"></div><div class="section-content"><div class="section-inner sectionLayout--insetColumn"><h3 name="f0b8" id="f0b8" class="graf graf--h3 graf--leading graf--title">Tensorflow alapozó 4.</h3><h4 name="2a8d" id="2a8d" class="graf graf--h4 graf-after--h3 graf--subtitle">(Reinforcement learning, Deep Q-learning és OpenAI Gym)</h4><p name="f899" id="f899" class="graf graf--p graf-after--h4">Annak idején bejárta a tech sajtót a hír, hogy a Google által felvásárolt DeepMind olyan mesterséges intelligenciát fejlesztett, ami pusztán a képernyő figyelésével képes volt megtanulni Atari játékokat játszani.</p><figure name="dae8" id="dae8" class="graf graf--figure graf-after--p"><img class="graf-image" data-image-id="1*fBRNFgr42ZsRwFEke0zoUA.jpeg" data-width="1280" data-height="720" data-is-featured="true" src="https://cdn-images-1.medium.com/max/800/1*fBRNFgr42ZsRwFEke0zoUA.jpeg"><figcaption class="imageCaption">Google DeepMind’s Deep Q-learning playing Atari Breakout Forrás: Youtube</figcaption></figure><p name="bc46" id="bc46" class="graf graf--p graf-after--figure">Annak ellenére, hogy a dolog baromi izgalmas, azért felmerül az emberben a kérdés, hogy miért vásárol fel a Google fél milliárd dollárért egy céget, ahol retro játékokhoz készítenek mesterséges intelligenciát? Nos, egy rendszer, ami képes megtanulni retro játékokat játszani, ugyanúgy képes lehet elvezérelni egy robotot is ami mondjuk homokzsákokat pakol árvíz idején, vagy éppen embereket ment ki egy földrengés helyszínéről.</p><p name="f84b" id="f84b" class="graf graf--p graf-after--p">A DeepMind által használt technikát Deep Q-learningnek nevezik, és a reinforcement learning (megerősítéses tanulás) egy formája.</p><figure name="0ea4" id="0ea4" class="graf graf--figure graf-after--p"><img class="graf-image" data-image-id="1*6KEByEw2ZlEhe8EIVgXFlw.png" data-width="300" data-height="290" src="https://cdn-images-1.medium.com/max/800/1*6KEByEw2ZlEhe8EIVgXFlw.png"><figcaption class="imageCaption">Forrás: <a href="https://en.wikipedia.org/wiki/Reinforcement_learning" data-href="https://en.wikipedia.org/wiki/Reinforcement_learning" class="markup--anchor markup--figure-anchor" rel="noopener" target="_blank">https://en.wikipedia.org/wiki/Reinforcement_learning</a></figcaption></figure><p name="1722" id="1722" class="graf graf--p graf-after--figure">Akármi is legyen a konkrét feladat, minden esetben ráhúzható a fenti képen látható modell. Minden esetben van valamilyen környezet (environment), amiben a robotunk (agent) létezik. Ez lehet egy számítógépes játék egyszerű szabályokkal, de lehet akár a valós tér is (aminek működését a fizika szabályai határozzák meg). Ezt a környezetet értelmezi a robot. A környezet absztrakt értelmezése az állapot (state). A robot képes műveleteket (action) végezni, amik hatással vannak a környezetre. Egy számítógépes játék esetén a műveletek száma elég korlátozott (egy Atari játéknál pl. egy joystick mozgatása fel/le/jobbra/balra), míg egy valós robot esetén sokkal többféle mozgás elképzelhető. A művelet elvégzését követően a környezet valahogy változik, amiből újra generálódik az absztrakt állapot (state) és még egy fontos dolog: a jutalom (reward). A jutalom miatt hívjuk ezt a tanulási módszert megerősítéses tanulásnak, hiszen ahogyan az állatok idomítása esetén, itt is a sikeres akciókat jutalmazzuk, a sikerteleneket pedig büntetjük. A robot célja, hogy minél jobban elvégezze a feladatát, tehát minél nagyobb értékben gyűjtsön össze reward-ot (pl. egy számítógépes játék esetén minél több pontot szerezzen).</p><p name="61b8" id="61b8" class="graf graf--p graf-after--p">A DeepMind által használt Deep Q-learning lényege, hogy az aktuális állapot (state) alapján minden egyes művelethez (action) megpróbáljuk megtippelni a összes várható jutalmat (reward), és mindig azt a műveletet választjuk, amelyik a legnagyobb jutalommal kecsegtet. A Q-learningben a Q (quality of action) ezt az elvárható jutalmat jelenti. A játék indulásakor még semmit nem tud a rendszer a Q értékéről, így csak vaktában próbálkozik felderíteni a játékot. Minél tovább játszik, annál több adata lesz arról, hogy melyik akció milyen jutalommal jár, így annál pontosabban tudja megtippelni a Q értékét. Fontos megjegyezni, hogy egy akcióhoz tartozó Q nem az akcióért közvetlenül kapható jutalom értékét jelenti, hanem azt, hogy ha az adott akciót választva végigjátsszuk a játékot, mekkora összes jutalomra számíthatunk. Vegyünk például egy labirintusos játékot, ahol a cél kijutni a labirintusból úgy, hogy eközben a legtöbb kincset szedjük fel. Választanunk kell, hogy jobbra, vagy balra megyünk. Balra van egy kincs, viszont arra nem lehet kijutni a labirintusból. Jobbra nincs semmi, de ha a következő sarkon jobbra fordulunk, akkor kijutunk a labirintusból. Ha csak közvetlenül az akció által elérhető rewardot néznénk, minden esetben balra kellene mennünk, hiszen arra van a kincs (nagyobb reward), míg a jobbra menéssel közvetlenül nem nyerünk semmit (0 reward). Ennek fényében a Q értékét a következőképpen számoljuk ki:</p><figure name="34d9" id="34d9" class="graf graf--figure graf-after--p"><img class="graf-image" data-image-id="1*5PD2fP-xrC0JAVBlMlDuIA.gif" data-width="407" data-height="34" src="https://cdn-images-1.medium.com/max/800/1*5PD2fP-xrC0JAVBlMlDuIA.gif"><figcaption class="imageCaption">Bellman equation (<a href="https://en.wikipedia.org/wiki/Bellman_equation" data-href="https://en.wikipedia.org/wiki/Bellman_equation" class="markup--anchor markup--figure-anchor" rel="noopener" target="_blank">https://en.wikipedia.org/wiki/Bellman_equation</a>)</figcaption></figure><p name="087b" id="087b" class="graf graf--p graf-after--figure">A fenti képletet Bellman egyenletnek hívjuk és valójában nem túl bonyolult. Az <strong class="markup--strong markup--p-strong"><em class="markup--em markup--p-em">s</em></strong> jelenti az állapotot, az <strong class="markup--strong markup--p-strong"><em class="markup--em markup--p-em">a</em></strong> az akciót. Az R(s,a) az adott állapotból adott akcióval elérhető jutalom (reward), a Q(s) pedig az adott állapotból elérhető legnagyobb összes jutalom. Hogy megkapjuk a Q értékét, az aktuális akcióval elérhető jutalomhoz (R(s,a)) adjuk hozzá az akció által előállt következő állapot (s’) várható Q értékét megszorozva egy gamma konstanssal, ami azt jelképezi, hogy a jövőben várható jutalom mennyit ér nekünk a jelenben. Tehát fogjuk az akcióval nyerhető jutalmat, hozzáadjuk a jövőben elvárható jutalom “diszkontált” értékét, és azt az akciót választjuk, ahol ez a legnagyobb. Dióhéjban igazából ennyi a Q-learning lényege. Attól lesz belőle Deep Q-learning, hogy a Q érték megtippelésére neurális hálót használunk.</p><p name="6669" id="6669" class="graf graf--p graf-after--p">Mielőtt elkezdenénk kódolni, ismerkedjünk meg az <a href="https://gym.openai.com/" data-href="https://gym.openai.com/" class="markup--anchor markup--p-anchor" rel="noopener" target="_blank">OpenAI Gym</a>mel, ami pont az, amire elsőre gondol az ember: egy edzőterem robotoknak. Kicsit pontosabban: a Gym tulajdonképpen egy szabványos interfész bármilyen környezethez. A programkönyvtár letöltésével eleve kapunk egy csomó környezetet (pl. Atari játékokat), de igény esetén további szimulációs környezetekkel is bővíthető. Vannak olyan környezetek, amelyek komoly fizikai engine-el rendelkeznek, így akár valós robotok szimulált környezetben való tanítására is megfelelőek lehetnek (inkább itt törjünk össze pár robotot a tanítás során, mint a valóságban). Lássuk hogy néz ki mindez a gyakorlatban.</p><figure name="a353" id="a353" class="graf graf--figure graf--iframe graf-after--p"><script src="https://gist.github.com/TheBojda/69ecc3f9854f416a8aceec0778947da3.js"></script></figure><p name="5352" id="5352" class="graf graf--p graf-after--figure">A fenti nagyon egyszerű kód a Breakout nevű Atari játék környezetet tölti be, majd véletlenszerűen játszik 10 000 fordulót. Sok értelme nincs, viszont szépen láthatjuk, hogyan működik a Gym. A 3. sorban inicializáljuk a környezetet. Itt adjuk meg, hogy a Breakout játékkal szeretnénk játszani. Az elérhető környezetek listája <a href="https://gym.openai.com/envs/" data-href="https://gym.openai.com/envs/" class="markup--anchor markup--p-anchor" rel="noopener" target="_blank">itt</a> található. A 4. sorban alapállapotba hozzuk a környezetet a reset függvénnyel. A függvény visszatérési értéke egy observation tenzor, ami egy pillanatkép a környezet aktuális állapotáról. Ez lehet valami előemésztett dolog, pl. a labda helyzete és sebességvektora, de lehet simán egy képernyőkép is, mint a DeepMind esetén. Hogy pontosan milyen az observation tenzor szerkezete, az a környezettől függ. Az env.render() megjeleníti a környezet aktuális állapotát, így szemmel is követhetjük a történéseket. Az env.action_space változóban kérhetjük el a környezetben végezhető akciókat, a sample függvény pedig rögtön választ is egyet közölük véletlenszerűen, megspórolva nekünk pár sor kódot. Az env.step függvény alkalmazza az akciót a környezetre, amitől az meg fog változni. A függvény visszatérési értéke a megváltozott környezetről egy új pillanatkép (observation), az akcióért kapott közvetlen jutalom (reward), egy boolean változó, ami azt jelzi, hogy véget ért a játék (done), és némi kiegészítő infó (info). Az utolsó előtti pár sorban ellenőrizzük a done változó értékét, és ha véget ért a játék, alap állapotba hozzuk a környezetet (reset). Ennyi igazából elég is a Gym-ről. Jöhet az igazi kódolás.</p><p name="0f0b" id="0f0b" class="graf graf--p graf-after--p">Ha nincs még tapasztalatod a Tensorflow-val, érdemes elolvasni a <a href="https://medium.com/@thebojda/tensorflow-alapoz%C3%B3-d2d1ee97c9db" data-href="https://medium.com/@thebojda/tensorflow-alapoz%C3%B3-d2d1ee97c9db" class="markup--anchor markup--p-anchor" target="_blank">cikksorozat első részét</a>:</p><div name="4c07" id="4c07" class="graf graf--mixtapeEmbed graf-after--p"><a href="https://medium.com/@thebojda/tensorflow-alapoz%C3%B3-d2d1ee97c9db" data-href="https://medium.com/@thebojda/tensorflow-alapoz%C3%B3-d2d1ee97c9db" class="markup--anchor markup--mixtapeEmbed-anchor" title="https://medium.com/@thebojda/tensorflow-alapoz%C3%B3-d2d1ee97c9db"><strong class="markup--strong markup--mixtapeEmbed-strong">TensorFlow alapozó</strong><br><em class="markup--em markup--mixtapeEmbed-em">(neurális hálózatok, tenzorok és képfelismerés a gyakorlatban)</em>medium.com</a><a href="https://medium.com/@thebojda/tensorflow-alapoz%C3%B3-d2d1ee97c9db" class="js-mixtapeImage mixtapeImage u-ignoreBlock" data-media-id="15660260b8dadb0ef12eafefd0be7499" data-thumbnail-img-id="1*T4ARzySpEQvEnr_9pc78pg.jpeg" style="background-image: url(https://cdn-images-1.medium.com/fit/c/160/160/1*T4ARzySpEQvEnr_9pc78pg.jpeg);"></a></div><p name="5359" id="5359" class="graf graf--p graf-after--mixtapeEmbed">A Gym-ben elérhető egyik legegyszerűbb játék a CartPole, ahol egy kis virtuális kocsit kell vezérelnünk, ami egy botot egyensúlyoz. Az ehhez tartozó Gym-es animáció elég sután néz ki, így inkább nézzünk egy videót arról, ahogyan egy igazi robot csinálja ugyanezt a betanított algoritmus segítségével.</p><figure name="bd52" id="bd52" class="graf graf--figure graf--iframe graf-after--p"><iframe src="https://www.youtube.com/embed/0wSAGwgzrp8?feature=oembed" width="700" height="393" frameborder="0" scrolling="no"></iframe></figure><p name="6583" id="6583" class="graf graf--p graf-after--figure">Szóval a cél a fenti robot betanítása, hogy minél tovább képes legyen egyensúlyozni a bottal. Az alábbi kód az én megvalósításom, amit pár másik kódból lapátoltam össze. A <a href="https://colab.research.google.com/gist/TheBojda/a421d7dd566244a427cdd40cca9aabdd/cartpole.ipynb" data-href="https://colab.research.google.com/gist/TheBojda/a421d7dd566244a427cdd40cca9aabdd/cartpole.ipynb" class="markup--anchor markup--p-anchor" rel="noopener" target="_blank">kód elérhető Google Colabban is</a>, így aki akarja, rögtön ki is próbálhatja.</p><figure name="b43b" id="b43b" class="graf graf--figure graf--iframe graf-after--p"><script src="https://gist.github.com/TheBojda/54b668f10c59a1bb89c3b86ac61cfee6.js"></script></figure><p name="9ecb" id="9ecb" class="graf graf--p graf-after--figure">A kód elején inicializálunk pár konstanst a Q-learninghez, majd inicializáljuk a Gym-et és lekérdezzük az observation valamint az action tenzor méretét. A játék tenzorainak részletes leírása <a href="https://github.com/openai/gym/blob/master/gym/envs/classic_control/cartpole.py" data-href="https://github.com/openai/gym/blob/master/gym/envs/classic_control/cartpole.py" class="markup--anchor markup--p-anchor" rel="noopener" target="_blank">itt</a> található a játék forráskódjával együtt. Az observation tenzor a CartPole esetén egy 4 elemű vektor. Az első elem az autó helyzetét adja meg, a második az autó sebességét, a harmadik elem a bot dőlésszöge, míg a negyedik a bot dőlésének sebessége. Ez alapján kell nekünk két akció közül választani. Az egyik akció a kocsi jobbra mozgatása, a másik a balra mozgatás. A játék akkor ér véget, ha a bot dőlésszöge 12 foknál nagyobb, vagy sikerült 200 lépésen keresztül egyensúlyozni a botot. A tanításhoz a következő egyszerű hálózatot fogjuk használni:</p><pre name="be78" id="be78" class="graf graf--pre graf-after--p">model = Sequential()<br>model.add(Dense(16, input_shape=(observation_space_size, ), activation=&#39;relu&#39;))<br>model.add(Dense(16, activation=&#39;relu&#39;))<br>model.add(Dense(action_space_size, activation=&#39;linear&#39;))</pre><pre name="4fcd" id="4fcd" class="graf graf--pre graf-after--pre">_________________________________________________________________<br>Layer (type)                 Output Shape              Param #   <br>=================================================================<br>dense_6 (Dense)              (None, 16)                80        <br>_________________________________________________________________<br>dense_7 (Dense)              (None, 16)                272       <br>_________________________________________________________________<br>dense_8 (Dense)              (None, 2)                 34        <br>=================================================================<br>Total params: 386<br>Trainable params: 386<br>Non-trainable params: 0<br>_________________________________________________________________</pre><p name="90d9" id="90d9" class="graf graf--p graf-after--pre">A hálózat 3 rétegből áll. Az első teljesen huzalozott réteg 4 bemenettel rendelkezik (ekkora az aktuális állapotot leíró observation tenzor mérete), amit 16 kimenetre képez le. A következő réteg egy 16 bemenettel és 16 kimenettel rendelkező teljesen huzalozott réteg, míg az utolsó réteg a 16 bemenetet képzi le 2 kimenetre, ami az adott akciókhoz (jobbra/balra mozgás) tartozó Q érték. Ez összesen 386 állítható paramétert fog jelenteni.</p><pre name="e146" id="e146" class="graf graf--pre graf-after--p">model.compile(loss=”mse”, optimizer=Adam(lr=LEARNING_RATE))</pre><p name="b922" id="b922" class="graf graf--p graf-after--pre">A modell optimalizálásához a szokásos Adam-et fogjuk használni, hibafüggvénynek pedig a négyzetes hibák átlagát (mean squared error), mivel a konkrét Q értékek betanítása a cél.</p><p name="737a" id="737a" class="graf graf--p graf-after--p">Maga a program 2 egymásba ágyazott ciklusból áll. A külső ciklusban futtatjuk a játékokat, a belső ciklus pedig egy játék végigjátszása. Ennek megfelelőn a belső ciklus mindig akkor ér véget, ha az env.step metódus játék végét jelző done visszaadott értéke igaz.</p><p name="9bb1" id="9bb1" class="graf graf--p graf-after--p">A belső ciklus elején található kódblokk határozza meg a következő akciót.</p><pre name="36ae" id="36ae" class="graf graf--pre graf-after--p">if np.random.rand() &lt; exploration_rate: <br>  action = random.randrange(action_space_size) <br>else: <br>  state = np.reshape(observation, [1, observation_space_size])<br>  q_values = model.predict(state)<br>  action = np.argmax(q_values[0])</pre><p name="fcc6" id="fcc6" class="graf graf--p graf-after--pre">Itt kezdjünk az else ággal, amiben a hálózat által tippelt Q érték alapján megyünk tovább. Az első sor megfelelő formába hozza az observation vektort, a neurális háló ugyanis [batch, input] formában várja a bemeneteket. A predict függvénnyel futtatjuk a hálózatot, majd az argmax-al meghatározzuk a nagyobb Q értékhez tartozó akciót.</p><p name="27c7" id="27c7" class="graf graf--p graf-after--p">Mivel kezdetben semmit nem tudunk a környezetről, ezért véletlenszerűen választunk a lehetséges lépések között. Ehhez bevezetünk egy exploration_rate változót, ami azt mondja meg, hogy a következő lépésben milyen valószínűséggel választunk véletlen akciót (felfedezünk) ahelyett, hogy a tippelt Q értéknek megfelelően mennénk tovább. Az exploration_rate-et minden lépésben csökkentjük, így minél okosabb a hálózat, annál inkább a tippelt Q értékekre hagyatkozunk a véletlen választás helyett. Érdemes definiálni egy minimális exploration_rate értéket, hogy mindig legyen véletlen próbálkozás, ezzel elkerülve hogy lokális optimumokban elakadjon az algoritmus. Felmerülhet a kérdés, hogy ha úgyis véletlen súlyokkal inicializáljuk a hálózatot, miért nem elég simán a tippelt Q értékre támaszkodni, ami a futások elején valamilyen véletlen függvénye az állapotnak? Ezzel az a baj, hogy ahogy tanul a rendszer hamar megtanulja, hogy merre vannak reward-ok, és mindig afelé menne. Emlékezzünk vissza a labirintusos példára. Ha balra van egy kincs, jobbra viszont nincs semmi, viszont jobbra van a kijárat, akkor egy hálózat ami rátalált a kincsre, soha nem indulna meg jobbra, így a rendszer elakadna egy lokális optimumban. Azzal, hogy bevezetjük az exploration_rate-et és a véletlen lépéseket, “ki tudjuk rázni” a rendszert az ilyen lokális optimumokból.</p><pre name="69a7" id="69a7" class="graf graf--pre graf-after--p">observation_next, reward, done, info = env.step(action)<br>reward = reward if not done else -200<br>memory.append((observation, action, reward, observation_next, done))<br>observation = observation_next</pre><p name="46d8" id="46d8" class="graf graf--p graf-after--pre">A középső blokkban alkalmazzuk az akciót a környezetre, majd egy memóriában letároljuk az adatokat, vagyis azt, hogy milyen állapotból milyen akcióval milyen állapotba jutottunk el. Ez később a tanulásnál lesz érdekes.</p><p name="b621" id="b621" class="graf graf--p graf-after--p">Az utolsó blokk a tanulás, ami az egyes játékok végén történik, és ami tulajdonképpen az egész rendszer esszenciája.</p><pre name="f0c4" id="f0c4" class="graf graf--pre graf-after--p">state_batch, qvalue_batch = [], []<br>batch = random.sample(memory, BATCH_SIZE)<br>for observation, action, reward, observation_next, done in batch:<br>  q_update = reward<br>  if not done:<br>    state_next = np.reshape(observation_next, [1, observation_space_size])<br>    q_predicted = np.amax(model.predict(state_next)[0])<br>    q_update = reward + (GAMMA * q_predicted)<br>  state = np.reshape(observation, [1, observation_space_size])<br>  q_values = model.predict(state)<br>  q_values[0][action] = q_update<br>  state_batch.append(state[0])<br>  qvalue_batch.append(q_values[0])<br>model.fit(np.array(state_batch), np.array(qvalue_batch), batch_size=len(state_batch), epochs=1, verbose=0)</pre><p name="aa0e" id="aa0e" class="graf graf--p graf-after--pre">A kód elején véletlenszerűen kiválasztunk pár mintát a memóriából (ezek mennyisége a BATCH_SIZE konstansban van definiálva). Ezekkel a mintákkal fogjuk az adott fordulóban tanítani a hálózatot. A q_update mezőbe kerül a Q érték, ami végállapot esetén megegyezik a reward-al, más esetben pedig a Bellman egyenletnek megfelelően számoljuk. Ez utóbbi esetben tehát fogjuk a rewardot, majd hozzáadjuk a modellünk által tippelt Q érték GAMMA szorosát. A következő pár sorban lefuttatjuk a hálózatunkat a memóriában szereplő állapotra, majd a memóriában szereplő akcióhoz tartozó Q értéket frissítjük a most számolttal. A state-eket és az akciókhoz számolt Q értékeket egy batch-be pakoljuk, majd egyben betanítjuk a hálózatnak. Lényegében ennyi a Q-learning. Minden körben választunk pár mintát a memóriából, a memória alapján újraszámoljuk a Q értékeket, majd betanítjuk a hálózatnak, egészen addig, míg a hálózat nem lesz elég pontos. A fenti kódot Colabban futtatva nekem olyan 7000 iteráció után állt be konstansan a maximális 200 pontra, de addig is szépen látszik az emelkedés a számokban (amik azt jelentik, hogy hány lépésen keresztül sikerült egyensúlyozni a botot).</p><p name="cec9" id="cec9" class="graf graf--p graf-after--p">Sokan csalódottak lehetnek, hisz a cikk elején Atari játékokat játszó MI-ről írtam, erre kaptak egy vacak botot egyensúlyozó robotot. A helyzet azonban az, hogy valójában Atari játékokat játszani sem sokkal bonyolultabb. Akit érdekel a konkrét megvalósítás, az <a href="https://github.com/gsurma/atari" data-href="https://github.com/gsurma/atari" class="markup--anchor markup--p-anchor" rel="noopener" target="_blank">itt megtalálja a forráskódot</a>. Gyorsan szaladjunk is végig a főbb részeken és különbségeken.</p><p name="b6cd" id="b6cd" class="graf graf--p graf-after--p">Nyilván az egyik különbség maga a hálózat, amit a <a href="https://github.com/gsurma/atari/blob/master/convolutional_neural_network.py" data-href="https://github.com/gsurma/atari/blob/master/convolutional_neural_network.py" class="markup--anchor markup--p-anchor" rel="noopener" target="_blank">convolutional_neural_network.py</a> állományban találunk. Igazából nem sokban különbözik a mi botos megoldásunktól. A mi megoldásunk egy 3 rétegű teljesen huzalozott hálózat, aminek a bemenete a kocsi és a bot helyzete valamint sebessége. Mivel az Atari játékoknál a bemenet egy pixeles kép, ezért a helyzetet és sebesség adatokat is a hálózatnak kell kinyernie. Ehhez a fenti példában (és a DeepMind megoldásában is) egy 3 rétegű konvolúciós hálót használnak. Tehát ez a hálózat mindössze abban különbözik a mi hálózatunktól, hogy az elejére raktak pár konvolúciós réteget a szükséges adatok kinyerésére, amit a cartpole-os játék esetén mi készen kaptunk.</p><p name="d424" id="d424" class="graf graf--p graf-after--p">Persze a dolog azért nem ennyire egyszerű. Ha megnézünk egy képkockát a Breakout játékból (pl. azt ami a cikk elején található), rájöhetünk, hogy ebből nem fogjuk megtudni sem a labda mozgásának irányát, sem annak sebességét. Ahhoz, hogy ezeket meghatározhassuk, több képkockára van szükség. A Gym-es Breakout esetén az observation tenzor 250x160x3-as méretű és olyan szerkezetű, mint amit a <a href="https://medium.com/@thebojda/tensorflow-alapoz%C3%B3-d2d1ee97c9db" data-href="https://medium.com/@thebojda/tensorflow-alapoz%C3%B3-d2d1ee97c9db" class="markup--anchor markup--p-anchor" target="_blank">cikksorozat első részében</a> a CIFAR10-es képek kódolására használtunk. Ekkora képre nincs szükség (kisebb monitoron is lehet Breakout-ot játszani), ráadásul a színeknek sincs jelentősége (fekete-fehér monitoron is lehet Breakout-ot játszani). Éppen ezért első körben a képet 84x84x1-es szürke árnyalatos tenzorokká konvertálják, majd ezekből összerakják az aktuális és az azt megelőző három fázis képkockáit. Végül az így kapott 84x84x4-es tenzorok képzik majd a konvolúciós háló bemenetét, amiből már kinyerhető minden szükséges adat. Ezt a fenti program a <a href="https://github.com/gsurma/atari/blob/master/gym_wrappers.py" data-href="https://github.com/gsurma/atari/blob/master/gym_wrappers.py" class="markup--anchor markup--p-anchor" rel="noopener" target="_blank">gym_wrappers.py</a>-ben található csomagoló osztállyal oldja meg, aminek eredményeként a gym környezet által visszaadott observation tenzor már a kívánt 84x84x4-es formában fog jönni.</p><p name="d618" id="d618" class="graf graf--p graf-after--p">Van egy harmadik trükk is. A tanítás folyamán a program két ugyanolyan szerkezetű hálózatot használ. Egyet a Q értékek tippeléséhez (target network) és egyet a tanításhoz. A tanítás során módosult hálózat alapján néha frissítjük a target networkot is, de nem minden körben. Ezzel a trükkel hatékonyabbá tehető a tanítás, amit a <a href="https://github.com/gsurma/atari/blob/master/game_models/ddqn_game_model.py" data-href="https://github.com/gsurma/atari/blob/master/game_models/ddqn_game_model.py" class="markup--anchor markup--p-anchor" rel="noopener" target="_blank">ddqn_game_model.py</a>-ban található DDQNSolver osztály valósít meg.</p><p name="63c7" id="63c7" class="graf graf--p graf-after--p">Igazából ez a 3 főbb dolog az, amiben a fenti (univerzális!) Atari játékokat játszani képes program a mi primitív bot egyensúlyozó robotunktól küölnbözik.</p><p name="5d03" id="5d03" class="graf graf--p graf-after--p">Körülbelül ennyit szerettem volna írni a Q-learningről és a megerősítéses tanulásról. Fontos megjegyezni, hogy a deep reinfocement learning egy igen forró terület. Naponta jönnek ki publikációk, új topológiák és új algoritmusok. Némelyikük a Q-learning valamilyen módosított változata, míg mások ettől eltérő módon működnek. Azt gondolom, hogy a fenti írás jó kiinduló alap lehet bárkinek. Akit pedig mélyebben érdekel a téma, az az Interneten rengeteg anyagot találhat a továbblépéshez.</p><p name="2e70" id="2e70" class="graf graf--p graf-after--p">UPDATE: Tensorflow-hoz készült egy <a href="https://www.tensorflow.org/agents" data-href="https://www.tensorflow.org/agents" class="markup--anchor markup--p-anchor" rel="noopener" target="_blank">Tensorflow Agents</a> nevű programkönyvtár, ami reinforcement learninghez tartalmaz mindenféle okosságot valamint az ismertebb módszerek implementációját, így akit mélyebben érdekel a téma, mindenképpen látogasson el a projekt oldalára.</p><p name="7b0e" id="7b0e" class="graf graf--p graf-after--p">Ha tetszett az írás, olvasd el az előző részeket is:</p><div name="2497" id="2497" class="graf graf--mixtapeEmbed graf-after--p"><a href="https://medium.com/@thebojda/tensorflow-alapoz%C3%B3-d2d1ee97c9db" data-href="https://medium.com/@thebojda/tensorflow-alapoz%C3%B3-d2d1ee97c9db" class="markup--anchor markup--mixtapeEmbed-anchor" title="https://medium.com/@thebojda/tensorflow-alapoz%C3%B3-d2d1ee97c9db"><strong class="markup--strong markup--mixtapeEmbed-strong">TensorFlow alapozó</strong><br><em class="markup--em markup--mixtapeEmbed-em">(neurális hálózatok, tenzorok és képfelismerés a gyakorlatban)</em>medium.com</a><a href="https://medium.com/@thebojda/tensorflow-alapoz%C3%B3-d2d1ee97c9db" class="js-mixtapeImage mixtapeImage u-ignoreBlock" data-media-id="15660260b8dadb0ef12eafefd0be7499" data-thumbnail-img-id="1*T4ARzySpEQvEnr_9pc78pg.jpeg" style="background-image: url(https://cdn-images-1.medium.com/fit/c/160/160/1*T4ARzySpEQvEnr_9pc78pg.jpeg);"></a></div><div name="4769" id="4769" class="graf graf--mixtapeEmbed graf-after--mixtapeEmbed"><a href="https://medium.com/@thebojda/tensorflow-alapoz%C3%B3-2-14720a33aca" data-href="https://medium.com/@thebojda/tensorflow-alapoz%C3%B3-2-14720a33aca" class="markup--anchor markup--mixtapeEmbed-anchor" title="https://medium.com/@thebojda/tensorflow-alapoz%C3%B3-2-14720a33aca"><strong class="markup--strong markup--mixtapeEmbed-strong">Tensorflow alapozó 2.</strong><br><em class="markup--em markup--mixtapeEmbed-em">(backpropagation, avagy hogyan működik a varázslat)</em>medium.com</a><a href="https://medium.com/@thebojda/tensorflow-alapoz%C3%B3-2-14720a33aca" class="js-mixtapeImage mixtapeImage u-ignoreBlock" data-media-id="acf8cadc94685ee340590198b4aca40e" data-thumbnail-img-id="1*YTDwPXrnfbPndXxNw77O-w.png" style="background-image: url(https://cdn-images-1.medium.com/fit/c/160/160/1*YTDwPXrnfbPndXxNw77O-w.png);"></a></div><div name="5d8a" id="5d8a" class="graf graf--mixtapeEmbed graf-after--mixtapeEmbed"><a href="https://medium.com/@thebojda/tensorflow-alapoz%C3%B3-3-ac3d26071b27" data-href="https://medium.com/@thebojda/tensorflow-alapoz%C3%B3-3-ac3d26071b27" class="markup--anchor markup--mixtapeEmbed-anchor" title="https://medium.com/@thebojda/tensorflow-alapoz%C3%B3-3-ac3d26071b27"><strong class="markup--strong markup--mixtapeEmbed-strong">Tensorflow alapozó 3.</strong><br><em class="markup--em markup--mixtapeEmbed-em">(autoencoderek, word2vec és embedding avagy dimenzió redukció neurális hálókkal)</em>medium.com</a><a href="https://medium.com/@thebojda/tensorflow-alapoz%C3%B3-3-ac3d26071b27" class="js-mixtapeImage mixtapeImage u-ignoreBlock" data-media-id="073a0d7e739a04910bab567864201655" data-thumbnail-img-id="1*JOkfva-B2_D3vJV3AYPOCQ.png" style="background-image: url(https://cdn-images-1.medium.com/fit/c/160/160/1*JOkfva-B2_D3vJV3AYPOCQ.png);"></a></div><p name="254b" id="254b" class="graf graf--p graf-after--mixtapeEmbed">A következő rész pedig itt érhető el:</p><div name="b9ed" id="b9ed" class="graf graf--mixtapeEmbed graf-after--p graf--trailing"><a href="https://medium.com/@thebojda/tensorflow-alapoz%C3%B3-5-df99bc48e306" data-href="https://medium.com/@thebojda/tensorflow-alapoz%C3%B3-5-df99bc48e306" class="markup--anchor markup--mixtapeEmbed-anchor" title="https://medium.com/@thebojda/tensorflow-alapoz%C3%B3-5-df99bc48e306"><strong class="markup--strong markup--mixtapeEmbed-strong">Tensorflow alapozó 5.</strong><br><em class="markup--em markup--mixtapeEmbed-em">Visszacsatolt hálózatok és LSTM</em>medium.com</a><a href="https://medium.com/@thebojda/tensorflow-alapoz%C3%B3-5-df99bc48e306" class="js-mixtapeImage mixtapeImage u-ignoreBlock" data-media-id="af1095d9c81da080648a19e670d10652" data-thumbnail-img-id="1*IMalbwl6uj3nlqxixZYFvA.jpeg" style="background-image: url(https://cdn-images-1.medium.com/fit/c/160/160/1*IMalbwl6uj3nlqxixZYFvA.jpeg);"></a></div></div></div></section>
</section>
<footer><p>By <a href="https://medium.com/@thebojda" class="p-author h-card">Laszlo Fazekas</a> on <a href="https://medium.com/p/cfeee8b9e34c"><time class="dt-published" datetime="2019-12-13T08:16:26.073Z">December 13, 2019</time></a>.</p><p><a href="https://medium.com/@thebojda/tensorflow-alapoz%C3%B3-4-cfeee8b9e34c" class="p-canonical">Canonical link</a></p><p>Exported from <a href="https://medium.com">Medium</a> on September 9, 2021.</p></footer></article></body></html>