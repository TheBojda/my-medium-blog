<!DOCTYPE html><html><head><meta http-equiv="Content-Type" content="text/html; charset=utf-8"><title>Tensorflow alapozó 3.</title><style>
      * {
        font-family: Georgia, Cambria, "Times New Roman", Times, serif;
      }
      html, body {
        margin: 0;
        padding: 0;
      }
      h1 {
        font-size: 50px;
        margin-bottom: 17px;
        color: #333;
      }
      h2 {
        font-size: 24px;
        line-height: 1.6;
        margin: 30px 0 0 0;
        margin-bottom: 18px;
        margin-top: 33px;
        color: #333;
      }
      h3 {
        font-size: 30px;
        margin: 10px 0 20px 0;
        color: #333;
      }
      header {
        width: 640px;
        margin: auto;
      }
      section {
        width: 640px;
        margin: auto;
      }
      section p {
        margin-bottom: 27px;
        font-size: 20px;
        line-height: 1.6;
        color: #333;
      }
      section img {
        max-width: 640px;
      }
      footer {
        padding: 0 20px;
        margin: 50px 0;
        text-align: center;
        font-size: 12px;
      }
      .aspectRatioPlaceholder {
        max-width: auto !important;
        max-height: auto !important;
      }
      .aspectRatioPlaceholder-fill {
        padding-bottom: 0 !important;
      }
      header,
      section[data-field=subtitle],
      section[data-field=description] {
        display: none;
      }
      </style></head><body><article class="h-entry">
<header>
<h1 class="p-name">Tensorflow alapozó 3.</h1>
</header>
<section data-field="subtitle" class="p-summary">
(autoencoderek, word2vec és embedding avagy dimenzió redukció neurális hálókkal)
</section>
<section data-field="body" class="e-content">
<section name="f849" class="section section--body section--first section--last"><div class="section-divider"><hr class="section-divider"></div><div class="section-content"><div class="section-inner sectionLayout--insetColumn"><h3 name="1bdc" id="1bdc" class="graf graf--h3 graf--leading graf--title">Tensorflow alapozó 3.</h3><h4 name="508f" id="508f" class="graf graf--h4 graf-after--h3 graf--subtitle">(autoencoderek, word2vec és embedding avagy dimenzió redukció neurális hálókkal)</h4><p name="0b06" id="0b06" class="graf graf--p graf-after--h4">Amikor először hallottam a word2vec-ről valami egészen misztikus dolognak tűnt, hiszen úgy volt képes szavakat vektorokká leképezni, hogy azok információt hordoztak magukban a szó jelentésével kapcsolatban. Tehát az alma vektorának és a körte vektorának távolsága kicsi, míg az alma és az autó távolsága nagy. Még olyan egyszerűbb matematikai műveleteket is lehet végezni a vektorokkal, hogy a “király” szó vektorából kivonjuk a “férfi” szó vektorát, majd hozzáadjuk a “nő” szó vektorát, így megkaptuk a “királynő” szó vektorát. Az az érzése támad az embernek, mintha ez a valami tényleg értené valamilyen szinten a világot, hisz tudja, hogy a királynő a király női változata, ráadásul mindezt a tudás egy sokdimenziós térben elhelyezkedő ponthalmaz reprezentálja. Mikor utánanéztem a word2vec működésének, rá kellett jönnöm, hogy valójában egyszerű dologról van szó, ráadásul az egész szinte adja magát. Így a misztikumát ugyan elvesztette a word2vec, de zsenialitása szemernyit sem kopott.</p><p name="8256" id="8256" class="graf graf--p graf-after--p">Neurális hálók esetén gyakori feladat, hogy bizonyos dolgokat vektorokká kell leképezni, hogy aztán könnyebben, gyorsabban és egyszerűbben végezhessünk rajtuk műveleteket. Valójában az <a href="https://medium.com/@thebojda/tensorflow-alapoz%C3%B3-d2d1ee97c9db" data-href="https://medium.com/@thebojda/tensorflow-alapoz%C3%B3-d2d1ee97c9db" class="markup--anchor markup--p-anchor" target="_blank">első részben</a> épített osztályozó hálózat is felfogható vektorleképzésként, hiszen mikor a képeket 10 osztályba soroljuk az eredmény tulajdonképpen egy 10 dimenziós vektor. Ebben az esetben egyszerű a dolgunk, hiszen minden dimenzióhoz adott jelentés társul (mennyire cica, mennyire kutya, mennyire repülő, stb.), a legtöbb esetben viszont nem hordoznak jelentést az egyes dimenziók, egyszerűen csak a dimenziószámot rögzítjük. Például egy arcfelismerő rendszer esetén az arcot egy adott méretű (mondjuk 512 dimenziós) vektorrá konvertáljuk, majd a tárolt mintavektorral összehasonlítva döntjük el, hogy megegyezik-e a két arc. De hogyan képezhetőek ilyen fix dimenziószámú vektorok?</p><p name="23c8" id="23c8" class="graf graf--p graf-after--p">Amikor valamit vektorrá képzünk le az felfogható egyfajta veszteséges tömörítésnek is. Olyan tömörítést kell tehát találnunk ami csak az elfogadható mértékben veszteséges. Erre a feladatra találták ki az autoencodereket.</p><figure name="6e7a" id="6e7a" class="graf graf--figure graf-after--p"><img class="graf-image" data-image-id="1*JOkfva-B2_D3vJV3AYPOCQ.png" data-width="841" data-height="765" data-is-featured="true" src="https://cdn-images-1.medium.com/max/800/1*JOkfva-B2_D3vJV3AYPOCQ.png"><figcaption class="imageCaption">Forrás: <a href="https://en.wikipedia.org/wiki/Autoencoder" data-href="https://en.wikipedia.org/wiki/Autoencoder" class="markup--anchor markup--figure-anchor" rel="noopener" target="_blank">https://en.wikipedia.org/wiki/Autoencoder</a></figcaption></figure><p name="e892" id="e892" class="graf graf--p graf-after--figure">Az autoencoderek két részből állnak. Egy encoderből és egy decoderből. Az encoder az a rész, ami megcsinálja a vektor leképzést, a decoder pedig arra szolgál, hogy a vektor leképzésből visszaállítsa az eredeti adatot. A tömörítéses hasonlatnál maradva tehát készítünk egy hangolható betömörítő és kitömörítő hálózatot, majd addig hangolgatjuk amíg a kitömörítés által adott eredmény csak a megengedett mértékben tér el a bemenettől. Ennyire egyszerű az egész. Ha megvan a kellően jó tömörítés a decodert el is dobhatjuk, az encodert pedig felhasználhatjuk más hálózatokban. A tanítást persze lehet cifrázni például negatív minták alkalmazásával. Egy arcfelismerő hálózat esetén nem csak az lényeges, hogy a rendszer csak kellő mértékben legyen veszteséges, hanem az is, hogy egy arcot ne ismerjen fel másik arcként. Két különböző arc vektorainak tehát megfelelő távolságban kell lenniük egymástól. Nézzünk is gyorsan egy mintát az autoencoderek használatára. Egy 14 szavas szótárat fogunk leképezni 3 dimenziós vektorokká Tensorflow segítségével.</p><figure name="6144" id="6144" class="graf graf--figure graf--iframe graf-after--p"><script src="https://gist.github.com/TheBojda/f5cda5d1674433fdd23b6bb516894c4d.js"></script><figcaption class="imageCaption">Forrás: <a href="https://gist.github.com/TheBojda/f5cda5d1674433fdd23b6bb516894c4d" data-href="https://gist.github.com/TheBojda/f5cda5d1674433fdd23b6bb516894c4d" class="markup--anchor markup--figure-anchor" rel="noopener" target="_blank">https://gist.github.com/TheBojda/f5cda5d1674433fdd23b6bb516894c4d</a></figcaption></figure><p name="5569" id="5569" class="graf graf--p graf-after--figure">A kód elején definiáljuk a 14 szavas szótárat, pár konstanst és a to_one_hot függvényt, ami a szavakat “one hot” vektorokká alakítja. Ezek olyan vektorok, ahol minden érték 0, csak egyetlen helyen van 1-es. Jelen esetben a “one hot” vektoraink 14 dimenziósak lesznek (ekkora a szótár), és az első szó esetén az 1. helyen áll 1-es, a második szó esetén a 2. helyen áll 1-es, stb. A 14. sorban a shuffle metódussal összekeverjük a vektorokat, így a tanítási mintánk valahogy így fog kinézni:</p><pre name="768d" id="768d" class="graf graf--pre graf-after--p">tf.Tensor(<br>[[1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]<br> [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 0. 0. 0.]<br> [0. 0. 0. 0. 0. 0. 0. 0. 1. 0. 0. 0. 0. 0.]<br> [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 0. 0.]<br> [0. 0. 0. 0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0.]<br> [0. 0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]<br> [0. 0. 0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]<br> [0. 0. 0. 0. 0. 0. 0. 1. 0. 0. 0. 0. 0. 0.]<br> [0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 0. 0. 0. 0.]<br> [0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]<br> [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 0.]<br> [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1.]<br> [0. 0. 0. 0. 0. 0. 1. 0. 0. 0. 0. 0. 0. 0.]<br> [0. 0. 0. 0. 0. 1. 0. 0. 0. 0. 0. 0. 0. 0.]], shape=(14, 14), dtype=float32)</pre><p name="ef59" id="ef59" class="graf graf--p graf-after--pre">Maga a neurális háló egészen egyszerű. Mindössze 2 rétegből áll. Az első réteg az encoder, ami egy teljesen összekapcsolt Dense réteg átviteli függvény és bias nélkül, tehát valójában itt csak egy sima szorzás történik a súlymátrixszal. A rétegnek 3 kimenete van mivel 3D-s vektorokat szeretnénk kapni a kódolás végén. A réteg súlymátrixa tehát 3x14-es.</p><pre name="da7c" id="da7c" class="graf graf--pre graf-after--p">model.add(layers.Dense(3, activation=&#39;linear&#39;, input_shape=(dict_len,), use_bias=False))</pre><p name="c99e" id="c99e" class="graf graf--p graf-after--pre">A második réteg a decoder, ami ugyancsak egy teljesen összekapcsolt Dense réteg, aminek 3 bemenete és 14 kimenete van. Itt a kimeneti függvény az első részben megismert softmax, amit osztályozó hálóknál szoktunk használni. Valójában itt is egyfajta osztályozásról van szó, hiszen minden szónak egy osztály felel meg.</p><pre name="aef7" id="aef7" class="graf graf--pre graf-after--p">model.add(layers.Dense(dict_len, activation=&#39;softmax&#39;))</pre><p name="3a12" id="3a12" class="graf graf--p graf-after--pre">A modell tanításánál az osztályozásoknál szokásos categorical_crossentropy hibafüggvényt fogjuk használni.</p><pre name="6caf" id="6caf" class="graf graf--pre graf-after--p">model.compile(optimizer=&#39;adam&#39;, loss=&#39;categorical_crossentropy&#39;)</pre><p name="fbac" id="fbac" class="graf graf--p graf-after--pre">Végül jöhet a tanítás 2000 ciklusban. Ennyi elegendő ahhoz, hogy megtaláljuk a 3D-s vektorokat.</p><pre name="d468" id="d468" class="graf graf--pre graf-after--p">model.fit(train_data, train_data, epochs=2000, verbose=0)</pre><p name="aecd" id="aecd" class="graf graf--p graf-after--pre">Ahogy látható, a tanítás bemenete és kimenete egyaránt a train_data, mivel a cél, hogy a dekódolás után a bemeneti vektort kapjuk vissza. A tanítás végeztével a predict függvény által adott indexek jó esetben megegyeznek a bemeneti indexekkel (sikerült a bemenetet saját magára leképezni).</p><pre name="95ad" id="95ad" class="graf graf--pre graf-after--p">tf.Tensor([ 0 10 8 11 4 2 3 7 9 1 12 13 6 5], shape=(14,), dtype=int64)<br>tf.Tensor([ 0 10 8 11 4 2 3 7 9 1 12 13 6 5], shape=(14,), dtype=int64)</pre><p name="4c51" id="4c51" class="graf graf--p graf-after--pre">Persze ez esetben nekünk nem a hálózat kimenete az érdekes, hanem az első réteg súlymátrixsza, ami valahogy így néz majd ki.</p><pre name="5357" id="5357" class="graf graf--pre graf-after--p">array([[ 1.3063056 , 1.7305588 , -0.2699419 ],<br> [-0.89637613, -2.263997 , -2.460185 ],<br> [ 1.3254083 , 0.20771885, 1.8607068 ],<br> [-1.1201528 , -0.79722226, 2.4123664 ],<br> [-0.1385253 , -2.3953307 , 0.49405864],<br> [-1.1062597 , 1.5653315 , 1.733906 ],<br> [-1.5009217 , 2.517614 , -1.2985629 ],<br> [ 1.8398327 , -0.9083853 , -0.38720697],<br> [ 1.1028037 , 1.0116827 , -2.245019 ],<br> [-1.7471856 , 0.23885375, -2.1513062 ],<br> [ 1.7920303 , -1.8545195 , 1.8756298 ],<br> [ 2.405476 , 2.3393812 , 2.088418 ],<br> [-2.1129463 , -1.6200206 , -0.38237098],<br> [ 1.6532408 , -1.7920396 , -2.3582103 ]], dtype=float32)&gt;]</pre><p name="828c" id="828c" class="graf graf--p graf-after--pre">A mátrix minden egyes sora egy 3D-s vektor, hiszen ha a mátrixot megszorozzuk az adott szó one hot vektorával, pont a szó indexének megfelelő sort kapjuk eredményként.</p><p name="c455" id="c455" class="graf graf--p graf-after--p">Ezzel kész is életünk első autoencodere. Persze a legtöbb esetben bonyolultabb az encoder és a decoder rész, és ha csupán szavak kódolása a cél, akkor is lényegesen nagyobb a szótár. Ez persze mind szép és jó, de hogyan képes egy ilyen vektor leképzés olyan misztikus tulajdonságokat felmutatni amilyenekről a cikk elején beszéltem?</p><p name="f8b6" id="f8b6" class="graf graf--p graf-after--p">Nos, a word2vec esetén van egy kis csavar a dologban. Ott ugyanis nem csak szavakat rendelnek saját magukhoz a vektorizálás folyamán, hanem a szót a hozzá kapcsolódó szavakhoz rendelik. A word2vec esetén tehát a bemenet a királynő szó, az elvárt kimenet pedig a király és a nő szó. Ráadásul a tanítási mintát sem manuálisan válogatják össze, egyszerűen fognak egy nagy szöveghalmazt és az adott szó környezetéből szedik össze, hogy mit rendeljenek hozzá. Ha elég nagy és átfogó a szöveg, úgy pusztán statisztikai alapon az egymáshoz tartalmilag kapcsolódó szavak közeledni fognak egymáshoz, míg a nem kapcsolódóak távolodnak, így tesznek szert a szavakból képzett vektorok olyan misztikus tulajdonságokra, amilyeneket az írás elején említettem.</p><p name="e3ef" id="e3ef" class="graf graf--p graf-after--p">Akit mélyebben érdekel a dolog működése, annak ajánlom Xin Rong fél órás előadását a témában:</p><figure name="3843" id="3843" class="graf graf--figure graf--iframe graf-after--p"><iframe src="https://www.youtube.com/embed/D-ekE-Wlcds?feature=oembed" width="700" height="393" frameborder="0" scrolling="no"></iframe></figure><p name="b659" id="b659" class="graf graf--p graf-after--figure">Ha egy neurális háló építésekor szavak vektorokká történő leképzésére lenne szükség, akkor érdemes tudni, hogy létezik egy Embedding nevű réteg, ami bemenetként szó indexeket vár, így nincs szükség a one hot kódolásra. Ennek bemutatására a <a href="https://www.tensorflow.org/tutorials/text/word_embeddings" data-href="https://www.tensorflow.org/tutorials/text/word_embeddings" class="markup--anchor markup--p-anchor" rel="noopener" target="_blank">Tensorflow word embedding tutorial</a> alapján készíteni fogunk egy hálózatot, ami IMDB commenteket osztályoz aszerint, hogy pozitívak, vagy negatívak. Hasonló lesz tehát a hálózat, mint a word2vec esetén, csak itt nem hasonló szavakat fogunk a szavakhoz rendelni, hanem érzelmi töltetet. A várakozás az, hogy az így kialakuló vektor leképzés olyan lesz, ahol a pozitív töltetű szavak közel vannak egymáshoz, ahogyan a negatív töltetű szavak is, a pozitív és negatív szavak közti távolság viszont nagy. Lássuk a kódot!</p><figure name="1c1a" id="1c1a" class="graf graf--figure graf--iframe graf-after--p"><script src="https://gist.github.com/TheBojda/a3fef8fafb324e93fb0ccd4de66eee81.js"></script></figure><p name="7048" id="7048" class="graf graf--p graf-after--figure">A kód elején beolvassuk az IMDB commenteket tartalmazó mintahalmazt. A tanító és teszt adatok a train_data és test_data változókba kerülnek, míg a vonatkozó metaadat az info változóba. Az info tartalmazza a 8185 szavas szótárat, amire a train_data és a test_data hivatkozik. Maga az adathalmaz egy lista, amiben minden comment egy szavak indexeit tartalmazó újabb lista. Emellett minden commenthez tartozik egy 1-es vagy 0-s címke, attól függően, hogy a comment pozitív, vagy negatív hangvételű. A beolvasást követő pár sor 10-es csomagokba (batch) szervezi a commenteket, és egységes formára hozza őket.</p><pre name="a403" id="a403" class="graf graf--pre graf-after--p">padded_shapes = ([None],())<br>train_batches = train_data.padded_batch(10, padded_shapes = padded_shapes)<br>test_batches = test_data.padded_batch(10, padded_shapes = padded_shapes)</pre><p name="cdda" id="cdda" class="graf graf--p graf-after--pre">Minden batch egy mátrix, ami 10 sorral rendelkezik, a hossza pedig megegyezik a leghosszabb comment szavainak számával. A rövidebb commentek esetén a fennmaradó helyeket 0-val töltjük fel (padding). Az eredmény valahogy így néz ki:</p><pre name="9601" id="9601" class="graf graf--pre graf-after--p">(10, 855)<br>[[ 249 4 277 … 5418 8029 7975]<br> [2080 4956 90 … 0 0 0]<br> [ 12 284 14 … 0 0 0]<br> …<br> [ 12 604 1694 … 0 0 0]<br> [ 133 67 1011 … 0 0 0]<br> [ 62 9 1 … 0 0 0]]<br>[1 1 1 1 1 1 0 1 1 0]</pre><p name="6a17" id="6a17" class="graf graf--p graf-after--pre">Az első sorban látszik a mátrix dimenziója. Az első batch esetén ez 10x855, mivel a leghosszabb comment (pont az első) 855 szó hosszú. Ezt követi a mátrix rövidített képe. Az utolsó sor az elvárt kimeneteket tartalmazó vektor 10 elemmel, ami az egyes commentek esetén megadja, hogy a comment pozitív, vagy negatív. Ilyen csomagokból áll a tanító minta.</p><pre name="eabb" id="eabb" class="graf graf--pre graf-after--p">model = models.Sequential([ <br>  layers.Embedding(vocab_size, 16),<br>  layers.GlobalAveragePooling1D(),<br>  layers.Dense(1, activation=’sigmoid’)<br>])</pre><p name="cf22" id="cf22" class="graf graf--p graf-after--pre">Maga a modell 3 rétegből épül fel. Az első a már említett Embedding réteg, ami a szavakat képzi le 16 dimenziós vektorokká. A tanítás célja, hogy minden szóhoz létrejöjjön egy ilyen vektor, ráadásul olyan módon, hogy a pozitív töltetű szavak vektorainak távolsága kicsi legyen, míg a negatív szavak kerüljenek ezektől távol. A hálózat második rétege a GlobalAvaragePooling1D, ami az egyes vektorok átlagát képzi. Tehát első körben az embedding 16 dimenziós vektorokat képez az egyes commentek szavaiból, majd a GlobalAvaragePooling1D-nek köszönhetően egyetlen átlagvektor keletkezik, ami nagyjából jellemző a commentre. Az utolsó réteg egy sima 16 bemenetű neuron sigmoid kimenettel, ami a vektorból bináris kimenetet képez. Ennyi lenne tehát a hálózat, amivel a betanítjuk a vektor leképzést végző Embedding réteget.</p><pre name="02dd" id="02dd" class="graf graf--pre graf-after--p">Model: “sequential”<br>_________________________________________________________________<br>Layer (type) Output Shape Param # <br>=================================================================<br>embedding (Embedding) (None, None, 16) 130960 <br>_________________________________________________________________<br>global_average_pooling1d (Gl (None, 16) 0 <br>_________________________________________________________________<br>dense (Dense) (None, 1) 17 <br>=================================================================<br>Total params: 130,977<br>Trainable params: 130,977<br>Non-trainable params: 0<br>_________________________________________________________________</pre><p name="1be6" id="1be6" class="graf graf--p graf-after--pre">Ha megnézzük a sumary-t, láthatjuk, hogy az embedding réteg 130960 paraméterrel rendelkezik, mivel a súlymátrix 8185x16-os, hiszen 8185 elemű a szótárunk és 16 dimenziós vektorokat szeretnénk kapni. A global_avarage_pooling-nak nincs paramétere, az egyetlen kimeneti neuronunk pedig 16 bemenettel rendelkezik, plusz egy bias, így jön ki a 17 paraméter.</p><p name="73b3" id="73b3" class="graf graf--p graf-after--p">A tanításhoz a szokásos adam optimalizálót használjuk, hibafüggvénynek pedig a binary_crossentropy-t. Tanítás után a vektorokat az embedding réteg súlyainak lekérdezésével kaphatjuk meg.</p><pre name="409e" id="409e" class="graf graf--pre graf-after--p">weights = model.layers[0].get_weights()[0]</pre><p name="b2b3" id="b2b3" class="graf graf--p graf-after--pre">Persze egy 8185x16-os mátrixot elég nehéz áttekinteni és bármiféle jelentést társítani hozzá. Szerencsére létezik egy <a href="http://projector.tensorflow.org/" data-href="http://projector.tensorflow.org/" class="markup--anchor markup--p-anchor" rel="noopener" target="_blank">Embedding Projector</a> nevű cucc, ami képes arra, hogy a kapott vektorokat 3D-ben megjelenítse. Az utolsó pár sor arra szolgál, hogy a vektorainkat a projector által olvasható tsv formátumba mentse. Két fájl fog keletkezni. Az első a vecs.tsv ami a vektorokat tartalmazza, a második a meta.tsv, ami a vektorokhoz tartozó szavakat tartalmazza. A fájlokat az Embedding Projector bal oldali Load gombjával tudjuk betölteni. Ha ezzel megvagyunk, a jobb oldalon keressünk rá mondjuk a ‘fun’ szóra. Alul rögtön meg is jelenik a vektortávolság alapján a 100 legközelebbi szó. Szépen látszik, hogy mind pozitív töltetű. Olyanok mint ‘beautiful’, ‘amazing’, stb. Próbáljunk valami negatív töltetű szót, pl. ‘boring’. Erre szűrve a közeli szavak ‘stupid’, ‘garbage’ és hasonlók, tehát a hálózatunk tényleg olyan vektor leképzést generált, amit vártunk tőle. Érdemes kipróbálni az Embedding Projector alap szettjeit is. Ott van például a cikk elején említett Word2Vec, ahol rákeresve a ‘queen’ szóra olyan közeli szavakat kapunk, mint ‘king’ vagy ‘princess’.</p><p name="14a5" id="14a5" class="graf graf--p graf-after--p">Körülbelül ennyit szerettem volna írni az autoencoderekről és az embeddingről. Persze a témát még nagyon sokféleképpen lehetne ragozni, de úgy gondolom, hogy a fentiek elég alapot biztosítanak arra, hogy akit mélyebben érdekelnek a dolgok, az utána tudjon nézni.</p><p name="bd02" id="bd02" class="graf graf--p graf-after--p">Ha tetszett az írás, és még nem tetted, olvasd el az előző két részt is:</p><div name="191e" id="191e" class="graf graf--mixtapeEmbed graf-after--p"><a href="https://medium.com/@thebojda/tensorflow-alapoz%C3%B3-d2d1ee97c9db" data-href="https://medium.com/@thebojda/tensorflow-alapoz%C3%B3-d2d1ee97c9db" class="markup--anchor markup--mixtapeEmbed-anchor" title="https://medium.com/@thebojda/tensorflow-alapoz%C3%B3-d2d1ee97c9db"><strong class="markup--strong markup--mixtapeEmbed-strong">TensorFlow alapozó</strong><br><em class="markup--em markup--mixtapeEmbed-em">(neurális hálózatok, tenzorok és képfelismerés a gyakorlatban)</em>medium.com</a><a href="https://medium.com/@thebojda/tensorflow-alapoz%C3%B3-d2d1ee97c9db" class="js-mixtapeImage mixtapeImage u-ignoreBlock" data-media-id="15660260b8dadb0ef12eafefd0be7499" data-thumbnail-img-id="1*T4ARzySpEQvEnr_9pc78pg.jpeg" style="background-image: url(https://cdn-images-1.medium.com/fit/c/160/160/1*T4ARzySpEQvEnr_9pc78pg.jpeg);"></a></div><div name="d393" id="d393" class="graf graf--mixtapeEmbed graf-after--mixtapeEmbed"><a href="https://medium.com/@thebojda/tensorflow-alapoz%C3%B3-2-14720a33aca" data-href="https://medium.com/@thebojda/tensorflow-alapoz%C3%B3-2-14720a33aca" class="markup--anchor markup--mixtapeEmbed-anchor" title="https://medium.com/@thebojda/tensorflow-alapoz%C3%B3-2-14720a33aca"><strong class="markup--strong markup--mixtapeEmbed-strong">Tensorflow alapozó 2.</strong><br><em class="markup--em markup--mixtapeEmbed-em">(backpropagation, avagy hogyan működik a varázslat)</em>medium.com</a><a href="https://medium.com/@thebojda/tensorflow-alapoz%C3%B3-2-14720a33aca" class="js-mixtapeImage mixtapeImage u-ignoreBlock" data-media-id="acf8cadc94685ee340590198b4aca40e" data-thumbnail-img-id="1*YTDwPXrnfbPndXxNw77O-w.png" style="background-image: url(https://cdn-images-1.medium.com/fit/c/160/160/1*YTDwPXrnfbPndXxNw77O-w.png);"></a></div><p name="62de" id="62de" class="graf graf--p graf-after--mixtapeEmbed">A következő részt pedig itt találod:</p><div name="95b8" id="95b8" class="graf graf--mixtapeEmbed graf-after--p graf--trailing"><a href="https://medium.com/@thebojda/tensorflow-alapoz%C3%B3-4-cfeee8b9e34c" data-href="https://medium.com/@thebojda/tensorflow-alapoz%C3%B3-4-cfeee8b9e34c" class="markup--anchor markup--mixtapeEmbed-anchor" title="https://medium.com/@thebojda/tensorflow-alapoz%C3%B3-4-cfeee8b9e34c"><strong class="markup--strong markup--mixtapeEmbed-strong">Tensorflow alapozó 4.</strong><br><em class="markup--em markup--mixtapeEmbed-em">(Reinforcement learning, Deep Q-learning és OpenAI Gym)</em>medium.com</a><a href="https://medium.com/@thebojda/tensorflow-alapoz%C3%B3-4-cfeee8b9e34c" class="js-mixtapeImage mixtapeImage u-ignoreBlock" data-media-id="fcc3c58bb1fa6621af2b7b98eac4efa8" data-thumbnail-img-id="1*fBRNFgr42ZsRwFEke0zoUA.jpeg" style="background-image: url(https://cdn-images-1.medium.com/fit/c/160/160/1*fBRNFgr42ZsRwFEke0zoUA.jpeg);"></a></div></div></div></section>
</section>
<footer><p>By <a href="https://medium.com/@thebojda" class="p-author h-card">Laszlo Fazekas</a> on <a href="https://medium.com/p/ac3d26071b27"><time class="dt-published" datetime="2019-11-30T07:39:15.567Z">November 30, 2019</time></a>.</p><p><a href="https://medium.com/@thebojda/tensorflow-alapoz%C3%B3-3-ac3d26071b27" class="p-canonical">Canonical link</a></p><p>Exported from <a href="https://medium.com">Medium</a> on September 9, 2021.</p></footer></article></body></html>