<!DOCTYPE html><html><head><meta http-equiv="Content-Type" content="text/html; charset=utf-8"><title>Tensorflow alapozó 5.</title><style>
      * {
        font-family: Georgia, Cambria, "Times New Roman", Times, serif;
      }
      html, body {
        margin: 0;
        padding: 0;
      }
      h1 {
        font-size: 50px;
        margin-bottom: 17px;
        color: #333;
      }
      h2 {
        font-size: 24px;
        line-height: 1.6;
        margin: 30px 0 0 0;
        margin-bottom: 18px;
        margin-top: 33px;
        color: #333;
      }
      h3 {
        font-size: 30px;
        margin: 10px 0 20px 0;
        color: #333;
      }
      header {
        width: 640px;
        margin: auto;
      }
      section {
        width: 640px;
        margin: auto;
      }
      section p {
        margin-bottom: 27px;
        font-size: 20px;
        line-height: 1.6;
        color: #333;
      }
      section img {
        max-width: 640px;
      }
      footer {
        padding: 0 20px;
        margin: 50px 0;
        text-align: center;
        font-size: 12px;
      }
      .aspectRatioPlaceholder {
        max-width: auto !important;
        max-height: auto !important;
      }
      .aspectRatioPlaceholder-fill {
        padding-bottom: 0 !important;
      }
      header,
      section[data-field=subtitle],
      section[data-field=description] {
        display: none;
      }
      </style></head><body><article class="h-entry">
<header>
<h1 class="p-name">Tensorflow alapozó 5.</h1>
</header>
<section data-field="subtitle" class="p-summary">
Visszacsatolt hálózatok és LSTM
</section>
<section data-field="body" class="e-content">
<section name="15c4" class="section section--body section--first section--last"><div class="section-divider"><hr class="section-divider"></div><div class="section-content"><div class="section-inner sectionLayout--insetColumn"><h3 name="1bb6" id="1bb6" class="graf graf--h3 graf--leading graf--title">Tensorflow alapozó 5.</h3><h4 name="9de7" id="9de7" class="graf graf--h4 graf-after--h3 graf--subtitle">Visszacsatolt hálózatok és LSTM</h4><p name="774a" id="774a" class="graf graf--p graf-after--h4">A visszacsatolás fogalmával fősulin találkoztam először. Ráadásul nem is a mesterséges intelligenciával kapcsolatban, hanem digitális technika órán. Ez a tantárgy arról szólt, hogy hogyan lehet logikai kapukból (ÉS/VAGY/NEM) logikai áramköröket összerakni. Minden digitális áramkör, ide értve a legbonyolultabb mikroprocesszorokat is ilyen pár tranzisztoros logikai kapukból épül fel. A logikai hálózatok egyszerűbbik változata a kombinációs hálózat. Ez egy olyan doboz, aminek van egy bináris bemenete, valamint egy bináris kimenete, és a kimenet csakis a bemenettől függ. Igazából ez a konstrukció nagyon hasonlít az eddig megismert neurális hálókra, azzal a különbséggel, hogy a neurális hálózat be- és kimenete nem bináris, valamint hogy a kombinációs hálózattal ellentétben a neurális hálózat tanítható. Kombinációs hálózattal sokmindent meg lehet oldani. Építhetünk belőle például összeadó vagy akár szorzó áramkört, de hamar beleüközünk a korlátaiba. A kombinációs hálózatnak ugyanis nincs “emlékezete”. Ez azt jelenti, hogy ahogyan az előbbiekben írtam, a kimenet csakis a bemenettől függ, nincs rá hatással semmilyen régebbi történés. Ez gondot jelent például akkor, ha egy számláló áramkört szeretnénk készíteni, aminek a bemenetére impulzusok érkeznek, a kimenete pedig az összeszámolt impulzusok száma. Ez nem túl bonyolult feladat, de egy kombinációs hálózatnak gondot jelent, hiszen “emlékzet” híján mindig elfelejti, hogy hol tartott a számolásban. Szerencsére a probléma könnyen orvosolható. A hálózat kimenetét (vagy annak egy részét) vissza kell csatolni a bemenetére. Ennek köszönhetően a bemenet most már nem csak az impulzus, hanem a számláló előző állapota is, ami alapján a kombinációs hálózat ki tudja számolni a következő állapotot. A visszacsatolás által tehát “emlékezetet” nyernek a hálózatok amivel lehetőség nyílik időben változó bemenetek kezelésére. A visszacsatolás lehetősége tehát rendkívűl fontos tulajdonsága egy hálózatnak. Olyannyira, hogy a visszacsatolt kombinációs hálózatoknak külön nevet is adtak: ezek a sorrendi hálózatok. Valójában bármilyen digitális áramkör amit valaha terveztek vagy tervezni fognak, a legegyszerűbb számlálótól a legbonyolultabb mikroprocesszorig leképezhető ilyen sorrendi hálózatra. A visszacsatolás az, ami a logikai hálózatokat univerzális eszközökké teszi! Nos, pont ugyanez igaz a neurális hálózatokra is. Elméletileg bármilyen létező algoritmus leképezhető visszacsatolt neurális hálózatra, legyen az valamilyen számítógépes játék végigjátszása, vagy akár az emberi agy modellezése a jövőben.</p><p name="6551" id="6551" class="graf graf--p graf-after--p">Ennyi elmélet után nézzünk egy egyszerű kódot. A “Hello World!” karaktersorozatot fogjuk betanítani egy hálózatnak, mégpedig oly módon, hogy mindig az előző karakterből jósoljuk a következőt. Tehát ha a bemenet “H”, a kimenet “e” lesz, ha a bemenet “e”, a kimenet “l” lesz, stb. Visszacsatolás nélkül erre nem lennénk képesek, mivel ez esetben a kimenet csak a bemenettől függene, így minden karakterhez csakis egy másik karakter tartozhatna. Ez láthatóan problémát okozna az “l” betű esetén, hiszen az első esteben az “l” betű után egy másik “l” betű jön, a második esetben “o” betű, a harmadik esetben pedig “d”. A hálózatnak tehát “emlékeznie” kell arra, hogy hol tart a szövegben.</p><figure name="9e69" id="9e69" class="graf graf--figure graf--iframe graf-after--p"><script src="https://gist.github.com/TheBojda/4e9460b5ec47fdcf0e41e6c479c81bf0.js"></script></figure><p name="28ac" id="28ac" class="graf graf--p graf-after--figure">A fenti kód egyes részei ismerősek lehetnek a <a href="https://medium.com/@thebojda/tensorflow-alapoz%C3%B3-3-ac3d26071b27" data-href="https://medium.com/@thebojda/tensorflow-alapoz%C3%B3-3-ac3d26071b27" class="markup--anchor markup--p-anchor" target="_blank">cikksorozat 3. részében</a> épített autoencoderből. Mindkét esetben one hot vektorok képzik a be és kimeneteket, csak míg a 3. részben szavakból képeztük a vektorokat, itt az egyes karakterek jelentik a szótárat.</p><p name="99cc" id="99cc" class="graf graf--p graf-after--p">A source_data-ba és a target_data-ba kerülnek a bemenetek és a kimenetek one hot kódolással. Mindkét tenzor 11x9-es méretű, mivel a szótárunk (egyedi karakterek) mérete 9 elem, a szöveg pedig 12 karakter hosszú (és mivel minden karakterhez a következőt rendeljük, 11 db ilyen összerendelésünk lesz).</p><p name="342b" id="342b" class="graf graf--p graf-after--p">Van még 2 expand_dims sor, ami egy plusz dimenzióval 3 dimenziósra bővíti az előző 2 dimenziós tenzorainkat. Erre azért van szükség, mert a SimpleRNN réteg (hamarosan lesz szó róla részletesen) [batch size, timesteps, input dim] formában várja a bemenetet. Az eddigi (nem visszacsatolt) példákhoz képest tehát bejött egy új dimenzió, a timesteps. Mikor elkezdtem ismerkedni a visszacsatolt hálókkal, engem nagyon zavart, hogy más a bemenet formátuma. Azt gondoltam, hogy egy visszacsatolt háló csupán annyiban fog különbözni egy nem visszacsatolt hálótól, hogy miközben tömöm bele az adatokat, nem felejti el az állapotat. De a dolog nem ilyen egyszerű. Egy nem visszacsatolt hálózatnál a <a href="https://medium.com/@thebojda/tensorflow-alapoz%C3%B3-2-14720a33aca" data-href="https://medium.com/@thebojda/tensorflow-alapoz%C3%B3-2-14720a33aca" class="markup--anchor markup--p-anchor" target="_blank">backpropagation</a> a bementig kell csak hogy visszamenjen, mivel a kimenet csakis a bemenettől függ. Egy visszacsatolt hálózatnál azonban az eredmény függhet az előző állapot kimenetétől, ami az előző állapot bemenetétől függ, valamint függhet az azt megelőző állapottól is, stb. egészen a legelső időpillanatig. Egy ilyen visszacsatolt hálózatnál tehát a backpropagationnek nem elég visszamenni a bemenetig, időben is vissza kell mennie. Éppen ezért a backpropagation ezen változatát szokták <a href="https://en.wikipedia.org/wiki/Backpropagation_through_time" data-href="https://en.wikipedia.org/wiki/Backpropagation_through_time" class="markup--anchor markup--p-anchor" rel="noopener" target="_blank">backpropagation through time</a>-nak is hívni. Logikailag olyan az egész, mint ha megismételnénk az egész hálózatot annyiszor, ahányi időpillanatot visszamegyünk, és erre a “kihajtogatott” hálózatra futtatnánk le a backpropagationt.</p><figure name="43f2" id="43f2" class="graf graf--figure graf-after--p"><img class="graf-image" data-image-id="1*IMalbwl6uj3nlqxixZYFvA.jpeg" data-width="902" data-height="725" data-is-featured="true" src="https://cdn-images-1.medium.com/max/800/1*IMalbwl6uj3nlqxixZYFvA.jpeg"><figcaption class="imageCaption">Forrás: <a href="http://karpathy.github.io/2015/05/21/rnn-effectiveness/" data-href="http://karpathy.github.io/2015/05/21/rnn-effectiveness/" class="markup--anchor markup--figure-anchor" rel="noopener" target="_blank">http://karpathy.github.io/2015/05/21/rnn-effectiveness/</a></figcaption></figure><p name="80c1" id="80c1" class="graf graf--p graf-after--figure">A fenti képen látszik egy ilyen hálózat az elő 4 időpillanatra kihajtogatva. Minden ciklusban van egy bemenet, egy kimenet, és egy állapot, ami a következő ciklus bemenetét képzi a hálózat bemenete mellett. Ezen a kihajtogatott hálón kell végigtolni a backpropagationt, hogy megfelelően álljanak be a súlyok. A kérdés már csak az, hogy hány ciklust menjen vissza a backpropagation? Itt jön a képbe a timestamps dimenzió. Ahány cikluson keresztül szeretnénk visszaterjeszteni a hibát, annyit kell belepakolnunk a tenzorunkba. Nem lehet tehát “folyamatosan tanítani” a hálót. Ehelyett “kell csinálnunk egy felvételt” a minátkról, majd azt vagy annak egy szakaszát egyben odaadni a hálózatnak. A tanító algoritmus csak ezt az átadott részt látja a folyamatból és erre futtatja le a backpropagationt.</p><pre name="f0eb" id="f0eb" class="graf graf--pre graf-after--p">Model: “sequential”<br>_________________________________________________________________<br>Layer (type) Output Shape Param # <br>=================================================================<br>simple_rnn_1 (SimpleRNN) (None, 11, 4) 56 <br>_________________________________________________________________<br>dense (Dense) (None, 11, 9) 45 <br>=================================================================<br>Total params: 101<br>Trainable params: 101<br>Non-trainable params: 0<br>_________________________________________________________________</pre><p name="4323" id="4323" class="graf graf--p graf-after--pre">A kód következő része a hálózat definiálása. A hálózatunk két rétegből áll. Az első egy SimpleRNN réteg, ami egy egyszerű visszacsatolt háló, ami jelen esetben 4 neuronból áll és minden neuronnak 9 bemenete van. Ez valójában 13 (9+4) bemenet neurononként, mivel a 9 bemenet mellé jön a 4 neuron visszacsatolt kimenete, plusz egy bias. Ennek a rétegnek tehát összesen 56 (4*14) paramétere lesz. Ezt követi egy sima teljesen csatolt réteg, ami 4 bemenettel (plusz 1 bias) és 9 kimenettel rendelkezik, így 45 (5*9) paramétere van. Ennek a cellának a kimeneti fv.-e softmax, mivel azt várjuk tőle, hogy megmondja, melyik karakter következik legnagyobb valószínűséggel az előzőek függvényében. A hálózat valójában nagyon hasonlít a <a href="https://medium.com/@thebojda/tensorflow-alapoz%C3%B3-3-ac3d26071b27" data-href="https://medium.com/@thebojda/tensorflow-alapoz%C3%B3-3-ac3d26071b27" class="markup--anchor markup--p-anchor" target="_blank">3. részben</a> megismert autoencoderhez, csak itt a 4 dimenziós kimenet a 9 dimenziós bemenet mellett az előző állapot 4 dimenziós vektorától is függ. Ez valósítja meg azt a fajta emlékezetet, ami képessé teszi a hálózatot a “Hello World!” karaktersorozat megtanulására.</p><p name="6afa" id="6afa" class="graf graf--p graf-after--p">A SimpleRNN réteggel és úgy általában a visszacsatolással van egy nagy probléma. Ahogy haladunk az időben, egyre inkább “elkopik” az információ (<a href="https://en.wikipedia.org/wiki/Vanishing_gradient_problem" data-href="https://en.wikipedia.org/wiki/Vanishing_gradient_problem" class="markup--anchor markup--p-anchor" rel="noopener" target="_blank">Vanishing gradient problem</a>). Megoldásként olyan “okos memóriákat” találtak ki, mint az <a href="https://en.wikipedia.org/wiki/Long_short-term_memory" data-href="https://en.wikipedia.org/wiki/Long_short-term_memory" class="markup--anchor markup--p-anchor" rel="noopener" target="_blank">LSTM</a> vagy a <a href="https://en.wikipedia.org/wiki/Gated_recurrent_unit" data-href="https://en.wikipedia.org/wiki/Gated_recurrent_unit" class="markup--anchor markup--p-anchor" rel="noopener" target="_blank">GRU</a>.</p><figure name="9b98" id="9b98" class="graf graf--figure graf-after--p"><img class="graf-image" data-image-id="1*vmcUultdCEOoTVvwG0DTlw.png" data-width="2014" data-height="1322" src="https://cdn-images-1.medium.com/max/800/1*vmcUultdCEOoTVvwG0DTlw.png"><figcaption class="imageCaption">Forrás: <a href="https://en.wikipedia.org/wiki/Long_short-term_memory" data-href="https://en.wikipedia.org/wiki/Long_short-term_memory" class="markup--anchor markup--figure-anchor" rel="noopener" target="_blank">https://en.wikipedia.org/wiki/Long_short-term_memory</a></figcaption></figure><p name="2480" id="2480" class="graf graf--p graf-after--figure">A fenti ábrán látható az LSTM elvi felépítése. Dióhéjban annyit kell róla tudni, hogy a SimpleRNN-el ellentétben nem simán a bemenetből és a kimenetből képzi a kimenetet, hanem a bemenettől függően tárolja a kimenetet, amit csak a megfelelő bemenet hatására töröl. Összességében tehát tényleg olyasmi mint egy memória cella, ami hosszú távon képes megjegyezni dolgokat. Akit bővebben érdekel az LSTM működése, az <a href="https://colah.github.io/posts/2015-08-Understanding-LSTMs/" data-href="https://colah.github.io/posts/2015-08-Understanding-LSTMs/" class="markup--anchor markup--p-anchor" rel="noopener" target="_blank">itt talál róla egy jó összefoglaló cikket</a>. Az LSTM tehát képes kiküszöbölni azt a fajta “információ kopást” ami a SimpleRNN-nél problémát jelentett. Andrej Karpathy szavaival élve olyan mint egy cső vagy szupersztráda amin az adatok gond nélkül közlekedhetnek a multból a jövő felé.</p><p name="77ca" id="77ca" class="graf graf--p graf-after--p">A <a href="https://en.wikipedia.org/wiki/Gated_recurrent_unit" data-href="https://en.wikipedia.org/wiki/Gated_recurrent_unit" class="markup--anchor markup--p-anchor" rel="noopener" target="_blank">GRU</a> az LSTM kistestvére. Eggyel kevesebb paraméterrel rendelkezik, így kevesebb számítási kapacitás kell a tanításához, úgyanakkor sok esetben az LSTM-el megegyező teljesítményt nyújt. A GRU használatára jó példa a <a href="https://www.tensorflow.org/tutorials/text/text_generation" data-href="https://www.tensorflow.org/tutorials/text/text_generation" class="markup--anchor markup--p-anchor" rel="noopener" target="_blank">Tensorflow text generation tutorialja</a>, ahol egy 1024 GRU cellás hálózatnak tanítanak be Shakespeare szövegeket hasonlóan karakter alapon, ahogyan mi is tettük. Annyi a csavar a dologban, hogy a következő karaktert mindig véletlenszerűen választják a jósolt valószínűség alapján, így a hálózat minden futtatás után más-más szöveget generál. A dolog érdekessége, hogy az eredmény minden esetben a szavak szintjén szinte hibátlan angol szöveg lesz. Ez azért elég nagy dolog azt figyelembe véve, hogy csupán karakter statisztikákat tanítottunk a hálózatnak.</p><p name="2c61" id="2c61" class="graf graf--p graf-after--p"><a href="http://karpathy.github.io/2015/05/21/rnn-effectiveness/" data-href="http://karpathy.github.io/2015/05/21/rnn-effectiveness/" class="markup--anchor markup--p-anchor" rel="noopener" target="_blank">Andrej Karpathy egy hasonló hálózatot használva</a> a Linux kernel forrását tanította be egy neurális hálónak, ami így egész értelmes C kódokat generált. Bár maga a kód értelmetlen, jól látható a mintában, hogy a szintaktikát (a C nyelv nyelvtanát) egész jól megtanulta. Egy ilyen hálózatnak már lehet gyakorlati haszna például egy intelligens kódkiegészítő formájában.</p><p name="1032" id="1032" class="graf graf--p graf-after--p">Bár kissé bonyolultabb architektúrájú, de ugyancsak LSTM-et használó visszacsatolt háló az <a href="https://openai.com/blog/better-language-models/" data-href="https://openai.com/blog/better-language-models/" class="markup--anchor markup--p-anchor" rel="noopener" target="_blank">OpenAI GPT-2 nyelvi modellje</a>, ami úgy képes szöveget generálni, hogy figyel a kontextusra. Ezek már nyelvtanilag szinte tökéletes, értelmes szövegek. A rendszert bárki kipróbálhatja a <a href="https://talktotransformer.com/" data-href="https://talktotransformer.com/" class="markup--anchor markup--p-anchor" rel="noopener" target="_blank">https://talktotransformer.com/</a> oldalon. Emelett a GPT és a hozzá hasonló nyelvi modellekkel leírásokat generálhatunk egy képhez (egy CNN hálózattal összekötve), vagy akár egy videóhoz.</p><p name="4c3b" id="4c3b" class="graf graf--p graf-after--p">Végül ide linkelném még az <a href="https://d4mucfpksywv.cloudfront.net/research-covers/openai-five/network-architecture.pdf" data-href="https://d4mucfpksywv.cloudfront.net/research-covers/openai-five/network-architecture.pdf" class="markup--anchor markup--p-anchor" rel="noopener" target="_blank">OpenAI Dota hálózatának diagramját</a>, ami “super human” szinten képes játszani a játékot és ugyancsak kulcsfontosságú része egy 1024 LSTM cellából álló réteg.</p><p name="aa73" id="aa73" class="graf graf--p graf-after--p">Jól látható tehát, hogy a visszacsatolt hálózatokban rengeteg a lehetőség szinte minden területen, legyen az idősorok elemzése, számítógépes játékok, vagy akár robotok vezérlése. Mivel visszacsatolt hálók segítségével elvileg bármilyen aloritmus megvalósítható, itt van kezünkben az univerzális eszköz, a kérdés, hogy mi mindenre leszünk képesek általa.</p><p name="52f5" id="52f5" class="graf graf--p graf-after--p">Ha tetszett az írás, olvasd el az előző részeket is:</p><div name="0834" id="0834" class="graf graf--mixtapeEmbed graf-after--p"><a href="https://medium.com/@thebojda/tensorflow-alapoz%C3%B3-d2d1ee97c9db" data-href="https://medium.com/@thebojda/tensorflow-alapoz%C3%B3-d2d1ee97c9db" class="markup--anchor markup--mixtapeEmbed-anchor" title="https://medium.com/@thebojda/tensorflow-alapoz%C3%B3-d2d1ee97c9db"><strong class="markup--strong markup--mixtapeEmbed-strong">TensorFlow alapozó</strong><br><em class="markup--em markup--mixtapeEmbed-em">(neurális hálózatok, tenzorok és képfelismerés a gyakorlatban)</em>medium.com</a><a href="https://medium.com/@thebojda/tensorflow-alapoz%C3%B3-d2d1ee97c9db" class="js-mixtapeImage mixtapeImage u-ignoreBlock" data-media-id="15660260b8dadb0ef12eafefd0be7499" data-thumbnail-img-id="1*T4ARzySpEQvEnr_9pc78pg.jpeg" style="background-image: url(https://cdn-images-1.medium.com/fit/c/160/160/1*T4ARzySpEQvEnr_9pc78pg.jpeg);"></a></div><div name="c89e" id="c89e" class="graf graf--mixtapeEmbed graf-after--mixtapeEmbed"><a href="https://medium.com/@thebojda/tensorflow-alapoz%C3%B3-2-14720a33aca" data-href="https://medium.com/@thebojda/tensorflow-alapoz%C3%B3-2-14720a33aca" class="markup--anchor markup--mixtapeEmbed-anchor" title="https://medium.com/@thebojda/tensorflow-alapoz%C3%B3-2-14720a33aca"><strong class="markup--strong markup--mixtapeEmbed-strong">Tensorflow alapozó 2.</strong><br><em class="markup--em markup--mixtapeEmbed-em">(backpropagation, avagy hogyan működik a varázslat)</em>medium.com</a><a href="https://medium.com/@thebojda/tensorflow-alapoz%C3%B3-2-14720a33aca" class="js-mixtapeImage mixtapeImage u-ignoreBlock" data-media-id="acf8cadc94685ee340590198b4aca40e" data-thumbnail-img-id="1*YTDwPXrnfbPndXxNw77O-w.png" style="background-image: url(https://cdn-images-1.medium.com/fit/c/160/160/1*YTDwPXrnfbPndXxNw77O-w.png);"></a></div><div name="7793" id="7793" class="graf graf--mixtapeEmbed graf-after--mixtapeEmbed"><a href="https://medium.com/@thebojda/tensorflow-alapoz%C3%B3-3-ac3d26071b27" data-href="https://medium.com/@thebojda/tensorflow-alapoz%C3%B3-3-ac3d26071b27" class="markup--anchor markup--mixtapeEmbed-anchor" title="https://medium.com/@thebojda/tensorflow-alapoz%C3%B3-3-ac3d26071b27"><strong class="markup--strong markup--mixtapeEmbed-strong">Tensorflow alapozó 3.</strong><br><em class="markup--em markup--mixtapeEmbed-em">(autoencoderek, word2vec és embedding avagy dimenzió redukció neurális hálókkal)</em>medium.com</a><a href="https://medium.com/@thebojda/tensorflow-alapoz%C3%B3-3-ac3d26071b27" class="js-mixtapeImage mixtapeImage u-ignoreBlock" data-media-id="073a0d7e739a04910bab567864201655" data-thumbnail-img-id="1*JOkfva-B2_D3vJV3AYPOCQ.png" style="background-image: url(https://cdn-images-1.medium.com/fit/c/160/160/1*JOkfva-B2_D3vJV3AYPOCQ.png);"></a></div><div name="8c74" id="8c74" class="graf graf--mixtapeEmbed graf-after--mixtapeEmbed"><a href="https://medium.com/@thebojda/tensorflow-alapoz%C3%B3-4-cfeee8b9e34c" data-href="https://medium.com/@thebojda/tensorflow-alapoz%C3%B3-4-cfeee8b9e34c" class="markup--anchor markup--mixtapeEmbed-anchor" title="https://medium.com/@thebojda/tensorflow-alapoz%C3%B3-4-cfeee8b9e34c"><strong class="markup--strong markup--mixtapeEmbed-strong">Tensorflow alapozó 4.</strong><br><em class="markup--em markup--mixtapeEmbed-em">(Reinforcement learning, Deep Q-learning és OpenAI Gym)</em>medium.com</a><a href="https://medium.com/@thebojda/tensorflow-alapoz%C3%B3-4-cfeee8b9e34c" class="js-mixtapeImage mixtapeImage u-ignoreBlock" data-media-id="fcc3c58bb1fa6621af2b7b98eac4efa8" data-thumbnail-img-id="1*fBRNFgr42ZsRwFEke0zoUA.jpeg" style="background-image: url(https://cdn-images-1.medium.com/fit/c/160/160/1*fBRNFgr42ZsRwFEke0zoUA.jpeg);"></a></div><p name="2e8f" id="2e8f" class="graf graf--p graf-after--mixtapeEmbed">A következő részt pedig itt találod:</p><div name="72d2" id="72d2" class="graf graf--mixtapeEmbed graf-after--p graf--trailing"><a href="https://medium.com/@thebojda/tensorflow-alapoz%C3%B3-6-1ba1a32a79d5" data-href="https://medium.com/@thebojda/tensorflow-alapoz%C3%B3-6-1ba1a32a79d5" class="markup--anchor markup--mixtapeEmbed-anchor" title="https://medium.com/@thebojda/tensorflow-alapoz%C3%B3-6-1ba1a32a79d5"><strong class="markup--strong markup--mixtapeEmbed-strong">Tensorflow alapozó 6.</strong><br><em class="markup--em markup--mixtapeEmbed-em">GAN-ok, avagy hogyan generáljunk cicákat neurális hálóval</em>medium.com</a><a href="https://medium.com/@thebojda/tensorflow-alapoz%C3%B3-6-1ba1a32a79d5" class="js-mixtapeImage mixtapeImage u-ignoreBlock" data-media-id="1dc770cd0ae3ee59870ee61ffe8a5b46" data-thumbnail-img-id="1*LqPaOCYKuzRjMis_sKWYwQ.png" style="background-image: url(https://cdn-images-1.medium.com/fit/c/160/160/1*LqPaOCYKuzRjMis_sKWYwQ.png);"></a></div></div></div></section>
</section>
<footer><p>By <a href="https://medium.com/@thebojda" class="p-author h-card">Laszlo Fazekas</a> on <a href="https://medium.com/p/df99bc48e306"><time class="dt-published" datetime="2020-01-12T10:48:27.212Z">January 12, 2020</time></a>.</p><p><a href="https://medium.com/@thebojda/tensorflow-alapoz%C3%B3-5-df99bc48e306" class="p-canonical">Canonical link</a></p><p>Exported from <a href="https://medium.com">Medium</a> on March 9, 2021.</p></footer></article></body></html>