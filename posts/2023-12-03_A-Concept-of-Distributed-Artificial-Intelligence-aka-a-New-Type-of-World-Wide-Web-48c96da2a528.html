<!DOCTYPE html><html><head><meta http-equiv="Content-Type" content="text/html; charset=utf-8"><title>A Concept of Distributed Artificial Intelligence aka a New Type of World Wide Web</title><style>
      * {
        font-family: Georgia, Cambria, "Times New Roman", Times, serif;
      }
      html, body {
        margin: 0;
        padding: 0;
      }
      h1 {
        font-size: 50px;
        margin-bottom: 17px;
        color: #333;
      }
      h2 {
        font-size: 24px;
        line-height: 1.6;
        margin: 30px 0 0 0;
        margin-bottom: 18px;
        margin-top: 33px;
        color: #333;
      }
      h3 {
        font-size: 30px;
        margin: 10px 0 20px 0;
        color: #333;
      }
      header {
        width: 640px;
        margin: auto;
      }
      section {
        width: 640px;
        margin: auto;
      }
      section p {
        margin-bottom: 27px;
        font-size: 20px;
        line-height: 1.6;
        color: #333;
      }
      section img {
        max-width: 640px;
      }
      footer {
        padding: 0 20px;
        margin: 50px 0;
        text-align: center;
        font-size: 12px;
      }
      .aspectRatioPlaceholder {
        max-width: auto !important;
        max-height: auto !important;
      }
      .aspectRatioPlaceholder-fill {
        padding-bottom: 0 !important;
      }
      header,
      section[data-field=subtitle],
      section[data-field=description] {
        display: none;
      }
      </style></head><body><article class="h-entry">
<header>
<h1 class="p-name">A Concept of Distributed Artificial Intelligence aka a New Type of World Wide Web</h1>
</header>
<section data-field="subtitle" class="p-summary">
Do you know how large corporate document databases work, where you can ask questions about the content of millions of documents? A Large…
</section>
<section data-field="body" class="e-content">
<section name="1e61" class="section section--body section--first"><div class="section-divider"><hr class="section-divider"></div><div class="section-content"><div class="section-inner sectionLayout--insetColumn"><h3 name="cb0e" id="cb0e" class="graf graf--h3 graf--leading graf--title">A Concept of Distributed Artificial Intelligence aka a New Type of World Wide Web</h3><figure name="e554" id="e554" class="graf graf--figure graf-after--h3"><img class="graf-image" data-image-id="0*tTrOhqahg-46LSIN" data-width="1792" data-height="1024" data-is-featured="true" src="https://cdn-images-1.medium.com/max/800/0*tTrOhqahg-46LSIN"><figcaption class="imageCaption">Image generated by DALL-E by the title of the article</figcaption></figure><p name="d02b" id="d02b" class="graf graf--p graf-after--figure">Do you know how large corporate document databases work, where you can ask questions about the content of millions of documents? A <a href="https://en.wikipedia.org/wiki/Large_language_model" data-href="https://en.wikipedia.org/wiki/Large_language_model" class="markup--anchor markup--p-anchor" rel="noopener" target="_blank">Large Language Model (LLM)</a> could be trained, but that would be prohibitively expensive and resource-intensive. Instead, the documents are chopped into chunks, each of which is assigned a vector and stored in a vector database. When someone asks a question, the question is also transformed into a vector, and the database is searched for chunks with vectors closest to the question vector. These chunks are then passed to an LLM to create a consistent answer to the question. The name of this technique is <a href="https://research.ibm.com/blog/retrieval-augmented-generation-RAG" data-href="https://research.ibm.com/blog/retrieval-augmented-generation-RAG" class="markup--anchor markup--p-anchor" rel="noopener" target="_blank">RAG</a>. This process of converting document chunks into vectors is known as embedding. For instance, OpenAI’s ada-002 model maps document chunks to <a href="https://platform.openai.com/docs/guides/embeddings/embedding-models" data-href="https://platform.openai.com/docs/guides/embeddings/embedding-models" class="markup--anchor markup--p-anchor" rel="noopener" target="_blank">1536-dimensional vectors</a>. This means that each document chunk corresponds to a point in a 1536-dimensional space, with chunks of similar content being close to each other in this space, while those with differing content are farther apart. But is this enough? Can we really map all possible questions and thoughts in the world to 1536 numbers?</p><p name="5a32" id="5a32" class="graf graf--p graf-after--p">In fact, 32 bytes is enough.</p><p name="9970" id="9970" class="graf graf--p graf-after--p">Distributed file systems like <a href="https://ipfs.tech/" data-href="https://ipfs.tech/" class="markup--anchor markup--p-anchor" rel="noopener" target="_blank">IPFS</a> or <a href="https://www.ethswarm.org/" data-href="https://www.ethswarm.org/" class="markup--anchor markup--p-anchor" rel="noopener" target="_blank">Ethereum Swarm</a> are content-addressable storage systems. Both systems are based on a distributed database, where content can be accessed based on a given hash. When we want to access a file in such a system, we can do so with a 32-byte hash.</p><p name="f030" id="f030" class="graf graf--p graf-after--p">By submitting this identifier to the system, it returns the content to us. This 32-byte identifier is generated based on the content and is unique for each piece of content. This means that every piece of content in the world can be assigned a unique 32-byte number.</p><p name="c80b" id="c80b" class="graf graf--p graf-after--p">Think about how fascinating this is. For every video, song, text, book, or program that has ever been created or will be created, there is a unique 32-byte identifier. If this were not the case, IPFS or Swarm would not be able to function, as an identifier would be associated with two different pieces of content in the system, causing collisions.</p><p name="b626" id="b626" class="graf graf--p graf-after--p">However, no one has ever found such a collision.</p><p name="0c5a" id="0c5a" class="graf graf--p graf-after--p">Although 32 bytes might not seem like much, it seems that in the world, we are able to compress almost everything into this size. Even a 32-byte hash can be imagined as a point in a 32-dimensional space. Therefore, embedding is a type of compression, similar to hashing. However, there is a major difference between hashing and embedding.</p><p name="2ed8" id="2ed8" class="graf graf--p graf-after--p">Hash algorithms map the contents to completely random points in the multi-dimensional space, while in the case of embedding, it is important for similar contents to be close to each other.</p><p name="551b" id="551b" class="graf graf--p graf-after--p">This is why hash algorithms use a relatively simple, fixed calculation, while embedding is a complex algorithm that is typically learned through training.</p><p name="ecc8" id="ecc8" class="graf graf--p graf-after--p">In the case of a distributed file system, we can access specific content with concrete hashes, while in the case of a large language model, we search for tokens that best match the embedding vector (which is some kind of a hash).</p><p name="28f6" id="28f6" class="graf graf--p graf-after--p">In both cases, we are talking about a vast database, but in the first case, the database contains specific values associated with concrete keys, while in the second case, both the keys and values are located in a continuous space.</p><p name="fd65" id="fd65" class="graf graf--p graf-after--p">How can we build a system that has access to all the knowledge in the world and can answer questions about anything? The first solution is a centralized system. Something like ChatGPT, which uses crawlers to collect data from the web, and then trains the model on it. Such systems are very static since if we want to add new knowledge to the model, it needs to be retrained (or at least fine-tuned).</p><figure name="9854" id="9854" class="graf graf--figure graf--iframe graf-after--p"><blockquote class="twitter-tweet"><a href="https://twitter.com/ChrSzegedy/status/1728184981678325789"></a></blockquote><script async src="https://platform.twitter.com/widgets.js" charset="utf-8"></script><figcaption class="imageCaption">Today LLMs are built from the garbage of the web</figcaption></figure><p name="d4f2" id="d4f2" class="graf graf--p graf-after--figure">We can give the LLM the ability to use tools, like a web search engine (as ChatGPT is also capable of doing this). In this case, the system is already dynamic but still centralized. The system’s operation is controlled by the operating company. Couldn’t this be somehow decentralized, as IPFS or Swarm does for storage?</p><p name="2df6" id="2df6" class="graf graf--p graf-after--p">Imagine the next version of the Web that is “self indexed”. Instead of crawling for information for central search engines and LLMs, content owners would generate embedding vectors for their content. This system would be similar to IPFS or Swarm in that the individual chunks can be accessed using hashes, but here we would use embedding vectors instead of the hash. Similarly to distributed file systems, we would store which content is available where in a <a href="https://en.wikipedia.org/wiki/Distributed_hash_table" data-href="https://en.wikipedia.org/wiki/Distributed_hash_table" class="markup--anchor markup--p-anchor" rel="noopener" target="_blank">DHT</a>, but instead of using <a href="https://en.wikipedia.org/wiki/Kademlia" data-href="https://en.wikipedia.org/wiki/Kademlia" class="markup--anchor markup--p-anchor" rel="noopener" target="_blank">Kademlia distance</a>, we would use <a href="https://en.wikipedia.org/wiki/Cosine_similarity" data-href="https://en.wikipedia.org/wiki/Cosine_similarity" class="markup--anchor markup--p-anchor" rel="noopener" target="_blank">cosine similarity</a>. If someone wants to query the system, they would generate an embedding vector from the query and send it to known nodes with the lowest cosine similarity. The nodes would further broadcast the query to their known nodes and so on, similar to how we search in a DHT. The nodes respond by sending back the K closest chunks for that query. Finally, the chunks received as a result are summarized on the client side and the response is given to an LLM which provides a consistent response to the query.</p><p name="b53c" id="b53c" class="graf graf--p graf-after--p">This system is very similar to the way large corporate document databases operate, but a decentralized system takes the place of the vector database. This database is owned by no one and is always up-to-date. <strong class="markup--strong markup--p-strong">Data mining from the web would not be necessary as the web itself would be the database.</strong></p><p name="075a" id="075a" class="graf graf--p graf-after--p">Of course, many problems should be solved with such a system. For instance, how can we trust the embeddings produced by the content creators? One solution would be to also generate a zero-knowledge proof for the embedding (<a href="https://worldcoin.org/blog/engineering/intro-to-zkml" data-href="https://worldcoin.org/blog/engineering/intro-to-zkml" class="markup--anchor markup--p-anchor" rel="noopener" target="_blank">ZKML</a>), which would prove that the embedding was indeed generated from the document chunk. The zero-knowledge proof can be easily validated by the client, eliminating any false chunks.</p><p name="4982" id="4982" class="graf graf--p graf-after--p">A similar problem is filtering out fake data. One solution to this could be to rank the chunks or their creators on the basis of some kind of reputation system.</p><p name="79c2" id="79c2" class="graf graf--p graf-after--p">The users could rate the answers given by the system, which would result in feedback on the chunk’s reputation, and through federated learning, also adjust the LLM accordingly, so that the system itself could evolve. This is a form of <a href="https://en.wikipedia.org/wiki/Reinforcement_learning_from_human_feedback" data-href="https://en.wikipedia.org/wiki/Reinforcement_learning_from_human_feedback" class="markup--anchor markup--p-anchor" rel="noopener" target="_blank">RLHF</a>.</p><p name="e185" id="e185" class="graf graf--p graf-after--p graf--trailing">In a nutshell, this is my concept of what the web’s future may look like, in the form of distributed artificial intelligence.</p></div></div></section><section name="dab5" class="section section--body section--last"><div class="section-divider"><hr class="section-divider"></div><div class="section-content"><div class="section-inner sectionLayout--insetColumn"><p name="9f64" id="9f64" class="graf graf--p graf--leading">Subscribe to DDIntel <a href="https://www.ddintel.com/" data-href="https://www.ddintel.com/" class="markup--anchor markup--p-anchor" rel="noopener ugc nofollow noopener" target="_blank">Here</a>.</p><p name="53b6" id="53b6" class="graf graf--p graf-after--p">Have a unique story to share? Submit to DDIntel <a href="https://datadriveninvestor.com/ddintelsubmission" data-href="https://datadriveninvestor.com/ddintelsubmission" class="markup--anchor markup--p-anchor" rel="noopener ugc nofollow noopener" target="_blank">here</a>.</p><p name="5a89" id="5a89" class="graf graf--p graf-after--p">Join our creator ecosystem <a href="https://join.datadriveninvestor.com/" data-href="https://join.datadriveninvestor.com/" class="markup--anchor markup--p-anchor" rel="noopener ugc nofollow noopener" target="_blank">here</a>.</p><p name="560f" id="560f" class="graf graf--p graf-after--p"><a href="https://ddintel.datadriveninvestor.com/" data-href="https://ddintel.datadriveninvestor.com/" class="markup--anchor markup--p-anchor" rel="noopener ugc nofollow noopener" target="_blank"><em class="markup--em markup--p-em">DDIntel</em> </a>captures the more notable pieces from our <a href="https://www.datadriveninvestor.com/" data-href="https://www.datadriveninvestor.com/" class="markup--anchor markup--p-anchor" rel="noopener ugc nofollow noopener" target="_blank"><em class="markup--em markup--p-em">main site</em></a> and our popular <a href="https://medium.datadriveninvestor.com/" data-href="https://medium.datadriveninvestor.com/" class="markup--anchor markup--p-anchor" rel="noopener ugc nofollow noopener" target="_blank"><em class="markup--em markup--p-em">DDI Medium publication</em></a>. Check us out for more insightful work from our community.</p><p name="a6cd" id="a6cd" class="graf graf--p graf-after--p">DDI Official Telegram Channel: <a href="https://t.me/+tafUp6ecEys4YjQ1" data-href="https://t.me/+tafUp6ecEys4YjQ1" class="markup--anchor markup--p-anchor" rel="noopener ugc nofollow noopener" target="_blank">https://t.me/+tafUp6ecEys4YjQ1</a></p><p name="235c" id="235c" class="graf graf--p graf-after--p graf--trailing">Follow us on <a href="https://www.linkedin.com/company/data-driven-investor" data-href="https://www.linkedin.com/company/data-driven-investor" class="markup--anchor markup--p-anchor" rel="noopener ugc nofollow noopener" target="_blank"><em class="markup--em markup--p-em">LinkedIn</em></a>, <a href="https://twitter.com/@DDInvestorHQ" data-href="https://twitter.com/@DDInvestorHQ" class="markup--anchor markup--p-anchor" rel="noopener ugc nofollow noopener" target="_blank"><em class="markup--em markup--p-em">Twitter</em></a>, <a href="https://www.youtube.com/c/datadriveninvestor" data-href="https://www.youtube.com/c/datadriveninvestor" class="markup--anchor markup--p-anchor" rel="noopener ugc nofollow noopener" target="_blank"><em class="markup--em markup--p-em">YouTube</em></a>, and <a href="https://www.facebook.com/datadriveninvestor" data-href="https://www.facebook.com/datadriveninvestor" class="markup--anchor markup--p-anchor" rel="noopener ugc nofollow noopener" target="_blank"><em class="markup--em markup--p-em">Facebook</em></a>.</p></div></div></section>
</section>
<footer><p>By <a href="https://medium.com/@thebojda" class="p-author h-card">Laszlo Fazekas</a> on <a href="https://medium.com/p/48c96da2a528"><time class="dt-published" datetime="2023-12-03T10:39:15.006Z">December 3, 2023</time></a>.</p><p><a href="https://medium.com/@thebojda/a-concept-of-distributed-artificial-intelligence-aka-a-new-type-of-world-wide-web-48c96da2a528" class="p-canonical">Canonical link</a></p><p>Exported from <a href="https://medium.com">Medium</a> on December 27, 2023.</p></footer></article></body></html>