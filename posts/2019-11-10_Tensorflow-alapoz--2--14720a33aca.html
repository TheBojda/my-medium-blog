<!DOCTYPE html><html><head><meta http-equiv="Content-Type" content="text/html; charset=utf-8"><title>Tensorflow alapozó 2.</title><style>
      * {
        font-family: Georgia, Cambria, "Times New Roman", Times, serif;
      }
      html, body {
        margin: 0;
        padding: 0;
      }
      h1 {
        font-size: 50px;
        margin-bottom: 17px;
        color: #333;
      }
      h2 {
        font-size: 24px;
        line-height: 1.6;
        margin: 30px 0 0 0;
        margin-bottom: 18px;
        margin-top: 33px;
        color: #333;
      }
      h3 {
        font-size: 30px;
        margin: 10px 0 20px 0;
        color: #333;
      }
      header {
        width: 640px;
        margin: auto;
      }
      section {
        width: 640px;
        margin: auto;
      }
      section p {
        margin-bottom: 27px;
        font-size: 20px;
        line-height: 1.6;
        color: #333;
      }
      section img {
        max-width: 640px;
      }
      footer {
        padding: 0 20px;
        margin: 50px 0;
        text-align: center;
        font-size: 12px;
      }
      .aspectRatioPlaceholder {
        max-width: auto !important;
        max-height: auto !important;
      }
      .aspectRatioPlaceholder-fill {
        padding-bottom: 0 !important;
      }
      header,
      section[data-field=subtitle],
      section[data-field=description] {
        display: none;
      }
      </style></head><body><article class="h-entry">
<header>
<h1 class="p-name">Tensorflow alapozó 2.</h1>
</header>
<section data-field="subtitle" class="p-summary">
(backpropagation, avagy hogyan működik a varázslat)
</section>
<section data-field="body" class="e-content">
<section name="9461" class="section section--body section--first section--last"><div class="section-divider"><hr class="section-divider"></div><div class="section-content"><div class="section-inner sectionLayout--insetColumn"><h3 name="62de" id="62de" class="graf graf--h3 graf--leading graf--title">Tensorflow alapozó 2.</h3><h4 name="a92b" id="a92b" class="graf graf--h4 graf-after--h3 graf--subtitle">(backpropagation, avagy hogyan működik a varázslat)</h4><p name="f53a" id="f53a" class="graf graf--p graf-after--h4">Az <a href="https://medium.com/@thebojda/tensorflow-alapoz%C3%B3-d2d1ee97c9db" data-href="https://medium.com/@thebojda/tensorflow-alapoz%C3%B3-d2d1ee97c9db" class="markup--anchor markup--p-anchor" target="_blank">előző cikkemben</a> azt írtam, hogy egy neurális háló egy olyan fekete doboz, aminek van egy bemenete, van egy kimenete, és amin rengeteg potméter található. Ezekkel a potméterekkel lehet beállítani, hogy milyen működést valósítson meg a hálózat. Azt is írtam, hogy a dologban az a varázslat, hogy a potmétereket nem nekünk kell beállítani. Elég ha mintákat mutatunk egy optimalizáló algoritmusnak, ami ez alapján szépen behangolja nekünk a hálózatot. Ebben az írásban kicsit lemerészkedünk a nyúl üregébe és megnézzük hogyan is működik a varászlat, vagyis hogyan működik az optimalizáló algoritmus ami behangolja a hálózatot. Aki esetleg nem olvasta az első részt, az itt bepótolhatja:</p><div name="5ea7" id="5ea7" class="graf graf--mixtapeEmbed graf-after--p"><a href="https://medium.com/@thebojda/tensorflow-alapoz%C3%B3-d2d1ee97c9db" data-href="https://medium.com/@thebojda/tensorflow-alapoz%C3%B3-d2d1ee97c9db" class="markup--anchor markup--mixtapeEmbed-anchor" title="https://medium.com/@thebojda/tensorflow-alapoz%C3%B3-d2d1ee97c9db"><strong class="markup--strong markup--mixtapeEmbed-strong">TensorFlow alapozó</strong><br><em class="markup--em markup--mixtapeEmbed-em">(neurális hálózatok, tenzorok és képfelismerés a gyakorlatban)</em>medium.com</a><a href="https://medium.com/@thebojda/tensorflow-alapoz%C3%B3-d2d1ee97c9db" class="js-mixtapeImage mixtapeImage u-ignoreBlock" data-media-id="15660260b8dadb0ef12eafefd0be7499" data-thumbnail-img-id="1*T4ARzySpEQvEnr_9pc78pg.jpeg" style="background-image: url(https://cdn-images-1.medium.com/fit/c/160/160/1*T4ARzySpEQvEnr_9pc78pg.jpeg);"></a></div><p name="deec" id="deec" class="graf graf--p graf-after--mixtapeEmbed">Azt már az első részből megtudhattuk, hogy a tanítás folyamán minden lépésben kiszámoljuk a hálózat kimenetét, majd ezt összehasonlítva a kívánt kimenettel kiszámoljuk a hálózat hibáját. A cél ennek a hibának a minimalizálása. Hogy lehet nekifutni egy ilyen feladatnak? Nos, ezt a hibát ábrázolhatjuk egy függvényként. A függvény paraméterei a súlyok (a potméterek aktuális állásai), az aktuális bemenet és kimenet pedig az adott lépésben a függvény konstansai. Ha ezt ábrázolni próbálnánk, egy dimbes-dombos hiperfelületet kapnánk. Az egyetlen probléma a dologgal, hogy az előző cikkben szereplő neurális háló hibafüggvényének ábrázolásához 122 571 dimenziós térre lenne szükség. Ezt elég nehéz elképzelni. Ne is próbálja senki, mert csak belefájdul a feje. Első körben inkább kezdjünk két súllyal, mert azt még pont lehet ábrázolni 3 dimenzióban.</p><figure name="9a2f" id="9a2f" class="graf graf--figure graf-after--p"><div class="aspectRatioPlaceholder is-locked" style="max-width: 700px; max-height: 575px;"><div class="aspectRatioPlaceholder-fill" style="padding-bottom: 82.1%;"></div><img class="graf-image" data-image-id="1*YTDwPXrnfbPndXxNw77O-w.png" data-width="760" data-height="624" data-is-featured="true" src="https://cdn-images-1.medium.com/max/800/1*YTDwPXrnfbPndXxNw77O-w.png"></div><figcaption class="imageCaption">Forrás: <a href="https://en.wikipedia.org/wiki/Gradient_descent" data-href="https://en.wikipedia.org/wiki/Gradient_descent" class="markup--anchor markup--figure-anchor" rel="noopener" target="_blank">https://en.wikipedia.org/wiki/Gradient_descent</a></figcaption></figure><p name="5358" id="5358" class="graf graf--p graf-after--figure">Minél magasabban van egy pont a dombon, annál nagyobb a hiba és minél alacsonyabban, annál kisebb. A tér minden egyes pontja a potméterek adott állásához tartozik. Amikor optimalizáljuk a súlyokat, olyan mint ha a domb egy adott pontján állnánk, ahonnan szeretnénk lejjebb jutni a kisebb hiba irányába. Külön nehezítés a dologban, hogy nem látjuk át a teljes teret, csak a legközvetlenebb környezetünkről van információnk. Tehát úgy kellene lemászni a hegyről, hogy mindeközben be van kötve a szemünk, és csak tapogatózni tudunk.</p><figure name="89af" id="89af" class="graf graf--figure graf-after--p"><div class="aspectRatioPlaceholder is-locked" style="max-width: 600px; max-height: 375px;"><div class="aspectRatioPlaceholder-fill" style="padding-bottom: 62.5%;"></div><img class="graf-image" data-image-id="1*kV4PoglalqnaOF2yEP_MPQ.png" data-width="600" data-height="375" src="https://cdn-images-1.medium.com/max/800/1*kV4PoglalqnaOF2yEP_MPQ.png"></div><figcaption class="imageCaption">Forrás: <a href="https://www.freecodecamp.org/news/understanding-gradient-descent-the-most-popular-ml-algorithm-a66c0d97307f/" data-href="https://www.freecodecamp.org/news/understanding-gradient-descent-the-most-popular-ml-algorithm-a66c0d97307f/" class="markup--anchor markup--figure-anchor" rel="noopener" target="_blank">https://www.freecodecamp.org/news/understanding-gradient-descent-the-most-popular-ml-algorithm-a66c0d97307f/</a></figcaption></figure><p name="ac72" id="ac72" class="graf graf--p graf-after--figure">Mit tehet ilyenkor az ember? Jobb híján kitapogatja, hogy merre lejt a hegy és az alapján elindul lefelé. No de egy sokparaméteres hibafüggvényről honnan tudjuk, hogy merre lejt egy adott pontban?</p><p name="e21a" id="e21a" class="graf graf--p graf-after--p">Fősulin analízisből egy komplett félévet szenteltek a deriválásnak és a függvénydiszkussziónak (függvényelemzésnek). A deriválás nagyon ment, mert alapvetően algoritmikus dologról volt szó amiben úgy általában jók szoktak lenni a programozó palánták. Le is deriváltam bármilyen függvényt, félév végén kaptam is rá szép jegyet, meg minden, de nagyon frusztrált, hogy nem értettem, mire lesz nekem ez jó. Aztán jöttek a neurális hálózatok, a lineáris regresszió, meg hasonlók, és hirtelen minden értelmet nyert… A deriválás lényege, hogy megmondjuk egy függvény adott pontjához húzott érintő meredekségét. Ez pont az ami nekünk kell, vagyis hogy “adott pontban merre lejt a hegy”. Ahogyan írtam, ha a deriválandó függvény megfelel bizonyos szabályoknak, akkor maga a deriválás simán algoritmikusan elvégezhető, tehát tudunk olyan programot írni, amibe bedobunk egy függvényt és kidobja annak a deriváltját. A neurális hálók összeállítása esetén direkt ilyen függvényeket használnak, így ha pl. Tensorflow-ban összerakunk egy hálózatot, akkor egy deriválható függvény lesz az eredmény. Innen már látszik nagyjából, hogy hogyan működik a tanítás: fogunk egy bemenetet, kiszámoljuk a kimenetet, kiszámoljuk a hibát, kiszámoljuk a deriváltakat, majd módosítjuk a súlyokat a deriváltaktól függően, hogy lejjebb jussunk a hegyen, vagyis minimalizáljuk a hibát. (Ezt a hegyről lemászós technikát egyébként gradient descent-nek hívják, amit nem tudok jól magyarra fordítani, de nem is nagyon érdemes, mert főleg angolul érhetőek el jó anyagok a témában.)</p><p name="4c9d" id="4c9d" class="graf graf--p graf-after--p">Ha valakit részletekbe menően érdekel, hogy hogyan tudunk egy olyan hatalmas függvényt lederiválni, mint egy neurális háló és rendelkezik a megfelelő matematikai alapokkal, az nézze meg Andrej Karpathy előadását a témában.</p><figure name="e3e3" id="e3e3" class="graf graf--figure graf--iframe graf-after--p"><iframe src="https://www.youtube.com/embed/i94OvYb6noo?feature=oembed" width="700" height="393" frameborder="0" scrolling="no"></iframe><figcaption class="imageCaption"><a href="https://www.youtube.com/watch?v=i94OvYb6noo" data-href="https://www.youtube.com/watch?v=i94OvYb6noo" class="markup--anchor markup--figure-anchor" rel="noopener" target="_blank">https://www.youtube.com/watch?v=i94OvYb6noo</a></figcaption></figure><p name="b5fa" id="b5fa" class="graf graf--p graf-after--figure">Dióhéjban összefoglalva arról van szó, hogy van egy láncszabály nevű deriválási szabály ami függvény függvényének a deriválásáról szól, tehát pont arról, hogy hogyan kell operátor (matematikai művelet) sorozatokat deriválni. Ennek értelmében ha van egy műveletsorozatunk aminek az elején bemegy valami, a végén pedig kijön egy hiba (ami a kimenet és az elvárt kimenet különbségéből adódik), ott a deriváltakat úgy számoljuk, hogy a hibát visszaszorozzuk az utolsó művelet “lokális deriváltjával”, így megkapjuk az utolsó művelet deriváltját. Ezt ha megszorozzuk az utolsó előtti művelet “lokális deriváltjával”, azzal megkapjuk az utolsó előtti művelet deriváltját, stb. Szóval ha szépen visszafelé haladunk a műveleteken, úgy szorzásokkal és a “lokális deriváltak” számolásával szépen kiszámolható minden paraméterhez a szükséges derivált (vagyis hogy merre lejt a hegy abban az irányban). Hogy mindezt meg lehessen tenni, minden Tensorflow operátor úgy van megadva, hogy nem csak a kimenetet tudja kiszámolni a bemenet és a paraméterek függvényében, hanem azt is, hogy a paraméterek függvényében mi a lokális derivált. Tehát minden Tensorflow operátor tudja magát deriválni! Ennek fényében a tanítás a következőképpen néz ki: Végigtoljuk a bemeneti tenzort az operátor folyamon, aminek a végén kijön az eredmény. Az eredmény alapján kiszámoljuk a hibát. Majd a hibát <strong class="markup--strong markup--p-strong">visszafelé</strong> újra végigtoljuk az operátor folyamon, csak ez esetben a lokális deriváltakat szorozgatjuk a láncszabály szerint. Mire visszaérünk az operátor folyam elejére, a művelet eredményeként kipotyog az összes paraméterhez tartozó derivált. A deriváltak alapján módosítjuk a súlyokat, és jöhet a következő minta. Ezt egészen addig ismételgetjük, amíg a modellünk nem lesz elég pontos (vagy amíg nem jövünk rá, hogy az aktuális hálózati struktúra nem lesz jó a problémához és másikat kell keresnünk). Ezt a módszert backpropagation-nek hívjuk, amit szoktak “hiba-visszaterjesztésnek” fordítani.</p><p name="e5b1" id="e5b1" class="graf graf--p graf-after--p">A deriváltak számolására Tensorflow-ban létezik egy GradientTape nevű osztály. Ez olyan mint egy videófelvevő. Ha elindítjuk, rögzít minden tensorflow műveletet, megállítás után pedig visszajátszhatjuk a műveleteket fordított sorrendben, hogy megkapjuk a deriváltakat. Nézzünk is egy példát a használatára.</p><figure name="f74b" id="f74b" class="graf graf--figure graf--iframe graf-after--p"><script src="https://gist.github.com/TheBojda/6133e24a17da2c79117a632332953fcf.js"></script></figure><p name="e83f" id="e83f" class="graf graf--p graf-after--figure">A kód elején definiálunk egy nagyon-nagyon egyszerű lineáris modellt, ami annyiból áll, hogy a bemenetet megszorozzuk egy W súllyal, és hozzáadunk egy b számot. Akinek még rémlik matekóráról, annak beugorhat, hogy ez pont egy egyenes egyenlete. Felfoghatjuk ezt egy nagyon egyszerű neurális hálózatnak is, ami 1 db neuronból áll, aminek egy bemenete van és nincs kimeneti függvénye. Ezt a modellt fogjuk tanítani, amihez generálunk egy rakás (1000 db) mintát. A modellben a W és a b kezdőértéke egyaránt 16, míg az elérendő értékek (TRUE_W és TRUE_b) 3 és 0.5. A következő pár sorral ki is rajzoltatjuk a modellünk által meghatározott egyenest, valamint a tanító halmazt.</p><figure name="8719" id="8719" class="graf graf--figure graf-after--p"><div class="aspectRatioPlaceholder is-locked" style="max-width: 640px; max-height: 480px;"><div class="aspectRatioPlaceholder-fill" style="padding-bottom: 75%;"></div><img class="graf-image" data-image-id="1*XHBsqWcnovwFCrxpkFxbgA.png" data-width="640" data-height="480" src="https://cdn-images-1.medium.com/max/800/1*XHBsqWcnovwFCrxpkFxbgA.png"></div></figure><p name="826e" id="826e" class="graf graf--p graf-after--figure">A cél ugye az lenne, hogy a narancssárga vonal egybeessen a kék vonallal. A tanításhoz definiálunk egy loss függvényt, aminek két paramétere két egydimenziós (1000 elemű) tenzor lesz. Az első paraméter (y) az elvárt kimeneteket tartalmazza, a második paraméter (pred_y) pedig a modell által adott értékeket. A hibát úgy számoljuk, hogy minden esetben vesszük a kapott és az elvárt érték különbségét és négyzetre emeljük, aztán az egészből átlagot számolunk. A loss függvény visszatérési értéke tehát egy szám lesz. A lényeg a train metódusban van. Itt hozzuk létre a GradientTape-et. Python-ban a <a href="https://effbot.org/zone/python-with-statement.htm" data-href="https://effbot.org/zone/python-with-statement.htm" class="markup--anchor markup--p-anchor" rel="noopener" target="_blank">with operátor</a> úgy működik, hogy meghívja a létrehozott objektum __enter__ metódusát, majd a with blokk végén az __exit__ metódust. A GradientTape esetén tehát a with részben kezdődik a rögzítés, és egészen a blokk végéig tart. A with blokkon belül lefuttatjuk a model-t az X bemeneten, majd a loss függvénnyel kiszámoljuk a hibát. A művelet közben végrehajtott minden operátort rögzít a GradientTape a későbbi deriváláshoz. A with blokkban tehát egyszer végigtoltuk a bemeneti tenzorunkat a modell gráfon, hogy kipottyanjon a hiba. Jöhet a visszafele működtetés. Erre szolgál a GradientTape gradient metódusa, aminek első paramétere az eredmény tenzor, a második paramétere pedig a változók listája, amikhez a deriváltakat kérjük. Jelen esetben a W és a b változó deriváltjaira lesz szükség, amit a függvény egy listában vissza is ad. Most, hogy megvannak a deriváltak, felszorozzuk őket egy lr változóval (learning rate), és levonjuk a W és a b aktuális értékéből. A hegyes példánál maradva a gradient metódus adja meg, hogy merre lejt a hegy (ez a tapogatás), míg a paraméterek módosítása az elmozdulás abba az irányba. A learnig rate azt mondja meg, hogy egy lépésben mekkorát lépjünk. Ha túl kicsi learning rate-et választunk, akkor sokáig fog tartani a tanítás (kis lépésekkel lassabban jutunk le a hegyről), míg nagy lépések esetén előfordulhat, hogy “átugorjuk” a völgyet, így soha nem érünk le a hegyről. Fontos tehát, hogy jó értéket válasszunk az lr (learning rate) változónak. A tanítás után megjelenítjük a paraméterek időbeni változását. Itt szépen látszik, hogy hogyan érte el a W és a b paraméter a kívánt értéket.</p><figure name="8d70" id="8d70" class="graf graf--figure graf-after--p"><div class="aspectRatioPlaceholder is-locked" style="max-width: 640px; max-height: 480px;"><div class="aspectRatioPlaceholder-fill" style="padding-bottom: 75%;"></div><img class="graf-image" data-image-id="1*XXzN5H34cLn_cu1j0ZeibQ.png" data-width="640" data-height="480" src="https://cdn-images-1.medium.com/max/800/1*XXzN5H34cLn_cu1j0ZeibQ.png"></div></figure><p name="69c8" id="69c8" class="graf graf--p graf-after--figure">Végül megjelenítjük újra a minta ponthalmazt és a modell által meghatározott egyenest, amik láthatóan most már egybe esnek.</p><figure name="e3dd" id="e3dd" class="graf graf--figure graf-after--p"><div class="aspectRatioPlaceholder is-locked" style="max-width: 640px; max-height: 480px;"><div class="aspectRatioPlaceholder-fill" style="padding-bottom: 75%;"></div><img class="graf-image" data-image-id="1*q5KN8MO047-2-GuKv7_UYg.png" data-width="640" data-height="480" src="https://cdn-images-1.medium.com/max/800/1*q5KN8MO047-2-GuKv7_UYg.png"></div></figure><p name="9cc1" id="9cc1" class="graf graf--p graf-after--figure">Most, hogy már értjük a tanítás működését és kicsit kifejtettük, hogy mi van a dobozban, vegyük elő az előző cikkben szereplő konvolúciós hálós kódunk tanítás részét. Az előző cikkben ezt a részt átugrottam, de most már rendelkezünk a szükséges tudással az értelmezéséhez.</p><pre name="cfbb" id="cfbb" class="graf graf--pre graf-after--p">model.compile(optimizer=’adam’, loss=’sparse_categorical_crossentropy’)</pre><p name="ad7a" id="ad7a" class="graf graf--p graf-after--pre">A fenti rész beállítja a tanítás paramétereit. A sparse_categorical_crossentropy egy hibafüggvény. Olyan mint a mi loss függvényünk az előző példában, csak kicsit bonyolultabb. Ezt a hibafüggvényt osztályozást végző neurális hálók esetén szokták használni. A másik paraméter az optimizer, ahol a tanítást végző algoritmust tudjuk megadni. Ez az a függvény, ami a deriváltak alapján a paramétereket fogja módosítani. A mi fenti példánkban ezt a feladatot az a két sor végezte, ahol a W és a b paraméterekből levontuk a vonatkozó derivált és a learning rate szorzatát. Ilyen optimalizációs algoritmusból is van több, amik a deriváltak ismeretében különböző stratégiák mentén lépnek kisebbeket vagy nagyobbakat.</p><pre name="d9dd" id="d9dd" class="graf graf--pre graf-after--p">model.fit(train_images, train_labels, epochs=10)</pre><p name="3e83" id="3e83" class="graf graf--p graf-after--pre">Az előző cikk konvolúciós hálós mintájában a fit metódus végezte a tanítást. Ez a metódus tartalmazza a GradientTape-es ciklust. A metódus első két paramétere a tanító képeket és a hozzájuk tartozó címkéket tartalmazza, míg az epochs azt, hogy hányszor ismételjük meg a tanítást. Most már tudjuk, hogy a metódus belsejében a GradientTape rögzíti a modell végrehajtása, valamint a hibafüggvény (sparse_category_crossentropy) futtatása közben futtatott operátorokat, majd a futtatás végeztével visszagörgeti a folyamatot, hogy kijöjjenek a deriváltak. Az így kapott deriváltakat megkapja az optimizer (adam), ami alapján módosítja a hálózat paramétereit. A folyamatot 10x ismételjük meg, ami alatt a hálózat egész jól megtanul cicákat felismerni.</p><p name="9c85" id="9c85" class="graf graf--p graf-after--p graf--trailing">Körülbelül ennyit szerettem volna írni a “varázslatról”, aminek köszönhetően a neurális hálók tanulni képesek. Bár deriválásról szórakoztató formában írni elég embert próbáló feladat, én megpróbáltam megtenni minden tőlem telhetőt. Remélem sokan gondolják majd hasznosnak a fentieket.</p></div></div></section>
</section>
<footer><p>By <a href="https://medium.com/@thebojda" class="p-author h-card">Laszlo Fazekas</a> on <a href="https://medium.com/p/14720a33aca"><time class="dt-published" datetime="2019-11-10T12:18:47.656Z">November 10, 2019</time></a>.</p><p><a href="https://medium.com/@thebojda/tensorflow-alapoz%C3%B3-2-14720a33aca" class="p-canonical">Canonical link</a></p><p>Exported from <a href="https://medium.com">Medium</a> on November 19, 2019.</p></footer></article></body></html>